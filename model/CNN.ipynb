{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pgniSQW82X1",
        "outputId": "ea66d4b7-7ed0-444e-d5d3-8de74d1a9cb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pythainlp\n",
            "  Downloading pythainlp-3.0.5-py3-none-any.whl (11.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.5 MB 4.3 MB/s \n",
            "\u001b[?25hCollecting tinydb>=3.0\n",
            "  Downloading tinydb-4.7.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from pythainlp) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (2.10)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from tinydb>=3.0->pythainlp) (3.10.0.2)\n",
            "Installing collected packages: tinydb, pythainlp\n",
            "Successfully installed pythainlp-3.0.5 tinydb-4.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pythainlp "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONK14jX-83nS",
        "outputId": "689c027f-39bc-4a54-ca82-9372dcacf643"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "Model = tf.keras.models.Model\n",
        "ModelCheckpoint = tf.keras.callbacks.ModelCheckpoint\n",
        "ReduceLROnPlateau = tf.keras.callbacks.ReduceLROnPlateau\n",
        "load_model = tf.keras.models.load_model\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from pythainlp.tokenize import word_tokenize, Tokenizer\n",
        "KRTokenizer = tf.keras.preprocessing.text.Tokenizer\n",
        "\n",
        "pad_sequences = tf.keras.preprocessing.sequence.pad_sequences\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU, LSTM, Bidirectional, Embedding, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle as p\n",
        "import plotly\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import os\n",
        "import string\n",
        "from os import listdir\n",
        "from string import punctuation\n",
        "from os import listdir\n",
        "\n",
        "#########################\n",
        "from pythainlp.tokenize import word_tokenize #, Tokenizer\n",
        "from pythainlp.corpus.common import thai_words\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "from pythainlp.corpus import thai_stopwords\n",
        "\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzHjQrIWLCjn"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 500\n",
        "BS = 32\n",
        "DIMENSION = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xB9V4sJnb2w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de160565-f57c-450c-b321-47797ef73a5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eg251wcufVjd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f801aaba-6afa-4b99-9528-3f26d789dc3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vktvQ7ncfVre",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60eaaed1-34c7-49b1-c212-c23792455964"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#โควิด19.csv', 'รวม.csv', 'tt.csv', 'รวมเพิ่ม.csv', 'FixeD.csv', 'fixedall.csv', '3121.csv', '2751.csv', '3193 with no รายงานสถานการณ์.csv', '6173.csv', 'no.csv', 'T1', '.ipynb_checkpoints', 'การรักษาเดือน 1 (11).csv', '10005.gsheet', '10005.csv', '9224.csv', '9224k.csv', '9224t.csv']\n"
          ]
        }
      ],
      "source": [
        "data_path = 'drive/My Drive/source'\n",
        "print(os.listdir(data_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dut09JaPfovP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19f66e18-504e-47cf-b35b-71d945353a82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#โควิด19.csv\n",
            "รวม.csv\n",
            "tt.csv\n",
            "รวมเพิ่ม.csv\n",
            "FixeD.csv\n",
            "fixedall.csv\n",
            "3121.csv\n",
            "2751.csv\n",
            "3193 with no รายงานสถานการณ์.csv\n",
            "6173.csv\n",
            "no.csv\n",
            "T1\n",
            ".ipynb_checkpoints\n",
            "การรักษาเดือน 1 (11).csv\n",
            "10005.gsheet\n",
            "10005.csv\n",
            "9224.csv\n",
            "9224k.csv\n",
            "9224t.csv\n"
          ]
        }
      ],
      "source": [
        "filelist = os.listdir(data_path)\n",
        "\n",
        "for filename in filelist:\n",
        "  print(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDbtoUbOfuvA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ecc2044d-42bb-4161-a8e0-c6ac82429365"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'drive/My Drive/source/10005.csv'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "file_name = data_path+'/10005.csv'\n",
        "file_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndua8lYlfu1H"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(file_name,encoding ='utf-8',  usecols=[0,1] ,names=['sentence','sentiment'], header=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icb0z9Xtfu6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "8097c8fe-4069-45d5-d8b3-52674f65e1b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a01ad5e8-ab9d-4dd4-8794-283618cd21f8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>#ม็อบ10กุมภา ตำรวจประกาศให้ยุติการชุม!! #เรื่อ...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@babyfforfaii นานแล้ว ตั้งแต่ม้อบรอบแรกๆ ตั้งแ...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@fishho213 ทุกครั้งก็ไม่มีใครติดโควิดมาจากม็อบ...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@guangne @SabaiFreedom ด่ากันไป\\nดูผลงานลุงตู่...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@phoom91 นานาจิตตัง โลกนี้เป๋าธรรมดา\\nถ้าเป็นน...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a01ad5e8-ab9d-4dd4-8794-283618cd21f8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a01ad5e8-ab9d-4dd4-8794-283618cd21f8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a01ad5e8-ab9d-4dd4-8794-283618cd21f8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                            sentence     sentiment\n",
              "0  #ม็อบ10กุมภา ตำรวจประกาศให้ยุติการชุม!! #เรื่อ...  ไม่เดือดร้อน\n",
              "1  @babyfforfaii นานแล้ว ตั้งแต่ม้อบรอบแรกๆ ตั้งแ...  ไม่เดือดร้อน\n",
              "2  @fishho213 ทุกครั้งก็ไม่มีใครติดโควิดมาจากม็อบ...  ไม่เดือดร้อน\n",
              "3  @guangne @SabaiFreedom ด่ากันไป\\nดูผลงานลุงตู่...  ไม่เดือดร้อน\n",
              "4  @phoom91 นานาจิตตัง โลกนี้เป๋าธรรมดา\\nถ้าเป็นน...  ไม่เดือดร้อน"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZV3amv-aZ4m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa848df5-1968-4b06-998d-cb542aa284d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.sample of                                                 sentence     sentiment\n",
              "0      #ม็อบ10กุมภา ตำรวจประกาศให้ยุติการชุม!! #เรื่อ...  ไม่เดือดร้อน\n",
              "1      @babyfforfaii นานแล้ว ตั้งแต่ม้อบรอบแรกๆ ตั้งแ...  ไม่เดือดร้อน\n",
              "2      @fishho213 ทุกครั้งก็ไม่มีใครติดโควิดมาจากม็อบ...  ไม่เดือดร้อน\n",
              "3      @guangne @SabaiFreedom ด่ากันไป\\nดูผลงานลุงตู่...  ไม่เดือดร้อน\n",
              "4      @phoom91 นานาจิตตัง โลกนี้เป๋าธรรมดา\\nถ้าเป็นน...  ไม่เดือดร้อน\n",
              "...                                                  ...           ...\n",
              "9999   ขอขอบคุณคุณลูกค้าทุกท่านที่มาอุดหนุนคะ🥰🥰🥰พบกัน...  ไม่เดือดร้อน\n",
              "10000  วิธี ใช้เครื่องพ่นไอน้ำ ขจัดโควิด 19\\nhttps://...  ไม่เดือดร้อน\n",
              "10001               @UptoSZ เวลคัม โควิด มหาสารคาม 55555     เดือดร้อน\n",
              "10002  ที่จริงปีนี้แปลนต้องจัดมีตวันเกิดนะ​ โควิดเมื่...     เดือดร้อน\n",
              "10003  โรงแรม เคยู โฮม ใน มหาวิทยาลัยเกษตรศาสตร์\\n\\nแ...  ไม่เดือดร้อน\n",
              "\n",
              "[10004 rows x 2 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "df.sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFx9Fnq8akPF"
      },
      "outputs": [],
      "source": [
        "df = df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG6QE62ZbfCD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "706cb957-8f72-452e-a29a-e169828d2435"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f410e576-d762-494a-ab05-880c5ceae6b5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>@TLHR2014 @ItsMinute เมื่อไหร่จะหมดโควิด อยากไ...</td>\n",
              "      <td>เดือดร้อน</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>ฉันผุ้อดสูกับแฮชแทกนี้ #ไม่สู้ก็อยู่อย่างไทย เ...</td>\n",
              "      <td>เดือดร้อน</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>ทำไมเมื่อวานถึงปล่อยให้มีคนพม่ามาประท้วงกันที่...</td>\n",
              "      <td>เดือดร้อน</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>บ้านพ่อมึงซื้อทิชชู่เป็นหมื่นล้านเหรอ สติปัญญา...</td>\n",
              "      <td>เดือดร้อน</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>บ้าไปแล้ว 1.เรื่องการจับกุมแกนนำม๊อบ พวกเขาทำผ...</td>\n",
              "      <td>เดือดร้อน</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f410e576-d762-494a-ab05-880c5ceae6b5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f410e576-d762-494a-ab05-880c5ceae6b5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f410e576-d762-494a-ab05-880c5ceae6b5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                             sentence  sentiment\n",
              "8   @TLHR2014 @ItsMinute เมื่อไหร่จะหมดโควิด อยากไ...  เดือดร้อน\n",
              "12  ฉันผุ้อดสูกับแฮชแทกนี้ #ไม่สู้ก็อยู่อย่างไทย เ...  เดือดร้อน\n",
              "14  ทำไมเมื่อวานถึงปล่อยให้มีคนพม่ามาประท้วงกันที่...  เดือดร้อน\n",
              "16  บ้านพ่อมึงซื้อทิชชู่เป็นหมื่นล้านเหรอ สติปัญญา...  เดือดร้อน\n",
              "17  บ้าไปแล้ว 1.เรื่องการจับกุมแกนนำม๊อบ พวกเขาทำผ...  เดือดร้อน"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "เดือดร้อน_df = df[df.sentiment == \"เดือดร้อน\"]\n",
        "เดือดร้อน_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(เดือดร้อน_df))"
      ],
      "metadata": {
        "id": "SUcicYv4RouH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8fc4e31-c28c-4ebc-808f-0977445932c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtFs8h4xatRO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "3d96c75b-9e23-46d4-c9f6-72d91aaf36a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d5f744f7-6450-414d-8c19-13fe05c77997\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2740</th>\n",
              "      <td>อัปเดตสถานการณ์ #โควิด19 วันนี้ (26 มิ.ย. 64) ...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1303</th>\n",
              "      <td>ล่าสุดต่อประกันโควิด</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8399</th>\n",
              "      <td>สถานการณ์การติดเชื้อ COVID-19 ในประเทศ วันที่ ...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8647</th>\n",
              "      <td>ด่วน! \\n\\n22:38 ศูนย์ปฏิบัติการ COVID-19 จังหว...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2436</th>\n",
              "      <td>@vanishkul นิวซีแลนด์มีประชากร 7 ล้าน ยังฉีดวั...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d5f744f7-6450-414d-8c19-13fe05c77997')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d5f744f7-6450-414d-8c19-13fe05c77997 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d5f744f7-6450-414d-8c19-13fe05c77997');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                               sentence     sentiment\n",
              "2740  อัปเดตสถานการณ์ #โควิด19 วันนี้ (26 มิ.ย. 64) ...  ไม่เดือดร้อน\n",
              "1303                               ล่าสุดต่อประกันโควิด  ไม่เดือดร้อน\n",
              "8399  สถานการณ์การติดเชื้อ COVID-19 ในประเทศ วันที่ ...  ไม่เดือดร้อน\n",
              "8647  ด่วน! \\n\\n22:38 ศูนย์ปฏิบัติการ COVID-19 จังหว...  ไม่เดือดร้อน\n",
              "2436  @vanishkul นิวซีแลนด์มีประชากร 7 ล้าน ยังฉีดวั...  ไม่เดือดร้อน"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "ไม่เดือดร้อน_df = df[df.sentiment == \"ไม่เดือดร้อน\"].sample(3677)\n",
        "ไม่เดือดร้อน_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5_zbsCJbrE0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "c92cc82a-0c79-4a04-ccef-30b998985cf9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f63ce77b-4d0f-4c82-9861-fda49774bc0e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8074</th>\n",
              "      <td>คิดถึงพี่น้องน้ำ 😊🥰🤟❤😷😷\\nป.โทจะไม่จบเพราะดูแต่...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2408</th>\n",
              "      <td>สถานการณ์การติดเชื้อโควิด-19 ในประเทศ วันที่ 1...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9844</th>\n",
              "      <td>ทำงานรอกันด้วยค่ะเพื่อน\\n\\n#งานก็ต้องค้องทำ#โค...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4848</th>\n",
              "      <td>อยากฉีด #โมเดอร์นา ฟรี!!!! \\n⚠FREE⚠</td>\n",
              "      <td>เดือดร้อน</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2436</th>\n",
              "      <td>@vanishkul นิวซีแลนด์มีประชากร 7 ล้าน ยังฉีดวั...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8201</th>\n",
              "      <td>ถ้าไม่มีโควิดคือทั้งพี่ทั้งน้องต้องปังมากแน่ๆ....</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5088</th>\n",
              "      <td>@onmyworlds เราก็เห็นด้วยนะคะ บางคนคือรอนานจนข...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1875</th>\n",
              "      <td>นักศึกษาหอบ-ตรวจพบเชื้อโควิดสายพันธุ์อินเดีย-เ...</td>\n",
              "      <td>เดือดร้อน</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3024</th>\n",
              "      <td>ตั้งแต่มีโรคโควิดเข้ามา กลายเป็นคนอยู่ติดบ้าน ...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6958</th>\n",
              "      <td>เหนื่อยใจกับการบริหารจัดการ สงสารคนในเรือนจำ ถ...</td>\n",
              "      <td>เดือดร้อน</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f63ce77b-4d0f-4c82-9861-fda49774bc0e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f63ce77b-4d0f-4c82-9861-fda49774bc0e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f63ce77b-4d0f-4c82-9861-fda49774bc0e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                               sentence     sentiment\n",
              "8074  คิดถึงพี่น้องน้ำ 😊🥰🤟❤😷😷\\nป.โทจะไม่จบเพราะดูแต่...  ไม่เดือดร้อน\n",
              "2408  สถานการณ์การติดเชื้อโควิด-19 ในประเทศ วันที่ 1...  ไม่เดือดร้อน\n",
              "9844  ทำงานรอกันด้วยค่ะเพื่อน\\n\\n#งานก็ต้องค้องทำ#โค...  ไม่เดือดร้อน\n",
              "4848                อยากฉีด #โมเดอร์นา ฟรี!!!! \\n⚠FREE⚠     เดือดร้อน\n",
              "2436  @vanishkul นิวซีแลนด์มีประชากร 7 ล้าน ยังฉีดวั...  ไม่เดือดร้อน\n",
              "8201  ถ้าไม่มีโควิดคือทั้งพี่ทั้งน้องต้องปังมากแน่ๆ....  ไม่เดือดร้อน\n",
              "5088  @onmyworlds เราก็เห็นด้วยนะคะ บางคนคือรอนานจนข...  ไม่เดือดร้อน\n",
              "1875  นักศึกษาหอบ-ตรวจพบเชื้อโควิดสายพันธุ์อินเดีย-เ...     เดือดร้อน\n",
              "3024  ตั้งแต่มีโรคโควิดเข้ามา กลายเป็นคนอยู่ติดบ้าน ...  ไม่เดือดร้อน\n",
              "6958  เหนื่อยใจกับการบริหารจัดการ สงสารคนในเรือนจำ ถ...     เดือดร้อน"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "sentiment_df = pd.concat([ไม่เดือดร้อน_df, เดือดร้อน_df])\n",
        "sentiment_df.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcipIrCXbyao",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "6c0c8cde-be10-483e-880e-7546618237d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a7d690ab-61ee-45ec-90c9-0be312c70eda\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>clean_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2568</th>\n",
              "      <td>@missPM92 กลัวโควิด😆😆😆</td>\n",
              "      <td>เดือดร้อน</td>\n",
              "      <td>@misspm92 กลัวโควิด😆😆😆</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8208</th>\n",
              "      <td>ชีวิตไม่มีไรเลยนะช่วงนี้มีแต่กักตัวตรวจโควิด</td>\n",
              "      <td>เดือดร้อน</td>\n",
              "      <td>ชีวิตไม่มีไรเลยนะช่วงนี้มีแต่กักตัวตรวจโควิด</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323</th>\n",
              "      <td>สาระน่ารู้” 8ขั้นตอน ฉีดวัคซีนโควิด-19 ภายใน 3...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "      <td>สาระน่ารู้” 8ขั้นตอน ฉีดวัคซีนโควิด-19 ภายใน 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7656</th>\n",
              "      <td>คือเป็นไรกะฟ้าทลายโจรเบ๋อ คือนี่ไม่สั่งให้คนไข...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "      <td>คือเป็นไรกะฟ้าทลายโจรเบ๋อ คือนี่ไม่สั่งให้คนไข...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2952</th>\n",
              "      <td>@PlsBemyself โควิดหายแล้วเราเจอกัน</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "      <td>@plsbemyself โควิดหายแล้วเราเจอกัน</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7d690ab-61ee-45ec-90c9-0be312c70eda')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a7d690ab-61ee-45ec-90c9-0be312c70eda button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a7d690ab-61ee-45ec-90c9-0be312c70eda');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                               sentence  ...                                     clean_sentence\n",
              "2568                             @missPM92 กลัวโควิด😆😆😆  ...                             @misspm92 กลัวโควิด😆😆😆\n",
              "8208       ชีวิตไม่มีไรเลยนะช่วงนี้มีแต่กักตัวตรวจโควิด  ...       ชีวิตไม่มีไรเลยนะช่วงนี้มีแต่กักตัวตรวจโควิด\n",
              "323   สาระน่ารู้” 8ขั้นตอน ฉีดวัคซีนโควิด-19 ภายใน 3...  ...  สาระน่ารู้” 8ขั้นตอน ฉีดวัคซีนโควิด-19 ภายใน 3...\n",
              "7656  คือเป็นไรกะฟ้าทลายโจรเบ๋อ คือนี่ไม่สั่งให้คนไข...  ...  คือเป็นไรกะฟ้าทลายโจรเบ๋อ คือนี่ไม่สั่งให้คนไข...\n",
              "2952                 @PlsBemyself โควิดหายแล้วเราเจอกัน  ...                 @plsbemyself โควิดหายแล้วเราเจอกัน\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "sentiment_df['clean_sentence'] = sentiment_df['sentence'].fillna('').apply(lambda x: x.lower())\n",
        "sentiment_df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZNghX0ZbzFc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "684ecd00-a566-4d7d-e242-8261d63c8191"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\"#\\'()*,-.;<=>[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "pun = '\"#\\'()*,-.;<=>[\\\\]^_`{|}~'\n",
        "pun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMCpsIS7b1SA"
      },
      "outputs": [],
      "source": [
        "sentiment_df['clean_sentence'] = sentiment_df['clean_sentence'].str.replace(r'[%s]' % (pun), '', regex=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIfujYvIHU0q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "70ae47f2-deb3-40c3-f1e8-2b96ddaea87b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-537f9ae8-aa34-4656-b345-74cac8a31ccf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>clean_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5681</th>\n",
              "      <td>ศบค.เผยไทยติดเชื้อโควิด-19 เพิ่ม 90 ราย ยอดสะส...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "      <td>ศบคเผยไทยติดเชื้อโควิด19 เพิ่ม 90 ราย ยอดสะสม ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6980</th>\n",
              "      <td>เรามีช่อง​ทางกลับบ้านแบบปลอดภัยต่อตัวเองและคนร...</td>\n",
              "      <td>เดือดร้อน</td>\n",
              "      <td>เรามีช่อง​ทางกลับบ้านแบบปลอดภัยต่อตัวเองและคนร...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5583</th>\n",
              "      <td>ต้องแมสนะคะ ป้องกันโควิด น้องวินอย่าลืมใส่แมสน...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "      <td>ต้องแมสนะคะ ป้องกันโควิด น้องวินอย่าลืมใส่แมสน...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9659</th>\n",
              "      <td>พูดกันตามตรงก็คืออยากเห็นเค้าเล่นซีรี่ย์จีน หว...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "      <td>พูดกันตามตรงก็คืออยากเห็นเค้าเล่นซีรี่ย์จีน หว...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7026</th>\n",
              "      <td>บำบัดความเงี่ยนกัน​ ช่วงโควิด​\\nหากเป็นคุณจะเล...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "      <td>บำบัดความเงี่ยนกัน​ ช่วงโควิด​\\nหากเป็นคุณจะเล...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-537f9ae8-aa34-4656-b345-74cac8a31ccf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-537f9ae8-aa34-4656-b345-74cac8a31ccf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-537f9ae8-aa34-4656-b345-74cac8a31ccf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                               sentence  ...                                     clean_sentence\n",
              "5681  ศบค.เผยไทยติดเชื้อโควิด-19 เพิ่ม 90 ราย ยอดสะส...  ...  ศบคเผยไทยติดเชื้อโควิด19 เพิ่ม 90 ราย ยอดสะสม ...\n",
              "6980  เรามีช่อง​ทางกลับบ้านแบบปลอดภัยต่อตัวเองและคนร...  ...  เรามีช่อง​ทางกลับบ้านแบบปลอดภัยต่อตัวเองและคนร...\n",
              "5583  ต้องแมสนะคะ ป้องกันโควิด น้องวินอย่าลืมใส่แมสน...  ...  ต้องแมสนะคะ ป้องกันโควิด น้องวินอย่าลืมใส่แมสน...\n",
              "9659  พูดกันตามตรงก็คืออยากเห็นเค้าเล่นซีรี่ย์จีน หว...  ...  พูดกันตามตรงก็คืออยากเห็นเค้าเล่นซีรี่ย์จีน หว...\n",
              "7026  บำบัดความเงี่ยนกัน​ ช่วงโควิด​\\nหากเป็นคุณจะเล...  ...  บำบัดความเงี่ยนกัน​ ช่วงโควิด​\\nหากเป็นคุณจะเล...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "sentiment_df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZtWUH-GHVUl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38459625-614c-42da-b0a1-c8225ae80d7c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "62051"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "custom_words_list = set(thai_words())\n",
        "len(custom_words_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLh8Vm7KHVZl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42dc7f0f-04f9-491c-e1ad-6dc018844353"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['โอเค', 'บ่', 'พวกเรา', 'รัก', 'ภาษา', 'บ้านเกิด']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "text = \"โอเคบ่พวกเรารักภาษาบ้านเกิด\" \n",
        "custom_tokenizer = Tokenizer(custom_words_list)\n",
        "custom_tokenizer.word_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzcVaae1HVfh"
      },
      "outputs": [],
      "source": [
        "sentiment_df['clean_sentence'] = sentiment_df['clean_sentence'].apply(lambda x: custom_tokenizer.word_tokenize(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHOMU_QxHbky",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "ad1b6f08-f825-4264-f21d-738e15752a20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-120dcca0-5f2b-492a-bf40-731032e28bbe\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>clean_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>771</th>\n",
              "      <td>อยากไปเป็นครูอาสาบนดอยจัง ถ้าไม่ติดโควิดมันคงไ...</td>\n",
              "      <td>เดือดร้อน</td>\n",
              "      <td>[อยาก, ไป, เป็น, ครู, อาสา, บน, ดอย, จัง,  , ถ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5650</th>\n",
              "      <td>ยืนยันตัวเลขผู้ติดเชื้อ#โควิด19  ในประเทศไทยวั...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "      <td>[ยืนยัน, ตัวเลข, ผู้, ติดเชื้อ, โควิด, 19,   ,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>695</th>\n",
              "      <td>มูลนิธิป่อเต็กตึ๊ง ส่งต่อธารน้ำใจ สู้ภัยโควิด-...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "      <td>[มูลนิธิป่อเต็กตึ๊ง,  , ส่งต่อ, ธาร, น้ำใจ,  ,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5045</th>\n",
              "      <td>เอ่ออ... แต่โควิดก็เริ่มซาลง(มั้ง)และคือวันนี้...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "      <td>[เอ่อ, อ,  , แต่, โควิด, ก็, เริ่ม, ซาลง, มั้ง...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8756</th>\n",
              "      <td>ด่วน! โควิดมากับเพื่อน\\n\\n13:08 ศูนย์ปฏิบัติกา...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "      <td>[ด่วน, !,  , โควิด, มา, กับ, เพื่อน, \\n, \\n, 1...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-120dcca0-5f2b-492a-bf40-731032e28bbe')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-120dcca0-5f2b-492a-bf40-731032e28bbe button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-120dcca0-5f2b-492a-bf40-731032e28bbe');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                               sentence  ...                                     clean_sentence\n",
              "771   อยากไปเป็นครูอาสาบนดอยจัง ถ้าไม่ติดโควิดมันคงไ...  ...  [อยาก, ไป, เป็น, ครู, อาสา, บน, ดอย, จัง,  , ถ...\n",
              "5650  ยืนยันตัวเลขผู้ติดเชื้อ#โควิด19  ในประเทศไทยวั...  ...  [ยืนยัน, ตัวเลข, ผู้, ติดเชื้อ, โควิด, 19,   ,...\n",
              "695   มูลนิธิป่อเต็กตึ๊ง ส่งต่อธารน้ำใจ สู้ภัยโควิด-...  ...  [มูลนิธิป่อเต็กตึ๊ง,  , ส่งต่อ, ธาร, น้ำใจ,  ,...\n",
              "5045  เอ่ออ... แต่โควิดก็เริ่มซาลง(มั้ง)และคือวันนี้...  ...  [เอ่อ, อ,  , แต่, โควิด, ก็, เริ่ม, ซาลง, มั้ง...\n",
              "8756  ด่วน! โควิดมากับเพื่อน\\n\\n13:08 ศูนย์ปฏิบัติกา...  ...  [ด่วน, !,  , โควิด, มา, กับ, เพื่อน, \\n, \\n, 1...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "sentiment_df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuY7-vOYHc-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5e77dbc-15fa-4875-8c61-d4eaaaeee097"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2740    [อัปเดต, สถานการณ์,  , โควิด, 19,  , วันนี้,  ...\n",
              "1303                         [ล่าสุด, ต่อ, ประกัน, โควิด]\n",
              "8399    [สถานการณ์, การ, ติดเชื้อ,  , covid, 19,  , ใน...\n",
              "8647    [ด่วน, !,  , \\n, \\n, 22, :, 38,  , ศูนย์ปฏิบัต...\n",
              "2436    [@, vanishkul,  , นิวซีแลนด์, มี, ประชากร,  , ...\n",
              "1344    [เช้านี้, อยู่, เกาะ,  , เตรียม, แมส, พร้อม, อ...\n",
              "4189    [รอ, คิว, ตัดผม, ใน, ห้าง, โลตัส, ​,   , คิว, ...\n",
              "8309    [@, popefc, 01,  , ดี, จัง, ผม, หนา,  , ทาง, น...\n",
              "8137    [วันที่,  , 3,  , ตุลาคม,  , 2564, \\n, ฮุก, 31...\n",
              "4680    [รอบ, บ่าย,  , ลง, พื้นที่, ตรวจ, โควิด, \\n, ร...\n",
              "Name: clean_sentence, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "tokenized_doc = sentiment_df['clean_sentence']\n",
        "tokenized_doc[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uw2jCaYVHgG8"
      },
      "outputs": [],
      "source": [
        "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1H1kq55EHg7N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a741083f-f441-425e-d3d6-ea809ab98719"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2740    [อัปเดต, สถานการณ์,  , โควิด, 19,  , วันนี้,  ...\n",
              "1303                         [ล่าสุด, ต่อ, ประกัน, โควิด]\n",
              "8399    [สถานการณ์, การ, ติดเชื้อ,  , covid, 19,  , ใน...\n",
              "8647    [ด่วน, !,  , \\n, \\n, 22, :, 38,  , ศูนย์ปฏิบัต...\n",
              "2436    [@, vanishkul,  , นิวซีแลนด์, มี, ประชากร,  , ...\n",
              "1344    [เช้านี้, อยู่, เกาะ,  , เตรียม, แมส, พร้อม, อ...\n",
              "4189    [รอ, คิว, ตัดผม, ใน, ห้าง, โลตัส, ​,   , คิว, ...\n",
              "8309    [@, popefc, 01,  , ดี, จัง, ผม, หนา,  , ทาง, น...\n",
              "8137    [วันที่,  , 3,  , ตุลาคม,  , 2564, \\n, ฮุก, 31...\n",
              "4680    [รอบ, บ่าย,  , ลง, พื้นที่, ตรวจ, โควิด, \\n, ร...\n",
              "Name: clean_sentence, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "tokenized_doc[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y66TXVEeHg91"
      },
      "outputs": [],
      "source": [
        "tokenized_doc = tokenized_doc.to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WorjfqIjHg_-"
      },
      "outputs": [],
      "source": [
        "# de-tokenization\n",
        "detokenized_doc = []\n",
        "for i in range(len(tokenized_doc)):\n",
        "#     print(tokenized_doc[i])\n",
        "    t = ' '.join(tokenized_doc[i])\n",
        "    detokenized_doc.append(t)\n",
        "    \n",
        "sentiment_df['clean_sentence'] = detokenized_doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xquzN-HHmhK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "413574f0-156a-4810-c4d3-8e809b4b4616"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-125a1ae8-92c2-4cb8-94a2-15493c581383\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>clean_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2740</th>\n",
              "      <td>อัปเดตสถานการณ์ #โควิด19 วันนี้ (26 มิ.ย. 64) ...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "      <td>อัปเดต สถานการณ์   โควิด 19   วันนี้   26   มิ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1303</th>\n",
              "      <td>ล่าสุดต่อประกันโควิด</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "      <td>ล่าสุด ต่อ ประกัน โควิด</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8399</th>\n",
              "      <td>สถานการณ์การติดเชื้อ COVID-19 ในประเทศ วันที่ ...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "      <td>สถานการณ์ การ ติดเชื้อ   covid 19   ในประเทศ  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8647</th>\n",
              "      <td>ด่วน! \\n\\n22:38 ศูนย์ปฏิบัติการ COVID-19 จังหว...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "      <td>ด่วน !   \\n \\n 22 : 38   ศูนย์ปฏิบัติการ   cov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2436</th>\n",
              "      <td>@vanishkul นิวซีแลนด์มีประชากร 7 ล้าน ยังฉีดวั...</td>\n",
              "      <td>ไม่เดือดร้อน</td>\n",
              "      <td>@ vanishkul   นิวซีแลนด์ มี ประชากร   7   ล้าน...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-125a1ae8-92c2-4cb8-94a2-15493c581383')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-125a1ae8-92c2-4cb8-94a2-15493c581383 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-125a1ae8-92c2-4cb8-94a2-15493c581383');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                               sentence  ...                                     clean_sentence\n",
              "2740  อัปเดตสถานการณ์ #โควิด19 วันนี้ (26 มิ.ย. 64) ...  ...  อัปเดต สถานการณ์   โควิด 19   วันนี้   26   มิ...\n",
              "1303                               ล่าสุดต่อประกันโควิด  ...                            ล่าสุด ต่อ ประกัน โควิด\n",
              "8399  สถานการณ์การติดเชื้อ COVID-19 ในประเทศ วันที่ ...  ...  สถานการณ์ การ ติดเชื้อ   covid 19   ในประเทศ  ...\n",
              "8647  ด่วน! \\n\\n22:38 ศูนย์ปฏิบัติการ COVID-19 จังหว...  ...  ด่วน !   \\n \\n 22 : 38   ศูนย์ปฏิบัติการ   cov...\n",
              "2436  @vanishkul นิวซีแลนด์มีประชากร 7 ล้าน ยังฉีดวั...  ...  @ vanishkul   นิวซีแลนด์ มี ประชากร   7   ล้าน...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "sentiment_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lLV8XM4HmrI"
      },
      "outputs": [],
      "source": [
        "cleaned_words = sentiment_df['clean_sentence'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcnls2WgHmwF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6de5301f-7a0e-4621-d987-57163c50c9af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['อัปเดต สถานการณ์   โควิด 19   วันนี้   26   มิ ย   64    เสียชีวิต   51   ราย   ผู้ป่วย ใหม่ รวม   4161   ราย \\n \\n จำแนก เป็น \\n ติดเชื้อ ใหม่   4089   ราย \\n ติดเชื้อ ภายใน เรือนจำ / ที่ ต้อง ขัง   72   ราย \\n หายป่วย กลับบ้าน   3569   ราย \\n ผู้ป่วย สะสม   211589   ราย   ตั้งแต่   1   เมษายน   https :// tco / q 0 zvz 9 snw 7']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "cleaned_words[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYLRPaNzHmyz"
      },
      "outputs": [],
      "source": [
        "def create_tokenizer(words, filters = ''):\n",
        "    token = KRTokenizer()\n",
        "    token.fit_on_texts(words)\n",
        "    return token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWbLtU2TojOC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8034e7c-083e-404b-ed0a-2d9984bd2d2c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'โควิด': 1,\n",
              " 'https': 2,\n",
              " 'tco': 3,\n",
              " '19': 4,\n",
              " 'ไม่': 5,\n",
              " 'ที่': 6,\n",
              " 'ไป': 7,\n",
              " 'ได้': 8,\n",
              " 'มี': 9,\n",
              " 'ก็': 10,\n",
              " '\\u200b': 11,\n",
              " 'จะ': 12,\n",
              " 'มา': 13,\n",
              " 'แล้ว': 14,\n",
              " 'คน': 15,\n",
              " 'ให้': 16,\n",
              " 'ใน': 17,\n",
              " 'ราย': 18,\n",
              " 'เลย': 19,\n",
              " 'เป็น': 20,\n",
              " 'covid': 21,\n",
              " 'ต้อง': 22,\n",
              " 'วันนี้': 23,\n",
              " 'แต่': 24,\n",
              " 'และ': 25,\n",
              " 'ติด': 26,\n",
              " 'ๆ': 27,\n",
              " 'นะ': 28,\n",
              " '1': 29,\n",
              " 'ว่า': 30,\n",
              " 'เรา': 31,\n",
              " 'ของ': 32,\n",
              " 'การ': 33,\n",
              " '2': 34,\n",
              " 'ไทย': 35,\n",
              " 'กัน': 36,\n",
              " 'นี้': 37,\n",
              " '3': 38,\n",
              " 'คือ': 39,\n",
              " 'จาก': 40,\n",
              " 'วัคซีน': 41,\n",
              " 'อยู่': 42,\n",
              " '4': 43,\n",
              " 'มาก': 44,\n",
              " 'กับ': 45,\n",
              " 'ยัง': 46,\n",
              " 'ติดเชื้อ': 47,\n",
              " 'มัน': 48,\n",
              " 'ครับ': 49,\n",
              " 'บ้าน': 50,\n",
              " '5': 51,\n",
              " 'เพราะ': 52,\n",
              " '7': 53,\n",
              " 'ตรวจ': 54,\n",
              " 'ค่ะ': 55,\n",
              " 'ดี': 56,\n",
              " '8': 57,\n",
              " '6': 58,\n",
              " 'อีก': 59,\n",
              " 'อยาก': 60,\n",
              " 'ใหม่': 61,\n",
              " '9': 62,\n",
              " 'ถ้า': 63,\n",
              " 'วัน': 64,\n",
              " '0': 65,\n",
              " 'ด้วย': 66,\n",
              " 'ผู้ป่วย': 67,\n",
              " 'ทำ': 68,\n",
              " 'นวด': 69,\n",
              " 'ประเทศ': 70,\n",
              " 'ก่อน': 71,\n",
              " 'ละ': 72,\n",
              " 'หมด': 73,\n",
              " 'อะไร': 74,\n",
              " 'ผู้': 75,\n",
              " 'ปี': 76,\n",
              " 'กู': 77,\n",
              " 'แบบ': 78,\n",
              " 'นะคะ': 79,\n",
              " 'สถานการณ์': 80,\n",
              " 'กว่า': 81,\n",
              " 'กลัว': 82,\n",
              " 'ฉีด': 83,\n",
              " 'เอา': 84,\n",
              " 'รับ': 85,\n",
              " 'ใคร': 86,\n",
              " 'รอบ': 87,\n",
              " 'นี่': 88,\n",
              " 'สู้': 89,\n",
              " 'แล': 90,\n",
              " 'ตาย': 91,\n",
              " 'ส่ง': 92,\n",
              " 'รอ': 93,\n",
              " 'ฉีดวัคซีน': 94,\n",
              " 'ตอนนี้': 95,\n",
              " 'ถึง': 96,\n",
              " 'หรือ': 97,\n",
              " 'เพื่อ': 98,\n",
              " 'งาน': 99,\n",
              " 'ไหน': 100,\n",
              " 'ช่วง': 101,\n",
              " 'หาย': 102,\n",
              " 'เจอ': 103,\n",
              " 'ออก': 104,\n",
              " 'เสียชีวิต': 105,\n",
              " 'แม่': 106,\n",
              " 'วันที่': 107,\n",
              " 'จน': 108,\n",
              " 'เพิ่ม': 109,\n",
              " 'ดู': 110,\n",
              " 'พี่': 111,\n",
              " 'อี': 112,\n",
              " '64': 113,\n",
              " 'เรื่อง': 114,\n",
              " '2564': 115,\n",
              " 'ถูก': 116,\n",
              " 'รัฐบาล': 117,\n",
              " 'รู้': 118,\n",
              " 'ระบาด': 119,\n",
              " 'หมอ': 120,\n",
              " 'เมือง': 121,\n",
              " 'แค่': 122,\n",
              " 'ขอ': 123,\n",
              " 'คิด': 124,\n",
              " 'ค่า': 125,\n",
              " 'เยอะ': 126,\n",
              " 'อย่าง': 127,\n",
              " 'เข้า': 128,\n",
              " 'ตั้งแต่': 129,\n",
              " 'ยอด': 130,\n",
              " 'ไว้': 131,\n",
              " 'พอ': 132,\n",
              " 'ใจ': 133,\n",
              " 'ข่าว': 134,\n",
              " 'ใช้': 135,\n",
              " 'เปิด': 136,\n",
              " 'ออนไลน์': 137,\n",
              " 'บาท': 138,\n",
              " 'รักษา': 139,\n",
              " 'เดือน': 140,\n",
              " '10': 141,\n",
              " 'น้อง': 142,\n",
              " 'พบ': 143,\n",
              " 'ต่อ': 144,\n",
              " 'ประชาชน': 145,\n",
              " 'ช่วย': 146,\n",
              " 'ซื้อ': 147,\n",
              " 'มึง': 148,\n",
              " 'ปลอดภัย': 149,\n",
              " 'ผ่าน': 150,\n",
              " 'ตลาดนัด': 151,\n",
              " 'หา': 152,\n",
              " 'จังหวัด': 153,\n",
              " 'พร้อม': 154,\n",
              " 'สะสม': 155,\n",
              " 'โค': 156,\n",
              " 'ฟรี': 157,\n",
              " 'รพ': 158,\n",
              " 'ปิด': 159,\n",
              " 'ทั้ง': 160,\n",
              " 'จริงๆ': 161,\n",
              " 'บอก': 162,\n",
              " 'ใช่': 163,\n",
              " 'ทุกคน': 164,\n",
              " 'ทำงาน': 165,\n",
              " 'อย่า': 166,\n",
              " 'นา': 167,\n",
              " 'กิน': 168,\n",
              " 'แบบนี้': 169,\n",
              " 'เขา': 170,\n",
              " 'เค้า': 171,\n",
              " 'รวม': 172,\n",
              " 'น': 173,\n",
              " 'ตัวเอง': 174,\n",
              " 'ร้าน': 175,\n",
              " 'อ': 176,\n",
              " 'เห็น': 177,\n",
              " 'ยุค': 178,\n",
              " 'คง': 179,\n",
              " 'กุ': 180,\n",
              " 'ทำให้': 181,\n",
              " 'คุณ': 182,\n",
              " 'รอด': 183,\n",
              " 'บ้าง': 184,\n",
              " 'ชีวิต': 185,\n",
              " '”': 186,\n",
              " 'ตัว': 187,\n",
              " 'อ่ะ': 188,\n",
              " 'เสี่ยง': 189,\n",
              " 'รับงาน': 190,\n",
              " '\\xa0': 191,\n",
              " 'โรค': 192,\n",
              " 'หลาย': 193,\n",
              " '์': 194,\n",
              " 'บาง': 195,\n",
              " 'ขาย': 196,\n",
              " 'จ้า': 197,\n",
              " 'เชื้อ': 198,\n",
              " 'ทุก': 199,\n",
              " 'อะ': 200,\n",
              " 'ประกัน': 201,\n",
              " 'ห้อง': 202,\n",
              " 'ขอให้': 203,\n",
              " 'ภัย': 204,\n",
              " 'แรก': 205,\n",
              " 'ขึ้น': 206,\n",
              " 'ย': 207,\n",
              " 'หน้า': 208,\n",
              " 'ง': 209,\n",
              " 'ไวรัส': 210,\n",
              " 'ตาม': 211,\n",
              " 'ชนะ': 212,\n",
              " 'ไหม': 213,\n",
              " 'เอง': 214,\n",
              " 'ลง': 215,\n",
              " 'psub': 216,\n",
              " 'psubroadcast': 217,\n",
              " 'จริง': 218,\n",
              " 'เวลา': 219,\n",
              " 'ป้องกัน': 220,\n",
              " 'เหมือน': 221,\n",
              " 'เงิน': 222,\n",
              " 'อร': 223,\n",
              " 'เริ่ม': 224,\n",
              " 'ลด': 225,\n",
              " 'ตอน': 226,\n",
              " 'จ': 227,\n",
              " 'โดน': 228,\n",
              " 'หลัง': 229,\n",
              " 'เคย': 230,\n",
              " 'เข็ม': 231,\n",
              " 'นั่ง': 232,\n",
              " 'ช่วงนี้': 233,\n",
              " 'โรงพยาบาล': 234,\n",
              " 'มั้ย': 235,\n",
              " 'เด': 236,\n",
              " 'ใส่': 237,\n",
              " 'เหี้ย': 238,\n",
              " 'ราคา': 239,\n",
              " 'คนไทย': 240,\n",
              " 'รัก': 241,\n",
              " 'ในประเทศ': 242,\n",
              " 'ผม': 243,\n",
              " 'คะ': 244,\n",
              " 'ทำไม': 245,\n",
              " 'ล้าน': 246,\n",
              " 'โม': 247,\n",
              " 'ควร': 248,\n",
              " 'หยุด': 249,\n",
              " 'เด็ก': 250,\n",
              " 'กลับ': 251,\n",
              " 'งง': 252,\n",
              " 'หนัก': 253,\n",
              " 'สุขภาพ': 254,\n",
              " 'หายป่วย': 255,\n",
              " 'กลับมา': 256,\n",
              " 'หรอ': 257,\n",
              " 'ชุด': 258,\n",
              " 'เพื่อน': 259,\n",
              " 'นนทบุรี': 260,\n",
              " 'ทาง': 261,\n",
              " 'ท': 262,\n",
              " 'ขนาด': 263,\n",
              " 'อาการ': 264,\n",
              " 'ยังไง': 265,\n",
              " 'พื้นที่': 266,\n",
              " 'ผล': 267,\n",
              " 'เตียง': 268,\n",
              " 'live': 269,\n",
              " 'ใกล้': 270,\n",
              " '“': 271,\n",
              " 'คิดถึง': 272,\n",
              " 'กรุงเทพ': 273,\n",
              " 'รถ': 274,\n",
              " 'เรียน': 275,\n",
              " 'วิ': 276,\n",
              " 'ค': 277,\n",
              " 'เ': 278,\n",
              " 'เศรษฐกิจ': 279,\n",
              " 'บอกต่อ': 280,\n",
              " 'ครอบครัว': 281,\n",
              " 'รัฐ': 282,\n",
              " 'สั่ง': 283,\n",
              " 'กก': 284,\n",
              " 'ได้รับ': 285,\n",
              " 'กำลัง': 286,\n",
              " 'ไลน์': 287,\n",
              " 'ข้อมูล': 288,\n",
              " 'พวก': 289,\n",
              " 'ฉัน': 290,\n",
              " 'เชียงใหม่': 291,\n",
              " 'กลับบ้าน': 292,\n",
              " '😷': 293,\n",
              " 'ศูนย์': 294,\n",
              " 'ออ': 295,\n",
              " '✅': 296,\n",
              " 'สุด': 297,\n",
              " 'บริการ': 298,\n",
              " 'ถาม': 299,\n",
              " 'โดย': 300,\n",
              " 'สมุทรสาคร': 301,\n",
              " 'บอ': 302,\n",
              " 'ไม่ต้อง': 303,\n",
              " 'รีวิว': 304,\n",
              " 'เดินทาง': 305,\n",
              " 'สอง': 306,\n",
              " 'แพร่ระบาด': 307,\n",
              " 'ไง': 308,\n",
              " 'หน่อย': 309,\n",
              " 'ขายของ': 310,\n",
              " 'ที่สุด': 311,\n",
              " 'สนใจ': 312,\n",
              " 'หลัก': 313,\n",
              " 'ทุกวัน': 314,\n",
              " 'q': 315,\n",
              " 'ด่วน': 316,\n",
              " 'ม': 317,\n",
              " 'ร้อย': 318,\n",
              " 'กักตัว': 319,\n",
              " 'กลุ่ม': 320,\n",
              " 'นั้น': 321,\n",
              " 'ไฟ': 322,\n",
              " 'ความ': 323,\n",
              " 'ส้นตีน': 324,\n",
              " 'ขอบคุณ': 325,\n",
              " 'x': 326,\n",
              " 'ยา': 327,\n",
              " 'เมื่อ': 328,\n",
              " 'จ่าย': 329,\n",
              " 'ตลาด': 330,\n",
              " 'เที่ยว': 331,\n",
              " 'ดูแล': 332,\n",
              " 'มิ': 333,\n",
              " 'จำนวน': 334,\n",
              " 'สามารถ': 335,\n",
              " 'ล่ะ': 336,\n",
              " 'สงขลา': 337,\n",
              " 'แจ้ง': 338,\n",
              " 'นาน': 339,\n",
              " 'เลื่อน': 340,\n",
              " 'สวัสดี': 341,\n",
              " 'สำหรับ': 342,\n",
              " 'โลก': 343,\n",
              " 'line': 344,\n",
              " 'หน้ากาก': 345,\n",
              " 'นอน': 346,\n",
              " 'ส่วน': 347,\n",
              " 'ที': 348,\n",
              " 'ต': 349,\n",
              " 'จบ': 350,\n",
              " 'ประกาศ': 351,\n",
              " 'ทัน': 352,\n",
              " '📌': 353,\n",
              " 'คุย': 354,\n",
              " 'บริหาร': 355,\n",
              " 'ชั้น': 356,\n",
              " 'เบื่อ': 357,\n",
              " 'นัด': 358,\n",
              " 'จัด': 359,\n",
              " 'น้อย': 360,\n",
              " '…': 361,\n",
              " '25': 362,\n",
              " '20': 363,\n",
              " 'f': 364,\n",
              " 'ลูกค้า': 365,\n",
              " 'เดียว': 366,\n",
              " 'น่าจะ': 367,\n",
              " 'ลูก': 368,\n",
              " 'ยืนยัน': 369,\n",
              " 'อนามัย': 370,\n",
              " 'สูง': 371,\n",
              " 'กทม': 372,\n",
              " '12': 373,\n",
              " 'วะ': 374,\n",
              " 'ออกมา': 375,\n",
              " 'ปกติ': 376,\n",
              " 'เถอะ': 377,\n",
              " 'ขาว': 378,\n",
              " 'ชา': 379,\n",
              " 'เมื่อไหร่': 380,\n",
              " 'นึง': 381,\n",
              " 'ป่วย': 382,\n",
              " 'ว': 383,\n",
              " 'ไร': 384,\n",
              " 'ชาติ': 385,\n",
              " 'สด': 386,\n",
              " '⏩': 387,\n",
              " 'ม็อบ': 388,\n",
              " '😭': 389,\n",
              " 'ใหญ่': 390,\n",
              " 'แมส': 391,\n",
              " 'n': 392,\n",
              " 'p': 393,\n",
              " 'ดีขึ้น': 394,\n",
              " 'ร้า': 395,\n",
              " 'ท่าน': 396,\n",
              " 'มากกว่า': 397,\n",
              " 'เกิด': 398,\n",
              " 'อายุ': 399,\n",
              " 'ทวิ': 400,\n",
              " 'ชอบ': 401,\n",
              " 'เข้ามา': 402,\n",
              " 'กล่อง': 403,\n",
              " 'gt': 404,\n",
              " 'มาจาก': 405,\n",
              " 'ฯ': 406,\n",
              " 'จอง': 407,\n",
              " 'พ่อ': 408,\n",
              " 'เกิน': 409,\n",
              " 'ep': 410,\n",
              " 'c': 411,\n",
              " 'ล': 412,\n",
              " 'สร้าง': 413,\n",
              " 'ด้วยกัน': 414,\n",
              " 'j': 415,\n",
              " 'เลือก': 416,\n",
              " 'b': 417,\n",
              " 'บริษัท': 418,\n",
              " 'ที่จะ': 419,\n",
              " 'เดิน': 420,\n",
              " 'ตลอด': 421,\n",
              " 'น่า': 422,\n",
              " 'เซอร์': 423,\n",
              " 'คิว': 424,\n",
              " 'ส': 425,\n",
              " 'ซึ่ง': 426,\n",
              " 'ครบ': 427,\n",
              " '14': 428,\n",
              " 'โปร': 429,\n",
              " 'ยิ่ง': 430,\n",
              " '100': 431,\n",
              " 'อิ': 432,\n",
              " 'พระ': 433,\n",
              " 'ล่าสุด': 434,\n",
              " '13': 435,\n",
              " 'สอน': 436,\n",
              " 'จุด': 437,\n",
              " 'โทร': 438,\n",
              " 'อีกแล้ว': 439,\n",
              " 'l': 440,\n",
              " 'กา': 441,\n",
              " 'ประยุทธ์': 442,\n",
              " 'ภายใน': 443,\n",
              " 'ทีม': 444,\n",
              " 'ร่วม': 445,\n",
              " 'แอด': 446,\n",
              " 'หนึ่ง': 447,\n",
              " 'ห้าม': 448,\n",
              " 'คอ': 449,\n",
              " 'พูด': 450,\n",
              " 'ดาวน์': 451,\n",
              " 'สนาม': 452,\n",
              " 'ทอง': 453,\n",
              " '555': 454,\n",
              " 'เหรอ': 455,\n",
              " 'เร็ว': 456,\n",
              " 'รถเช่า': 457,\n",
              " 'โร': 458,\n",
              " 'ควบคุม': 459,\n",
              " 'ระบบ': 460,\n",
              " 'ระวัง': 461,\n",
              " 'เจอกัน': 462,\n",
              " 'จีน': 463,\n",
              " 'เล่น': 464,\n",
              " 'k': 465,\n",
              " 'เนี่ย': 466,\n",
              " 'น้ำ': 467,\n",
              " 'เข้าใจ': 468,\n",
              " 'เรือนจำ': 469,\n",
              " 'ยาว': 470,\n",
              " 'เครียด': 471,\n",
              " 'หนู': 472,\n",
              " 'ก็ได้': 473,\n",
              " 'นอก': 474,\n",
              " 'u': 475,\n",
              " 'แย่': 476,\n",
              " 'ที่อยู่': 477,\n",
              " 'v': 478,\n",
              " 'รู้สึก': 479,\n",
              " 'อ่าน': 480,\n",
              " 'แถว': 481,\n",
              " 'สาย': 482,\n",
              " 'สรุป': 483,\n",
              " 'สะดวก': 484,\n",
              " '2019': 485,\n",
              " 'น่ากลัว': 486,\n",
              " 'z': 487,\n",
              " 'ฝาก': 488,\n",
              " 'บุคลากร': 489,\n",
              " 'thailand': 490,\n",
              " 'จัดการ': 491,\n",
              " 'จัง': 492,\n",
              " 'หมื่น': 493,\n",
              " 'แพ้': 494,\n",
              " 'เหนื่อย': 495,\n",
              " 'ก': 496,\n",
              " '18': 497,\n",
              " 'ทุกอย่าง': 498,\n",
              " 'พัก': 499,\n",
              " 'คนใน': 500,\n",
              " 'รายได้': 501,\n",
              " 'ดีกว่า': 502,\n",
              " 'สิ': 503,\n",
              " 'กระชาย': 504,\n",
              " 'เม': 505,\n",
              " 'กะ': 506,\n",
              " 'รีบ': 507,\n",
              " 'เต็ม': 508,\n",
              " 'สะอาด': 509,\n",
              " 'ซิ': 510,\n",
              " 'h': 511,\n",
              " 'ทัก': 512,\n",
              " 'แน่': 513,\n",
              " 'เก็บ': 514,\n",
              " 'ติดต่อ': 515,\n",
              " 'ชม': 516,\n",
              " 'สนับสนุน': 517,\n",
              " 'studio': 518,\n",
              " 'ช่วยเหลือ': 519,\n",
              " 'อันดับ': 520,\n",
              " 'เหตุ': 521,\n",
              " 'รูป': 522,\n",
              " 'ล็อก': 523,\n",
              " 'ฟัง': 524,\n",
              " 'ตก': 525,\n",
              " 'timeline': 526,\n",
              " 'r': 527,\n",
              " 'แสน': 528,\n",
              " 'ออโต้': 529,\n",
              " 'มีเดีย': 530,\n",
              " 'uncut': 531,\n",
              " 'บ': 532,\n",
              " 'ผ่อนคลาย': 533,\n",
              " 'อื่น': 534,\n",
              " 'แก': 535,\n",
              " '15': 536,\n",
              " 'เสีย': 537,\n",
              " 'ต่อไป': 538,\n",
              " 'แอ': 539,\n",
              " 'กี่': 540,\n",
              " 'ด่า': 541,\n",
              " 'ไปหา': 542,\n",
              " 'ที่มา': 543,\n",
              " 'ประจำ': 544,\n",
              " 'เฮีย': 545,\n",
              " 'เกือบ': 546,\n",
              " 'โน': 547,\n",
              " 'ผลกระทบ': 548,\n",
              " 'อาจ': 549,\n",
              " 'ไร้': 550,\n",
              " 'w': 551,\n",
              " 'ลืม': 552,\n",
              " 'ตัวเลข': 553,\n",
              " 'เช้า': 554,\n",
              " 'วัด': 555,\n",
              " 'ผลิต': 556,\n",
              " 'นักเรียน': 557,\n",
              " 'หญิง': 558,\n",
              " 'หรอก': 559,\n",
              " 'ปลอด': 560,\n",
              " 'พัน': 561,\n",
              " 'สิ่ง': 562,\n",
              " 'ใช้ชีวิต': 563,\n",
              " 'ลุง': 564,\n",
              " 'สี่': 565,\n",
              " 'เน้น': 566,\n",
              " 'เย็ด': 567,\n",
              " 'คุ้มครอง': 568,\n",
              " 'ฝุ่น': 569,\n",
              " '฿': 570,\n",
              " 'ครั้ง': 571,\n",
              " 'สถานที่': 572,\n",
              " 'เหลือ': 573,\n",
              " 'g': 574,\n",
              " 'ไอ': 575,\n",
              " 'นาง': 576,\n",
              " 'ปอด': 577,\n",
              " 'ระลอก': 578,\n",
              " 'นท': 579,\n",
              " 'ทุกท่าน': 580,\n",
              " 'ลอง': 581,\n",
              " 'รี': 582,\n",
              " 'สัก': 583,\n",
              " 'อาจจะ': 584,\n",
              " 'ทหาร': 585,\n",
              " 'บน': 586,\n",
              " 'ไปเที่ยว': 587,\n",
              " 'วิกฤต': 588,\n",
              " 'e': 589,\n",
              " 'เท่านั้น': 590,\n",
              " 'กด': 591,\n",
              " '30': 592,\n",
              " 'พา': 593,\n",
              " 'ร่างกาย': 594,\n",
              " 'นายก': 595,\n",
              " 'ปชช': 596,\n",
              " 'แล้วก็': 597,\n",
              " 'ชั่น': 598,\n",
              " 'เดิม': 599,\n",
              " 'ตั้ง': 600,\n",
              " 'อร่อย': 601,\n",
              " 'ซา': 602,\n",
              " '11': 603,\n",
              " '23': 604,\n",
              " 'ปัญหา': 605,\n",
              " 'สินค้า': 606,\n",
              " 'ง่าย': 607,\n",
              " '17': 608,\n",
              " 'บัง': 609,\n",
              " 'แถม': 610,\n",
              " 'สี': 611,\n",
              " 'กำลังใจ': 612,\n",
              " 'แฟน': 613,\n",
              " 'สัส': 614,\n",
              " 'เตรียม': 615,\n",
              " '31': 616,\n",
              " 'ยาก': 617,\n",
              " 'กาแฟ': 618,\n",
              " 'บริจาค': 619,\n",
              " 'เพิ่ง': 620,\n",
              " 'ศบค': 621,\n",
              " 'แดก': 622,\n",
              " 'ที่แล้ว': 623,\n",
              " 'ศูนย์ปฏิบัติการ': 624,\n",
              " 'เจ้ง': 625,\n",
              " 'ที่ทำงาน': 626,\n",
              " 'ป': 627,\n",
              " 'แข็งแรง': 628,\n",
              " 'ผู้เสียชีวิต': 629,\n",
              " 'น้า': 630,\n",
              " 'พุ่ง': 631,\n",
              " 'ต่างๆ': 632,\n",
              " 'เนื่องจาก': 633,\n",
              " 'เมษายน': 634,\n",
              " 'นำ': 635,\n",
              " 'สวน': 636,\n",
              " 'ชี้': 637,\n",
              " 'ชาย': 638,\n",
              " '‼️': 639,\n",
              " 'สวย': 640,\n",
              " 'ณ': 641,\n",
              " 'แคป': 642,\n",
              " 'ไหว': 643,\n",
              " 'คลิป': 644,\n",
              " 'นะจ๊ะ': 645,\n",
              " 'คลิก': 646,\n",
              " 'เสียว': 647,\n",
              " 'ออกจาก': 648,\n",
              " 'ทางการแพทย์': 649,\n",
              " 'คนไข้': 650,\n",
              " 'เหมือนกัน': 651,\n",
              " 'หาก': 652,\n",
              " 'ปล่อย': 653,\n",
              " 'รายงาน': 654,\n",
              " '2021': 655,\n",
              " 'เส้น': 656,\n",
              " 'สอบ': 657,\n",
              " 'ติดตาม': 658,\n",
              " 'แม่ค้า': 659,\n",
              " 'ไกล': 660,\n",
              " 'สาว': 661,\n",
              " 'ไอ้': 662,\n",
              " 'งด': 663,\n",
              " '🔸': 664,\n",
              " 'จันทร์': 665,\n",
              " 'แหละ': 666,\n",
              " 'บุรี': 667,\n",
              " 'ดูแลตัวเอง': 668,\n",
              " 'ยู': 669,\n",
              " 'มาตรการ': 670,\n",
              " 'มีความสุข': 671,\n",
              " 'ข้าง': 672,\n",
              " '50': 673,\n",
              " 'รายละเอียด': 674,\n",
              " 'bambam': 675,\n",
              " '24': 676,\n",
              " 'ด': 677,\n",
              " 'พยาบาล': 678,\n",
              " 'ริ': 679,\n",
              " 'คนอื่น': 680,\n",
              " 'หนี': 681,\n",
              " 'เคส': 682,\n",
              " 'แพทย์': 683,\n",
              " 'เซ': 684,\n",
              " 'โว้ย': 685,\n",
              " 'ซอง': 686,\n",
              " 'ล้างมือ': 687,\n",
              " 'แจก': 688,\n",
              " 'จึง': 689,\n",
              " 'sinovac': 690,\n",
              " 'ช่วยกัน': 691,\n",
              " 'โชค': 692,\n",
              " 'ดำ': 693,\n",
              " 'อา': 694,\n",
              " 'กรอบ': 695,\n",
              " 'ครีม': 696,\n",
              " 'ช่อง': 697,\n",
              " 'ทั่วโลก': 698,\n",
              " 'สงสาร': 699,\n",
              " 'ลำบาก': 700,\n",
              " 'ให้ได้': 701,\n",
              " 'เชื้อไวรัส': 702,\n",
              " 'ความเสี่ยง': 703,\n",
              " 'ใบ': 704,\n",
              " 'สาม': 705,\n",
              " 'เยียวยา': 706,\n",
              " 'ก้อ': 707,\n",
              " 'พัง': 708,\n",
              " 'ขัง': 709,\n",
              " 'สงกรานต์': 710,\n",
              " 'ผิว': 711,\n",
              " '🆔': 712,\n",
              " '5555': 713,\n",
              " 'แท้': 714,\n",
              " '’': 715,\n",
              " 'เจ็บ': 716,\n",
              " 'คนตาย': 717,\n",
              " 'pm': 718,\n",
              " 'แรง': 719,\n",
              " 'เชียร์': 720,\n",
              " 'เมื่อวาน': 721,\n",
              " 'คืน': 722,\n",
              " 'โรงแรม': 723,\n",
              " 'น่ะ': 724,\n",
              " 'ชง': 725,\n",
              " 'รึ': 726,\n",
              " 'เลิก': 727,\n",
              " 'id': 728,\n",
              " 'กรณี': 729,\n",
              " 'ญี่ปุ่น': 730,\n",
              " '16': 731,\n",
              " 'เจริญ': 732,\n",
              " 'ลงทะเบียน': 733,\n",
              " '27': 734,\n",
              " 'ตกงาน': 735,\n",
              " 'มิถุนายน': 736,\n",
              " 'ชลบุรี': 737,\n",
              " 'ทั้งหมด': 738,\n",
              " 'บางคน': 739,\n",
              " 'สูงสุด': 740,\n",
              " 'ระหว่าง': 741,\n",
              " 'มาถึง': 742,\n",
              " 'พม่า': 743,\n",
              " 'ครึ่ง': 744,\n",
              " 'โรคติดเชื้อ': 745,\n",
              " 'เว้น': 746,\n",
              " 'ยังมี': 747,\n",
              " 'ควย': 748,\n",
              " 'เสาร์': 749,\n",
              " 'เขต': 750,\n",
              " 'เครื่อง': 751,\n",
              " 'คอน': 752,\n",
              " 'ปาก': 753,\n",
              " 'เชื่อ': 754,\n",
              " 'เดี๋ยว': 755,\n",
              " 'จำแนก': 756,\n",
              " '🙂': 757,\n",
              " 'ไว': 758,\n",
              " 'มาตรการป้องกัน': 759,\n",
              " 'ขี้': 760,\n",
              " 'อิสระ': 761,\n",
              " 'ไล่': 762,\n",
              " '🇹🇭': 763,\n",
              " 'ไม่ค่อย': 764,\n",
              " 'ภาษี': 765,\n",
              " 'อาหาร': 766,\n",
              " 'เพิ่มขึ้น': 767,\n",
              " 'ล็อค': 768,\n",
              " 'ดารา': 769,\n",
              " 'สังคม': 770,\n",
              " 'มอง': 771,\n",
              " 'ต่างประเทศ': 772,\n",
              " 'ลิ': 773,\n",
              " 'อาทิตย์': 774,\n",
              " 'เช้านี้': 775,\n",
              " 'สอบถาม': 776,\n",
              " 'แก้': 777,\n",
              " 'คู่': 778,\n",
              " 'มี่': 779,\n",
              " 'เงี่ยน': 780,\n",
              " 'ที่ไหน': 781,\n",
              " 'สำคัญ': 782,\n",
              " 'ร': 783,\n",
              " 'ที่ผ่านมา': 784,\n",
              " 'อากาศ': 785,\n",
              " 'ชุมชน': 786,\n",
              " 'บิน': 787,\n",
              " 'า': 788,\n",
              " 'วิ่ง': 789,\n",
              " 'ช้าง': 790,\n",
              " 'รร': 791,\n",
              " 'ว่ะ': 792,\n",
              " 'ชาว': 793,\n",
              " 'เกาะ': 794,\n",
              " '📣': 795,\n",
              " 'กันยายน': 796,\n",
              " 'คลัสเตอร์': 797,\n",
              " 'เช็ค': 798,\n",
              " 'เรียก': 799,\n",
              " 'ฝัน': 800,\n",
              " 'ยิง': 801,\n",
              " 'ทรง': 802,\n",
              " 'ทั่ว': 803,\n",
              " 'คุม': 804,\n",
              " 'เย็น': 805,\n",
              " 'ทุกๆ': 806,\n",
              " 'เห้อ': 807,\n",
              " 'โคตร': 808,\n",
              " 'เก่ง': 809,\n",
              " 'ผิด': 810,\n",
              " 'คัด': 811,\n",
              " 'ทุกวันนี้': 812,\n",
              " 'ยอม': 813,\n",
              " 'มีเงิน': 814,\n",
              " '🥺': 815,\n",
              " 'มอ': 816,\n",
              " 'ที่นี่': 817,\n",
              " 'วัฒนะ': 818,\n",
              " 'ระยะห่าง': 819,\n",
              " 'ได้ที่': 820,\n",
              " 'อาชีพ': 821,\n",
              " 'ตรง': 822,\n",
              " 'ต้าน': 823,\n",
              " 'ต้องการ': 824,\n",
              " 'สิว': 825,\n",
              " 'กรอง': 826,\n",
              " 'สิงหาคม': 827,\n",
              " 'ซัก': 828,\n",
              " 'เป๊ก': 829,\n",
              " 'เจ้า': 830,\n",
              " 'เกาหลี': 831,\n",
              " 'การ์ด': 832,\n",
              " 'ตื่น': 833,\n",
              " 'รับผิดชอบ': 834,\n",
              " 'สักที': 835,\n",
              " '😖': 836,\n",
              " 'เบา': 837,\n",
              " 'โรงเรียน': 838,\n",
              " 'แสดง': 839,\n",
              " 'ดอนเมือง': 840,\n",
              " 'ใจเย็น': 841,\n",
              " 'หล่อ': 842,\n",
              " 'ปีใหม่': 843,\n",
              " 'หลังจาก': 844,\n",
              " 'เข้าไป': 845,\n",
              " 'ทาน': 846,\n",
              " 'ร้าย': 847,\n",
              " 'ลดลง': 848,\n",
              " 'ญาติ': 849,\n",
              " 'ลา': 850,\n",
              " '22': 851,\n",
              " 'อัน': 852,\n",
              " '•': 853,\n",
              " 'น่ารัก': 854,\n",
              " 'รีด': 855,\n",
              " 'ฟ้า': 856,\n",
              " 'คำ': 857,\n",
              " 'จิต': 858,\n",
              " 'เพียง': 859,\n",
              " 'เรื่อยๆ': 860,\n",
              " 'ภูเก็ต': 861,\n",
              " 'แนะนำ': 862,\n",
              " 'สต': 863,\n",
              " 'สายพันธุ์': 864,\n",
              " '112': 865,\n",
              " 'พร': 866,\n",
              " 'ลุ้น': 867,\n",
              " 'ต่าง': 868,\n",
              " '‘': 869,\n",
              " 'ธุรกิจ': 870,\n",
              " 'ด่าน': 871,\n",
              " 'เชิงรุก': 872,\n",
              " 'ป่าว': 873,\n",
              " 'เจ้าหน้าที่': 874,\n",
              " 'บางแค': 875,\n",
              " 'เเล้ว': 876,\n",
              " 'กล้า': 877,\n",
              " 'ชิบหาย': 878,\n",
              " 'เซเว่น': 879,\n",
              " 'เท': 880,\n",
              " 'สค': 881,\n",
              " 'พค': 882,\n",
              " 'ชื่อ': 883,\n",
              " 'เป้า': 884,\n",
              " 'หวัง': 885,\n",
              " 'วิกฤติ': 886,\n",
              " 'แน่นอน': 887,\n",
              " 'ลาย': 888,\n",
              " 'ภาพ': 889,\n",
              " 'เมื่อคืน': 890,\n",
              " 'สุข': 891,\n",
              " 'ป่ะ': 892,\n",
              " 'รุนแรง': 893,\n",
              " 'น้ำมัน': 894,\n",
              " 'ห้าง': 895,\n",
              " 'เมื่อย': 896,\n",
              " 'ไบร์': 897,\n",
              " 'ว่าง': 898,\n",
              " '📆': 899,\n",
              " 'หมู่บ้าน': 900,\n",
              " 'แทน': 901,\n",
              " '29': 902,\n",
              " 'ชิ้น': 903,\n",
              " 'อีกครั้ง': 904,\n",
              " 'โอกาส': 905,\n",
              " 'คัน': 906,\n",
              " 'ๆๆ': 907,\n",
              " 'ความสุข': 908,\n",
              " 'howtoperfect': 909,\n",
              " 'เย': 910,\n",
              " 'โรม่า': 911,\n",
              " 'ที่จอดรถ': 912,\n",
              " 'ค่อย': 913,\n",
              " 'ตอบ': 914,\n",
              " 'เวร': 915,\n",
              " 'got': 916,\n",
              " '21': 917,\n",
              " 'บัว': 918,\n",
              " 'พ': 919,\n",
              " 'สุดท้าย': 920,\n",
              " 'ทํา': 921,\n",
              " 'อยู่แล้ว': 922,\n",
              " 'อย่างไร': 923,\n",
              " 'เงินเดือน': 924,\n",
              " 'astrazeneca': 925,\n",
              " 'เกลียด': 926,\n",
              " 'wfh': 927,\n",
              " 'รบ': 928,\n",
              " 'ค้าบ': 929,\n",
              " 'คับ': 930,\n",
              " 'แม้': 931,\n",
              " 'เคร่งครัด': 932,\n",
              " 'เอม': 933,\n",
              " 'เก๊าคน': 934,\n",
              " 'wanyen': 935,\n",
              " '0101': 936,\n",
              " 'ด้าน': 937,\n",
              " 'เหงา': 938,\n",
              " 'ตี': 939,\n",
              " 'แอลกอฮอล์': 940,\n",
              " 'โง่': 941,\n",
              " 'ยย': 942,\n",
              " 'มือ': 943,\n",
              " 'เสียดาย': 944,\n",
              " 'ข้าว': 945,\n",
              " 'เท่า': 946,\n",
              " 'สลิ่ม': 947,\n",
              " 'ก้': 948,\n",
              " 'ควาย': 949,\n",
              " 'โทษ': 950,\n",
              " 'หัว': 951,\n",
              " '500': 952,\n",
              " 'กย': 953,\n",
              " '48': 954,\n",
              " 'สั่งซื้อ': 955,\n",
              " 'ดุสิต': 956,\n",
              " 'blackpink': 957,\n",
              " '94': 958,\n",
              " 'newnormal': 959,\n",
              " 'เริ่มต้น': 960,\n",
              " 'ตัวแทน': 961,\n",
              " 'ประมาณ': 962,\n",
              " 'พวกเรา': 963,\n",
              " 'แห่ง': 964,\n",
              " 'เปลี่ยน': 965,\n",
              " 'อห': 966,\n",
              " 'พรุ่งนี้': 967,\n",
              " '😂': 968,\n",
              " 'แวค': 969,\n",
              " 'ตัด': 970,\n",
              " 'สบาย': 971,\n",
              " 'โดส': 972,\n",
              " '33': 973,\n",
              " 'เพิ่มเติม': 974,\n",
              " 'ตรวจหา': 975,\n",
              " 'เพจ': 976,\n",
              " 'โครงการ': 977,\n",
              " '40': 978,\n",
              " 'เก่า': 979,\n",
              " 'ให้บริการ': 980,\n",
              " 'เหมือนเดิม': 981,\n",
              " 'นอกจาก': 982,\n",
              " 'สัปดาห์': 983,\n",
              " '❤️': 984,\n",
              " 'แน่ๆ': 985,\n",
              " 'นุช': 986,\n",
              " 'ยังคง': 987,\n",
              " 'แอบ': 988,\n",
              " 'การรักษา': 989,\n",
              " 'พนักงาน': 990,\n",
              " '26': 991,\n",
              " 'วันหยุด': 992,\n",
              " 'แพง': 993,\n",
              " '\\U0001f972': 994,\n",
              " 'มกราคม': 995,\n",
              " 'เล็ก': 996,\n",
              " 'เข้ม': 997,\n",
              " 'ไฮ': 998,\n",
              " 'าา': 999,\n",
              " 'นิ': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "train_word_tokenizer = create_tokenizer(cleaned_words)\n",
        "vocab_size = len(train_word_tokenizer.word_index) + 1\n",
        "\n",
        "train_word_tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGHO95YjojVs"
      },
      "outputs": [],
      "source": [
        "def max_length(words):\n",
        "    return(len(max(words, key = len)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zIHZ8q8ojYl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01196bd3-d9b4-48b4-f953-98da74f9989a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "max_length = max_length(tokenized_doc)\n",
        "max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5a4Yy5Kojbg"
      },
      "outputs": [],
      "source": [
        "def encoding_doc(token, words):\n",
        "    return(token.texts_to_sequences(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUIR_uIEojeS"
      },
      "outputs": [],
      "source": [
        "encoded_doc = encoding_doc(train_word_tokenizer, cleaned_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C06XmhVLojh7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4559b7e2-5b6f-46eb-a1d9-1b186e29576f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "อัปเดต สถานการณ์   โควิด 19   วันนี้   26   มิ ย   64    เสียชีวิต   51   ราย   ผู้ป่วย ใหม่ รวม   4161   ราย \n",
            " \n",
            " จำแนก เป็น \n",
            " ติดเชื้อ ใหม่   4089   ราย \n",
            " ติดเชื้อ ภายใน เรือนจำ / ที่ ต้อง ขัง   72   ราย \n",
            " หายป่วย กลับบ้าน   3569   ราย \n",
            " ผู้ป่วย สะสม   211589   ราย   ตั้งแต่   1   เมษายน   https :// tco / q 0 zvz 9 snw 7\n",
            "[1306, 80, 1, 4, 23, 991, 333, 207, 113, 105, 1361, 18, 67, 61, 172, 3663, 18, 756, 20, 47, 61, 6271, 18, 47, 443, 469, 6, 22, 709, 1637, 18, 255, 292, 6272, 18, 67, 155, 6273, 18, 129, 29, 634, 2, 3, 315, 65, 6274, 62, 8801, 53]\n"
          ]
        }
      ],
      "source": [
        "print(cleaned_words[0])\n",
        "print(encoded_doc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsK-ctPoFuXQ"
      },
      "outputs": [],
      "source": [
        "def padding_doc(encoded_doc, max_length):\n",
        "    return(pad_sequences(encoded_doc, maxlen = max_length, padding = \"post\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3h3zLGAaovf-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f30e32c-16f5-4149-8661-17a7032b41be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of padded docs =  (7354, 128)\n",
            "อัปเดต สถานการณ์   โควิด 19   วันนี้   26   มิ ย   64    เสียชีวิต   51   ราย   ผู้ป่วย ใหม่ รวม   4161   ราย \n",
            " \n",
            " จำแนก เป็น \n",
            " ติดเชื้อ ใหม่   4089   ราย \n",
            " ติดเชื้อ ภายใน เรือนจำ / ที่ ต้อง ขัง   72   ราย \n",
            " หายป่วย กลับบ้าน   3569   ราย \n",
            " ผู้ป่วย สะสม   211589   ราย   ตั้งแต่   1   เมษายน   https :// tco / q 0 zvz 9 snw 7\n",
            "[1306, 80, 1, 4, 23, 991, 333, 207, 113, 105, 1361, 18, 67, 61, 172, 3663, 18, 756, 20, 47, 61, 6271, 18, 47, 443, 469, 6, 22, 709, 1637, 18, 255, 292, 6272, 18, 67, 155, 6273, 18, 129, 29, 634, 2, 3, 315, 65, 6274, 62, 8801, 53]\n",
            "[1306   80    1    4   23  991  333  207  113  105 1361   18   67   61\n",
            "  172 3663   18  756   20   47   61 6271   18   47  443  469    6   22\n",
            "  709 1637   18  255  292 6272   18   67  155 6273   18  129   29  634\n",
            "    2    3  315   65 6274   62 8801   53    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n"
          ]
        }
      ],
      "source": [
        "padded_doc = padding_doc(encoded_doc, max_length)\n",
        "print(\"Shape of padded docs = \",padded_doc.shape)\n",
        "\n",
        "print(cleaned_words[0])\n",
        "print(encoded_doc[0])\n",
        "print(padded_doc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1rMrOSTovim"
      },
      "outputs": [],
      "source": [
        "sentiment = sentiment_df['sentiment'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zTQEKP2ovk-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d18c984-0b52-47c0-f449-36e5c1965add"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ไม่เดือดร้อน', 'เดือดร้อน']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "unique_sentiment = list(set(sentiment))\n",
        "unique_sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ioMOUPyovnV"
      },
      "outputs": [],
      "source": [
        "['pos', 'neg']\n",
        "\n",
        "output_tokenizer = create_tokenizer(unique_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lP7OYSq3ovpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d31f20-3c87-4be5-ee29-c065ce5b16e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ไม่เดือดร้อน', 'ไม่เดือดร้อน']\n",
            "[[1], [1]]\n"
          ]
        }
      ],
      "source": [
        "encoded_output = encoding_doc(output_tokenizer, sentiment)\n",
        "print(sentiment[0:2])\n",
        "print(encoded_output[0:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qPMryCmovrv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e0cf5ef-3867-4b0c-a880-3041e46ae9f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7354, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "encoded_output = np.array(encoded_output).reshape(len(encoded_output), 1)\n",
        "encoded_output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0Zobze4o4kL"
      },
      "outputs": [],
      "source": [
        "def one_hot(encode):\n",
        "  oh = OneHotEncoder(sparse = False)\n",
        "  return(oh.fit_transform(encode))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIP6B5rbo4nP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8b254b1-cd86-4ac3-a171-f536df0ccff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n",
            "[1. 0.]\n"
          ]
        }
      ],
      "source": [
        "output_one_hot = one_hot(encoded_output)\n",
        "print(encoded_output[0])\n",
        "print(output_one_hot[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HNXccc9o4p0"
      },
      "outputs": [],
      "source": [
        "train_X, val_X, train_Y, val_Y = train_test_split(padded_doc, output_one_hot, shuffle = True, test_size = 0.2, stratify=output_one_hot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2QrxNA2o4sc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5bc9b91-de44-4f2e-fd27-2fd735c94b68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train_X = (5883, 128) and train_Y = (5883, 2)\n",
            "Shape of val_X = (1471, 128) and val_Y = (1471, 2)\n"
          ]
        }
      ],
      "source": [
        "print(\"Shape of train_X = %s and train_Y = %s\" % (train_X.shape, train_Y.shape))\n",
        "print(\"Shape of val_X = %s and val_Y = %s\" % (val_X.shape, val_Y.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQ1v62HtpEq9"
      },
      "outputs": [],
      "source": [
        "num_classes = len(unique_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvI7S9XUpEt-"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "adam = Adam(learning_rate=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PrjVqrepEw0"
      },
      "outputs": [],
      "source": [
        "# define the model\n",
        "def define_model(length, vocab_size):\n",
        "    # channel 1\n",
        "    inputs1 = tf.keras.layers.Input(shape=(length,))\n",
        "    embedding1 = tf.keras.layers.Embedding(vocab_size, DIMENSION, trainable = True)(inputs1)\n",
        "    conv1 = tf.keras.layers.Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
        "    drop1 = tf.keras.layers.Dropout(0.5)(conv1)\n",
        "    pool1 = tf.keras.layers.MaxPooling1D(pool_size=2)(drop1)\n",
        "    flat1 = tf.keras.layers.Flatten()(pool1)\n",
        "    # channel 2\n",
        "    inputs2 = tf.keras.layers.Input(shape=(length,))\n",
        "    embedding2 = tf.keras.layers.Embedding(vocab_size, DIMENSION, trainable = True)(inputs2)\n",
        "    conv2 = tf.keras.layers.Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
        "    drop2 = tf.keras.layers.Dropout(0.5)(conv2)\n",
        "    pool2 = tf.keras.layers.MaxPooling1D(pool_size=2)(drop2)\n",
        "    flat2 = tf.keras.layers.Flatten()(pool2)\n",
        "    # channel 3\n",
        "    inputs3 = tf.keras.layers.Input(shape=(length,))\n",
        "    embedding3 = tf.keras.layers.Embedding(vocab_size, DIMENSION, trainable = True)(inputs3)\n",
        "    conv3 = tf.keras.layers.Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
        "    drop3 = tf.keras.layers.Dropout(0.5)(conv3)\n",
        "    pool3 = tf.keras.layers.MaxPooling1D(pool_size=2)(drop3)\n",
        "    flat3 = tf.keras.layers.Flatten()(pool3)\n",
        "    # merge\n",
        "    merged = tf.keras.layers.concatenate([flat1, flat2, flat3])\n",
        "    # interpretation\n",
        "    dense1 = tf.keras.layers.Dense(10, activation='relu')(merged)\n",
        "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(dense1)\n",
        "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "    # compile\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "    # summarize\n",
        "    print(model.summary())\n",
        "#     plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fFjIWw2pEza",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03ddae1d-d7c6-4f0e-ead7-6b600a418d9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 128, 100)     2269800     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 128, 100)     2269800     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, 128, 100)     2269800     ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 125, 32)      12832       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 123, 32)      19232       ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 121, 32)      25632       ['embedding_2[0][0]']            \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 125, 32)      0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 123, 32)      0           ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 121, 32)      0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 62, 32)       0           ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 61, 32)      0           ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 60, 32)      0           ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 1984)         0           ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 1952)         0           ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 1920)         0           ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 5856)         0           ['flatten[0][0]',                \n",
            "                                                                  'flatten_1[0][0]',              \n",
            "                                                                  'flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 10)           58570       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 2)            22          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,925,688\n",
            "Trainable params: 6,925,688\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = define_model(max_length, vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pj3kT-d8I_Ye",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 976
        },
        "outputId": "6564bada-38d8-4907-b66d-29faa343e2c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlox4vXOpE2N"
      },
      "outputs": [],
      "source": [
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Kk4HBo-pQxm"
      },
      "outputs": [],
      "source": [
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience = 3, verbose=1,factor=0.1, min_lr=0.000001)\n",
        "callbacks_list = [checkpoint, learning_rate_reduction]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKHOEY0KpQ0Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a50da3b1-8810-4316-b386-75eb45284648"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.6805 - accuracy: 0.5562\n",
            "Epoch 1: val_loss improved from inf to 0.65647, saving model to model.h5\n",
            "184/184 [==============================] - 12s 12ms/step - loss: 0.6805 - accuracy: 0.5562 - val_loss: 0.6565 - val_accuracy: 0.6220 - lr: 1.0000e-04\n",
            "Epoch 2/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.6229 - accuracy: 0.6549\n",
            "Epoch 2: val_loss improved from 0.65647 to 0.58918, saving model to model.h5\n",
            "184/184 [==============================] - 2s 11ms/step - loss: 0.6222 - accuracy: 0.6560 - val_loss: 0.5892 - val_accuracy: 0.7206 - lr: 1.0000e-04\n",
            "Epoch 3/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.5108 - accuracy: 0.7630\n",
            "Epoch 3: val_loss improved from 0.58918 to 0.52232, saving model to model.h5\n",
            "184/184 [==============================] - 2s 13ms/step - loss: 0.5104 - accuracy: 0.7627 - val_loss: 0.5223 - val_accuracy: 0.7580 - lr: 1.0000e-04\n",
            "Epoch 4/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.4236 - accuracy: 0.8063\n",
            "Epoch 4: val_loss improved from 0.52232 to 0.49549, saving model to model.h5\n",
            "184/184 [==============================] - 2s 11ms/step - loss: 0.4224 - accuracy: 0.8076 - val_loss: 0.4955 - val_accuracy: 0.7709 - lr: 1.0000e-04\n",
            "Epoch 5/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.3454 - accuracy: 0.8601\n",
            "Epoch 5: val_loss improved from 0.49549 to 0.49050, saving model to model.h5\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.3452 - accuracy: 0.8603 - val_loss: 0.4905 - val_accuracy: 0.7634 - lr: 1.0000e-04\n",
            "Epoch 6/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.2615 - accuracy: 0.9047\n",
            "Epoch 6: val_loss did not improve from 0.49050\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.2626 - accuracy: 0.9036 - val_loss: 0.4935 - val_accuracy: 0.7532 - lr: 1.0000e-04\n",
            "Epoch 7/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.1906 - accuracy: 0.9392\n",
            "Epoch 7: val_loss improved from 0.49050 to 0.48980, saving model to model.h5\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.1909 - accuracy: 0.9391 - val_loss: 0.4898 - val_accuracy: 0.7607 - lr: 1.0000e-04\n",
            "Epoch 8/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.1355 - accuracy: 0.9602\n",
            "Epoch 8: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.1355 - accuracy: 0.9602 - val_loss: 0.4998 - val_accuracy: 0.7519 - lr: 1.0000e-04\n",
            "Epoch 9/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.0961 - accuracy: 0.9758\n",
            "Epoch 9: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0959 - accuracy: 0.9760 - val_loss: 0.5358 - val_accuracy: 0.7403 - lr: 1.0000e-04\n",
            "Epoch 10/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0672 - accuracy: 0.9860\n",
            "Epoch 10: val_loss did not improve from 0.48980\n",
            "\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0675 - accuracy: 0.9857 - val_loss: 0.5711 - val_accuracy: 0.7383 - lr: 1.0000e-04\n",
            "Epoch 11/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0495 - accuracy: 0.9914\n",
            "Epoch 11: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0493 - accuracy: 0.9913 - val_loss: 0.5600 - val_accuracy: 0.7451 - lr: 1.0000e-05\n",
            "Epoch 12/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0478 - accuracy: 0.9917\n",
            "Epoch 12: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0475 - accuracy: 0.9918 - val_loss: 0.5628 - val_accuracy: 0.7471 - lr: 1.0000e-05\n",
            "Epoch 13/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0451 - accuracy: 0.9918\n",
            "Epoch 13: val_loss did not improve from 0.48980\n",
            "\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0451 - accuracy: 0.9918 - val_loss: 0.5718 - val_accuracy: 0.7458 - lr: 1.0000e-05\n",
            "Epoch 14/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9924\n",
            "Epoch 14: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 1s 8ms/step - loss: 0.0442 - accuracy: 0.9924 - val_loss: 0.5682 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 15/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0441 - accuracy: 0.9918\n",
            "Epoch 15: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0440 - accuracy: 0.9918 - val_loss: 0.5668 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 16/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0441 - accuracy: 0.9915\n",
            "Epoch 16: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0442 - accuracy: 0.9915 - val_loss: 0.5657 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 17/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0439 - accuracy: 0.9923\n",
            "Epoch 17: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0440 - accuracy: 0.9922 - val_loss: 0.5659 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 18/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.0434 - accuracy: 0.9926\n",
            "Epoch 18: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 1s 8ms/step - loss: 0.0431 - accuracy: 0.9927 - val_loss: 0.5673 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 19/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0446 - accuracy: 0.9907\n",
            "Epoch 19: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0446 - accuracy: 0.9907 - val_loss: 0.5667 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 20/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0434 - accuracy: 0.9929\n",
            "Epoch 20: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0435 - accuracy: 0.9927 - val_loss: 0.5672 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 21/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0437 - accuracy: 0.9924\n",
            "Epoch 21: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0435 - accuracy: 0.9925 - val_loss: 0.5669 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 22/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9903\n",
            "Epoch 22: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0442 - accuracy: 0.9903 - val_loss: 0.5678 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 23/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0432 - accuracy: 0.9911\n",
            "Epoch 23: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0428 - accuracy: 0.9913 - val_loss: 0.5686 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 24/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.0429 - accuracy: 0.9916\n",
            "Epoch 24: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0426 - accuracy: 0.9917 - val_loss: 0.5699 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 25/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0427 - accuracy: 0.9924\n",
            "Epoch 25: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0427 - accuracy: 0.9924 - val_loss: 0.5697 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 26/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.0414 - accuracy: 0.9930\n",
            "Epoch 26: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 1s 8ms/step - loss: 0.0416 - accuracy: 0.9925 - val_loss: 0.5698 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 27/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.0422 - accuracy: 0.9919\n",
            "Epoch 27: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0422 - accuracy: 0.9918 - val_loss: 0.5702 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 28/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0418 - accuracy: 0.9929\n",
            "Epoch 28: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0418 - accuracy: 0.9929 - val_loss: 0.5712 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 29/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9929\n",
            "Epoch 29: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0411 - accuracy: 0.9929 - val_loss: 0.5713 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 30/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0412 - accuracy: 0.9929\n",
            "Epoch 30: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0412 - accuracy: 0.9930 - val_loss: 0.5710 - val_accuracy: 0.7444 - lr: 1.0000e-06\n",
            "Epoch 31/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0408 - accuracy: 0.9927\n",
            "Epoch 31: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0407 - accuracy: 0.9927 - val_loss: 0.5715 - val_accuracy: 0.7444 - lr: 1.0000e-06\n",
            "Epoch 32/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0412 - accuracy: 0.9932\n",
            "Epoch 32: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 1s 8ms/step - loss: 0.0410 - accuracy: 0.9932 - val_loss: 0.5723 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 33/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0413 - accuracy: 0.9924\n",
            "Epoch 33: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0411 - accuracy: 0.9925 - val_loss: 0.5727 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 34/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0399 - accuracy: 0.9935\n",
            "Epoch 34: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0398 - accuracy: 0.9935 - val_loss: 0.5735 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 35/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0400 - accuracy: 0.9932\n",
            "Epoch 35: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0400 - accuracy: 0.9932 - val_loss: 0.5742 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 36/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9934\n",
            "Epoch 36: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0415 - accuracy: 0.9934 - val_loss: 0.5740 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 37/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0411 - accuracy: 0.9930\n",
            "Epoch 37: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0410 - accuracy: 0.9930 - val_loss: 0.5759 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 38/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0398 - accuracy: 0.9930\n",
            "Epoch 38: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0400 - accuracy: 0.9930 - val_loss: 0.5762 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 39/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0394 - accuracy: 0.9940\n",
            "Epoch 39: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 1s 8ms/step - loss: 0.0394 - accuracy: 0.9941 - val_loss: 0.5766 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 40/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0392 - accuracy: 0.9929\n",
            "Epoch 40: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0389 - accuracy: 0.9930 - val_loss: 0.5765 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 41/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0394 - accuracy: 0.9933\n",
            "Epoch 41: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0393 - accuracy: 0.9934 - val_loss: 0.5761 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 42/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0392 - accuracy: 0.9931\n",
            "Epoch 42: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0390 - accuracy: 0.9932 - val_loss: 0.5771 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 43/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0385 - accuracy: 0.9929\n",
            "Epoch 43: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0391 - accuracy: 0.9925 - val_loss: 0.5780 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 44/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0383 - accuracy: 0.9935\n",
            "Epoch 44: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0383 - accuracy: 0.9935 - val_loss: 0.5793 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 45/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0388 - accuracy: 0.9931\n",
            "Epoch 45: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0388 - accuracy: 0.9932 - val_loss: 0.5795 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 46/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0393 - accuracy: 0.9926\n",
            "Epoch 46: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0393 - accuracy: 0.9927 - val_loss: 0.5795 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 47/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0369 - accuracy: 0.9936\n",
            "Epoch 47: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0369 - accuracy: 0.9937 - val_loss: 0.5803 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 48/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0376 - accuracy: 0.9940\n",
            "Epoch 48: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0376 - accuracy: 0.9941 - val_loss: 0.5804 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 49/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0371 - accuracy: 0.9940\n",
            "Epoch 49: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0373 - accuracy: 0.9939 - val_loss: 0.5812 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 50/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0376 - accuracy: 0.9934\n",
            "Epoch 50: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0375 - accuracy: 0.9934 - val_loss: 0.5822 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 51/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0379 - accuracy: 0.9928\n",
            "Epoch 51: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0379 - accuracy: 0.9929 - val_loss: 0.5824 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 52/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0373 - accuracy: 0.9935\n",
            "Epoch 52: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0377 - accuracy: 0.9932 - val_loss: 0.5825 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 53/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0371 - accuracy: 0.9933\n",
            "Epoch 53: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0370 - accuracy: 0.9934 - val_loss: 0.5836 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 54/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0371 - accuracy: 0.9917\n",
            "Epoch 54: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0370 - accuracy: 0.9918 - val_loss: 0.5843 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 55/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0368 - accuracy: 0.9930\n",
            "Epoch 55: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0368 - accuracy: 0.9930 - val_loss: 0.5849 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 56/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0367 - accuracy: 0.9922\n",
            "Epoch 56: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0369 - accuracy: 0.9922 - val_loss: 0.5850 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 57/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0364 - accuracy: 0.9939\n",
            "Epoch 57: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0364 - accuracy: 0.9939 - val_loss: 0.5853 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 58/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0357 - accuracy: 0.9929\n",
            "Epoch 58: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0358 - accuracy: 0.9930 - val_loss: 0.5855 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 59/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 0.9942\n",
            "Epoch 59: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0346 - accuracy: 0.9942 - val_loss: 0.5853 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 60/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0362 - accuracy: 0.9925\n",
            "Epoch 60: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0362 - accuracy: 0.9925 - val_loss: 0.5857 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 61/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0354 - accuracy: 0.9931\n",
            "Epoch 61: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0355 - accuracy: 0.9930 - val_loss: 0.5870 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 62/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9939\n",
            "Epoch 62: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0344 - accuracy: 0.9939 - val_loss: 0.5867 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 63/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0356 - accuracy: 0.9938\n",
            "Epoch 63: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0356 - accuracy: 0.9939 - val_loss: 0.5882 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 64/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0351 - accuracy: 0.9930\n",
            "Epoch 64: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0350 - accuracy: 0.9930 - val_loss: 0.5882 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 65/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0341 - accuracy: 0.9936\n",
            "Epoch 65: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0341 - accuracy: 0.9935 - val_loss: 0.5889 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 66/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0349 - accuracy: 0.9944\n",
            "Epoch 66: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0347 - accuracy: 0.9946 - val_loss: 0.5896 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 67/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0351 - accuracy: 0.9927\n",
            "Epoch 67: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0348 - accuracy: 0.9929 - val_loss: 0.5912 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 68/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0335 - accuracy: 0.9950\n",
            "Epoch 68: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0334 - accuracy: 0.9949 - val_loss: 0.5907 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 69/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.0341 - accuracy: 0.9942\n",
            "Epoch 69: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0343 - accuracy: 0.9941 - val_loss: 0.5911 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 70/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0340 - accuracy: 0.9941\n",
            "Epoch 70: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0341 - accuracy: 0.9939 - val_loss: 0.5916 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 71/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0337 - accuracy: 0.9946\n",
            "Epoch 71: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0334 - accuracy: 0.9947 - val_loss: 0.5921 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 72/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0341 - accuracy: 0.9930\n",
            "Epoch 72: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0340 - accuracy: 0.9932 - val_loss: 0.5933 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 73/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0339 - accuracy: 0.9939\n",
            "Epoch 73: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0342 - accuracy: 0.9939 - val_loss: 0.5943 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 74/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0336 - accuracy: 0.9937\n",
            "Epoch 74: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0335 - accuracy: 0.9939 - val_loss: 0.5944 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 75/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0327 - accuracy: 0.9953\n",
            "Epoch 75: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0326 - accuracy: 0.9954 - val_loss: 0.5939 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 76/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9941\n",
            "Epoch 76: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0331 - accuracy: 0.9941 - val_loss: 0.5956 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 77/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0335 - accuracy: 0.9951\n",
            "Epoch 77: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0334 - accuracy: 0.9951 - val_loss: 0.5961 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 78/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0325 - accuracy: 0.9943\n",
            "Epoch 78: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0325 - accuracy: 0.9944 - val_loss: 0.5964 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 79/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0323 - accuracy: 0.9937\n",
            "Epoch 79: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0322 - accuracy: 0.9937 - val_loss: 0.5962 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 80/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0329 - accuracy: 0.9931\n",
            "Epoch 80: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0329 - accuracy: 0.9932 - val_loss: 0.5972 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 81/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0337 - accuracy: 0.9939\n",
            "Epoch 81: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0339 - accuracy: 0.9935 - val_loss: 0.5984 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 82/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0325 - accuracy: 0.9945\n",
            "Epoch 82: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0324 - accuracy: 0.9946 - val_loss: 0.5984 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 83/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0313 - accuracy: 0.9956\n",
            "Epoch 83: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0313 - accuracy: 0.9956 - val_loss: 0.5995 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 84/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0322 - accuracy: 0.9946\n",
            "Epoch 84: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0322 - accuracy: 0.9947 - val_loss: 0.5990 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 85/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0317 - accuracy: 0.9950\n",
            "Epoch 85: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0317 - accuracy: 0.9951 - val_loss: 0.6001 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 86/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0312 - accuracy: 0.9950\n",
            "Epoch 86: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0310 - accuracy: 0.9951 - val_loss: 0.5999 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 87/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0314 - accuracy: 0.9946\n",
            "Epoch 87: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0315 - accuracy: 0.9946 - val_loss: 0.6017 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 88/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0311 - accuracy: 0.9945\n",
            "Epoch 88: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0311 - accuracy: 0.9944 - val_loss: 0.6004 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 89/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0308 - accuracy: 0.9952\n",
            "Epoch 89: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0307 - accuracy: 0.9952 - val_loss: 0.6020 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 90/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0316 - accuracy: 0.9936\n",
            "Epoch 90: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0317 - accuracy: 0.9935 - val_loss: 0.6026 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 91/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0320 - accuracy: 0.9936\n",
            "Epoch 91: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0319 - accuracy: 0.9937 - val_loss: 0.6031 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 92/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0305 - accuracy: 0.9947\n",
            "Epoch 92: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0309 - accuracy: 0.9946 - val_loss: 0.6037 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 93/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0311 - accuracy: 0.9948\n",
            "Epoch 93: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0308 - accuracy: 0.9949 - val_loss: 0.6044 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 94/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.0302 - accuracy: 0.9956\n",
            "Epoch 94: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0303 - accuracy: 0.9956 - val_loss: 0.6049 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 95/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0298 - accuracy: 0.9953\n",
            "Epoch 95: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0300 - accuracy: 0.9952 - val_loss: 0.6054 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 96/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9952\n",
            "Epoch 96: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0294 - accuracy: 0.9952 - val_loss: 0.6055 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 97/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0296 - accuracy: 0.9952\n",
            "Epoch 97: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0295 - accuracy: 0.9952 - val_loss: 0.6059 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 98/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0299 - accuracy: 0.9936\n",
            "Epoch 98: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0298 - accuracy: 0.9937 - val_loss: 0.6067 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 99/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9944\n",
            "Epoch 99: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0302 - accuracy: 0.9944 - val_loss: 0.6077 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 100/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0292 - accuracy: 0.9950\n",
            "Epoch 100: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0292 - accuracy: 0.9951 - val_loss: 0.6070 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 101/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0301 - accuracy: 0.9946\n",
            "Epoch 101: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0299 - accuracy: 0.9947 - val_loss: 0.6079 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 102/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0294 - accuracy: 0.9948\n",
            "Epoch 102: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0293 - accuracy: 0.9949 - val_loss: 0.6087 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 103/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0293 - accuracy: 0.9950\n",
            "Epoch 103: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0291 - accuracy: 0.9951 - val_loss: 0.6086 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 104/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0293 - accuracy: 0.9943\n",
            "Epoch 104: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0292 - accuracy: 0.9944 - val_loss: 0.6105 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 105/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0295 - accuracy: 0.9951\n",
            "Epoch 105: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0293 - accuracy: 0.9952 - val_loss: 0.6115 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 106/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0289 - accuracy: 0.9962\n",
            "Epoch 106: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0288 - accuracy: 0.9963 - val_loss: 0.6112 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 107/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0297 - accuracy: 0.9941\n",
            "Epoch 107: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0298 - accuracy: 0.9939 - val_loss: 0.6122 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 108/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 0.9954\n",
            "Epoch 108: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0286 - accuracy: 0.9954 - val_loss: 0.6127 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 109/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0286 - accuracy: 0.9949\n",
            "Epoch 109: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0287 - accuracy: 0.9947 - val_loss: 0.6138 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 110/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0284 - accuracy: 0.9948\n",
            "Epoch 110: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0284 - accuracy: 0.9949 - val_loss: 0.6147 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 111/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0282 - accuracy: 0.9953\n",
            "Epoch 111: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0283 - accuracy: 0.9952 - val_loss: 0.6155 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 112/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0285 - accuracy: 0.9950\n",
            "Epoch 112: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0284 - accuracy: 0.9951 - val_loss: 0.6150 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 113/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0274 - accuracy: 0.9960\n",
            "Epoch 113: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0275 - accuracy: 0.9959 - val_loss: 0.6152 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 114/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0277 - accuracy: 0.9951\n",
            "Epoch 114: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0275 - accuracy: 0.9952 - val_loss: 0.6154 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 115/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0285 - accuracy: 0.9942\n",
            "Epoch 115: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0283 - accuracy: 0.9944 - val_loss: 0.6157 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 116/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0276 - accuracy: 0.9949\n",
            "Epoch 116: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0277 - accuracy: 0.9951 - val_loss: 0.6160 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 117/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0272 - accuracy: 0.9950\n",
            "Epoch 117: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0270 - accuracy: 0.9951 - val_loss: 0.6169 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 118/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0265 - accuracy: 0.9962\n",
            "Epoch 118: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0269 - accuracy: 0.9961 - val_loss: 0.6178 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 119/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9954\n",
            "Epoch 119: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0266 - accuracy: 0.9954 - val_loss: 0.6184 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 120/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0272 - accuracy: 0.9960\n",
            "Epoch 120: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0272 - accuracy: 0.9961 - val_loss: 0.6191 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 121/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0265 - accuracy: 0.9957\n",
            "Epoch 121: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0263 - accuracy: 0.9958 - val_loss: 0.6197 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 122/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0271 - accuracy: 0.9943\n",
            "Epoch 122: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0270 - accuracy: 0.9944 - val_loss: 0.6209 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 123/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0279 - accuracy: 0.9943\n",
            "Epoch 123: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0279 - accuracy: 0.9942 - val_loss: 0.6213 - val_accuracy: 0.7444 - lr: 1.0000e-06\n",
            "Epoch 124/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0261 - accuracy: 0.9962\n",
            "Epoch 124: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0261 - accuracy: 0.9963 - val_loss: 0.6210 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 125/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0265 - accuracy: 0.9962\n",
            "Epoch 125: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.0263 - accuracy: 0.9963 - val_loss: 0.6224 - val_accuracy: 0.7444 - lr: 1.0000e-06\n",
            "Epoch 126/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9949\n",
            "Epoch 126: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0268 - accuracy: 0.9949 - val_loss: 0.6217 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 127/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0257 - accuracy: 0.9960\n",
            "Epoch 127: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0255 - accuracy: 0.9961 - val_loss: 0.6221 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 128/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.0271 - accuracy: 0.9951\n",
            "Epoch 128: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0268 - accuracy: 0.9952 - val_loss: 0.6228 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 129/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0259 - accuracy: 0.9955\n",
            "Epoch 129: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0260 - accuracy: 0.9954 - val_loss: 0.6226 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 130/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0263 - accuracy: 0.9952\n",
            "Epoch 130: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0262 - accuracy: 0.9952 - val_loss: 0.6238 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 131/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0260 - accuracy: 0.9949\n",
            "Epoch 131: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0260 - accuracy: 0.9951 - val_loss: 0.6251 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 132/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0258 - accuracy: 0.9956\n",
            "Epoch 132: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0258 - accuracy: 0.9956 - val_loss: 0.6265 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 133/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.0254 - accuracy: 0.9949\n",
            "Epoch 133: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0253 - accuracy: 0.9949 - val_loss: 0.6261 - val_accuracy: 0.7444 - lr: 1.0000e-06\n",
            "Epoch 134/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9959\n",
            "Epoch 134: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0252 - accuracy: 0.9959 - val_loss: 0.6256 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 135/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0257 - accuracy: 0.9955\n",
            "Epoch 135: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0256 - accuracy: 0.9956 - val_loss: 0.6276 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 136/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.0264 - accuracy: 0.9949\n",
            "Epoch 136: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0261 - accuracy: 0.9951 - val_loss: 0.6266 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 137/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.0252 - accuracy: 0.9958\n",
            "Epoch 137: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0248 - accuracy: 0.9959 - val_loss: 0.6276 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 138/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0251 - accuracy: 0.9960\n",
            "Epoch 138: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0250 - accuracy: 0.9961 - val_loss: 0.6278 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 139/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9958\n",
            "Epoch 139: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0247 - accuracy: 0.9958 - val_loss: 0.6286 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 140/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.0245 - accuracy: 0.9951\n",
            "Epoch 140: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0244 - accuracy: 0.9952 - val_loss: 0.6302 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 141/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9966\n",
            "Epoch 141: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0241 - accuracy: 0.9966 - val_loss: 0.6298 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 142/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0247 - accuracy: 0.9951\n",
            "Epoch 142: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0247 - accuracy: 0.9951 - val_loss: 0.6306 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 143/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0241 - accuracy: 0.9968\n",
            "Epoch 143: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0240 - accuracy: 0.9968 - val_loss: 0.6312 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 144/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0238 - accuracy: 0.9962\n",
            "Epoch 144: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0238 - accuracy: 0.9961 - val_loss: 0.6310 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 145/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0236 - accuracy: 0.9962\n",
            "Epoch 145: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0241 - accuracy: 0.9959 - val_loss: 0.6327 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 146/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0241 - accuracy: 0.9969\n",
            "Epoch 146: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0238 - accuracy: 0.9969 - val_loss: 0.6334 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 147/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0237 - accuracy: 0.9959\n",
            "Epoch 147: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0236 - accuracy: 0.9959 - val_loss: 0.6337 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 148/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0245 - accuracy: 0.9957\n",
            "Epoch 148: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0245 - accuracy: 0.9958 - val_loss: 0.6344 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 149/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0233 - accuracy: 0.9952\n",
            "Epoch 149: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0231 - accuracy: 0.9952 - val_loss: 0.6350 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 150/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0238 - accuracy: 0.9964\n",
            "Epoch 150: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0238 - accuracy: 0.9964 - val_loss: 0.6358 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 151/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0228 - accuracy: 0.9966\n",
            "Epoch 151: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0228 - accuracy: 0.9966 - val_loss: 0.6354 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 152/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0232 - accuracy: 0.9962\n",
            "Epoch 152: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0232 - accuracy: 0.9963 - val_loss: 0.6361 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 153/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9975\n",
            "Epoch 153: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0232 - accuracy: 0.9975 - val_loss: 0.6362 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 154/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9961\n",
            "Epoch 154: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0235 - accuracy: 0.9961 - val_loss: 0.6366 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 155/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0227 - accuracy: 0.9962\n",
            "Epoch 155: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0227 - accuracy: 0.9961 - val_loss: 0.6377 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 156/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0228 - accuracy: 0.9967\n",
            "Epoch 156: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0228 - accuracy: 0.9968 - val_loss: 0.6386 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 157/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0220 - accuracy: 0.9967\n",
            "Epoch 157: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0221 - accuracy: 0.9968 - val_loss: 0.6385 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 158/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0229 - accuracy: 0.9957\n",
            "Epoch 158: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0228 - accuracy: 0.9958 - val_loss: 0.6397 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 159/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0224 - accuracy: 0.9958\n",
            "Epoch 159: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0226 - accuracy: 0.9956 - val_loss: 0.6397 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 160/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0222 - accuracy: 0.9964\n",
            "Epoch 160: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0224 - accuracy: 0.9961 - val_loss: 0.6406 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 161/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.0235 - accuracy: 0.9961\n",
            "Epoch 161: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0233 - accuracy: 0.9963 - val_loss: 0.6403 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 162/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0218 - accuracy: 0.9969\n",
            "Epoch 162: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0221 - accuracy: 0.9968 - val_loss: 0.6416 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 163/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0221 - accuracy: 0.9969\n",
            "Epoch 163: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0224 - accuracy: 0.9968 - val_loss: 0.6422 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 164/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0225 - accuracy: 0.9954\n",
            "Epoch 164: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0224 - accuracy: 0.9954 - val_loss: 0.6431 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 165/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0221 - accuracy: 0.9953\n",
            "Epoch 165: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0220 - accuracy: 0.9954 - val_loss: 0.6438 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 166/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0225 - accuracy: 0.9965\n",
            "Epoch 166: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0224 - accuracy: 0.9966 - val_loss: 0.6442 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 167/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.0221 - accuracy: 0.9965\n",
            "Epoch 167: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0220 - accuracy: 0.9964 - val_loss: 0.6438 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 168/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0220 - accuracy: 0.9962\n",
            "Epoch 168: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0220 - accuracy: 0.9963 - val_loss: 0.6454 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 169/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9959\n",
            "Epoch 169: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0225 - accuracy: 0.9959 - val_loss: 0.6463 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 170/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0215 - accuracy: 0.9969\n",
            "Epoch 170: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0215 - accuracy: 0.9969 - val_loss: 0.6463 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 171/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0220 - accuracy: 0.9964\n",
            "Epoch 171: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0222 - accuracy: 0.9964 - val_loss: 0.6472 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 172/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9956\n",
            "Epoch 172: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0214 - accuracy: 0.9956 - val_loss: 0.6483 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 173/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0215 - accuracy: 0.9969\n",
            "Epoch 173: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0214 - accuracy: 0.9969 - val_loss: 0.6478 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 174/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0215 - accuracy: 0.9967\n",
            "Epoch 174: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0219 - accuracy: 0.9964 - val_loss: 0.6481 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 175/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.0212 - accuracy: 0.9970\n",
            "Epoch 175: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0211 - accuracy: 0.9971 - val_loss: 0.6498 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 176/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.0219 - accuracy: 0.9967\n",
            "Epoch 176: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0216 - accuracy: 0.9968 - val_loss: 0.6484 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 177/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9969\n",
            "Epoch 177: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0204 - accuracy: 0.9969 - val_loss: 0.6487 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 178/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0217 - accuracy: 0.9956\n",
            "Epoch 178: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0220 - accuracy: 0.9952 - val_loss: 0.6506 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 179/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0206 - accuracy: 0.9976\n",
            "Epoch 179: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0205 - accuracy: 0.9976 - val_loss: 0.6518 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 180/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0209 - accuracy: 0.9962\n",
            "Epoch 180: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0209 - accuracy: 0.9963 - val_loss: 0.6513 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 181/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0205 - accuracy: 0.9969\n",
            "Epoch 181: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0205 - accuracy: 0.9969 - val_loss: 0.6523 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 182/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0201 - accuracy: 0.9967\n",
            "Epoch 182: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0202 - accuracy: 0.9968 - val_loss: 0.6526 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 183/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0214 - accuracy: 0.9958\n",
            "Epoch 183: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0214 - accuracy: 0.9958 - val_loss: 0.6537 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 184/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0204 - accuracy: 0.9962\n",
            "Epoch 184: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0204 - accuracy: 0.9963 - val_loss: 0.6551 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 185/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0203 - accuracy: 0.9974\n",
            "Epoch 185: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0203 - accuracy: 0.9973 - val_loss: 0.6549 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 186/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0201 - accuracy: 0.9962\n",
            "Epoch 186: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0202 - accuracy: 0.9963 - val_loss: 0.6555 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 187/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0199 - accuracy: 0.9963\n",
            "Epoch 187: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0200 - accuracy: 0.9964 - val_loss: 0.6561 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 188/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0198 - accuracy: 0.9971\n",
            "Epoch 188: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0196 - accuracy: 0.9971 - val_loss: 0.6551 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 189/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0198 - accuracy: 0.9969\n",
            "Epoch 189: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0199 - accuracy: 0.9968 - val_loss: 0.6557 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 190/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0198 - accuracy: 0.9970\n",
            "Epoch 190: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0198 - accuracy: 0.9969 - val_loss: 0.6567 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 191/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0201 - accuracy: 0.9965\n",
            "Epoch 191: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0200 - accuracy: 0.9966 - val_loss: 0.6580 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 192/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0188 - accuracy: 0.9974\n",
            "Epoch 192: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0189 - accuracy: 0.9975 - val_loss: 0.6578 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 193/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.0188 - accuracy: 0.9979\n",
            "Epoch 193: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0190 - accuracy: 0.9976 - val_loss: 0.6577 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 194/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0196 - accuracy: 0.9966\n",
            "Epoch 194: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0195 - accuracy: 0.9966 - val_loss: 0.6585 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 195/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0198 - accuracy: 0.9962\n",
            "Epoch 195: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0197 - accuracy: 0.9963 - val_loss: 0.6586 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 196/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0198 - accuracy: 0.9967\n",
            "Epoch 196: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0197 - accuracy: 0.9968 - val_loss: 0.6603 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 197/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9971\n",
            "Epoch 197: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0200 - accuracy: 0.9971 - val_loss: 0.6613 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 198/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0186 - accuracy: 0.9970\n",
            "Epoch 198: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0187 - accuracy: 0.9971 - val_loss: 0.6606 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 199/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9969\n",
            "Epoch 199: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0196 - accuracy: 0.9969 - val_loss: 0.6628 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 200/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0197 - accuracy: 0.9961\n",
            "Epoch 200: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0197 - accuracy: 0.9961 - val_loss: 0.6638 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 201/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0194 - accuracy: 0.9970\n",
            "Epoch 201: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0194 - accuracy: 0.9969 - val_loss: 0.6636 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 202/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0195 - accuracy: 0.9967\n",
            "Epoch 202: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0193 - accuracy: 0.9968 - val_loss: 0.6638 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 203/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9966\n",
            "Epoch 203: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0189 - accuracy: 0.9966 - val_loss: 0.6644 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 204/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0184 - accuracy: 0.9977\n",
            "Epoch 204: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0183 - accuracy: 0.9978 - val_loss: 0.6658 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 205/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0191 - accuracy: 0.9962\n",
            "Epoch 205: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0194 - accuracy: 0.9959 - val_loss: 0.6668 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 206/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0194 - accuracy: 0.9958\n",
            "Epoch 206: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0193 - accuracy: 0.9959 - val_loss: 0.6667 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 207/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0192 - accuracy: 0.9964\n",
            "Epoch 207: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0193 - accuracy: 0.9964 - val_loss: 0.6672 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 208/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0184 - accuracy: 0.9967\n",
            "Epoch 208: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0183 - accuracy: 0.9968 - val_loss: 0.6679 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 209/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0187 - accuracy: 0.9971\n",
            "Epoch 209: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0187 - accuracy: 0.9971 - val_loss: 0.6690 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 210/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0175 - accuracy: 0.9978\n",
            "Epoch 210: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0175 - accuracy: 0.9978 - val_loss: 0.6687 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 211/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0187 - accuracy: 0.9966\n",
            "Epoch 211: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0186 - accuracy: 0.9966 - val_loss: 0.6696 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 212/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0185 - accuracy: 0.9969\n",
            "Epoch 212: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0184 - accuracy: 0.9969 - val_loss: 0.6702 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 213/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0181 - accuracy: 0.9967\n",
            "Epoch 213: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0181 - accuracy: 0.9968 - val_loss: 0.6700 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 214/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.0178 - accuracy: 0.9967\n",
            "Epoch 214: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0176 - accuracy: 0.9968 - val_loss: 0.6700 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 215/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0183 - accuracy: 0.9960\n",
            "Epoch 215: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0184 - accuracy: 0.9961 - val_loss: 0.6722 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 216/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0180 - accuracy: 0.9974\n",
            "Epoch 216: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0182 - accuracy: 0.9975 - val_loss: 0.6728 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 217/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0182 - accuracy: 0.9974\n",
            "Epoch 217: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0181 - accuracy: 0.9975 - val_loss: 0.6720 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 218/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9963\n",
            "Epoch 218: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0175 - accuracy: 0.9963 - val_loss: 0.6725 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 219/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0175 - accuracy: 0.9974\n",
            "Epoch 219: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0176 - accuracy: 0.9975 - val_loss: 0.6729 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 220/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9964\n",
            "Epoch 220: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0174 - accuracy: 0.9964 - val_loss: 0.6728 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 221/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0176 - accuracy: 0.9969\n",
            "Epoch 221: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0175 - accuracy: 0.9969 - val_loss: 0.6738 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 222/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9963\n",
            "Epoch 222: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0176 - accuracy: 0.9963 - val_loss: 0.6747 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 223/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0169 - accuracy: 0.9972\n",
            "Epoch 223: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0169 - accuracy: 0.9973 - val_loss: 0.6754 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 224/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0174 - accuracy: 0.9974\n",
            "Epoch 224: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0172 - accuracy: 0.9975 - val_loss: 0.6761 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 225/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0168 - accuracy: 0.9974\n",
            "Epoch 225: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0169 - accuracy: 0.9973 - val_loss: 0.6769 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 226/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0175 - accuracy: 0.9970\n",
            "Epoch 226: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0172 - accuracy: 0.9971 - val_loss: 0.6778 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 227/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.0174 - accuracy: 0.9968\n",
            "Epoch 227: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0173 - accuracy: 0.9969 - val_loss: 0.6785 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 228/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0171 - accuracy: 0.9974\n",
            "Epoch 228: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0173 - accuracy: 0.9973 - val_loss: 0.6783 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 229/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0166 - accuracy: 0.9977\n",
            "Epoch 229: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0165 - accuracy: 0.9978 - val_loss: 0.6799 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 230/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0164 - accuracy: 0.9974\n",
            "Epoch 230: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0167 - accuracy: 0.9973 - val_loss: 0.6786 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 231/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0164 - accuracy: 0.9979\n",
            "Epoch 231: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0165 - accuracy: 0.9980 - val_loss: 0.6800 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 232/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0163 - accuracy: 0.9973\n",
            "Epoch 232: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0162 - accuracy: 0.9973 - val_loss: 0.6803 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 233/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0174 - accuracy: 0.9978\n",
            "Epoch 233: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0174 - accuracy: 0.9978 - val_loss: 0.6814 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 234/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0175 - accuracy: 0.9969\n",
            "Epoch 234: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0172 - accuracy: 0.9969 - val_loss: 0.6814 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 235/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0162 - accuracy: 0.9972\n",
            "Epoch 235: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0161 - accuracy: 0.9973 - val_loss: 0.6817 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 236/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9981\n",
            "Epoch 236: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0161 - accuracy: 0.9981 - val_loss: 0.6829 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 237/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9969\n",
            "Epoch 237: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0164 - accuracy: 0.9969 - val_loss: 0.6838 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 238/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0158 - accuracy: 0.9981\n",
            "Epoch 238: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0157 - accuracy: 0.9981 - val_loss: 0.6837 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 239/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9973\n",
            "Epoch 239: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0165 - accuracy: 0.9973 - val_loss: 0.6844 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 240/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0165 - accuracy: 0.9973\n",
            "Epoch 240: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0165 - accuracy: 0.9973 - val_loss: 0.6848 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 241/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0166 - accuracy: 0.9972\n",
            "Epoch 241: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0166 - accuracy: 0.9973 - val_loss: 0.6850 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 242/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0162 - accuracy: 0.9970\n",
            "Epoch 242: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0163 - accuracy: 0.9969 - val_loss: 0.6864 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 243/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9981\n",
            "Epoch 243: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0159 - accuracy: 0.9981 - val_loss: 0.6872 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 244/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0153 - accuracy: 0.9977\n",
            "Epoch 244: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0153 - accuracy: 0.9978 - val_loss: 0.6874 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 245/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0159 - accuracy: 0.9971\n",
            "Epoch 245: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0159 - accuracy: 0.9971 - val_loss: 0.6868 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 246/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0161 - accuracy: 0.9974\n",
            "Epoch 246: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0161 - accuracy: 0.9975 - val_loss: 0.6885 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 247/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0156 - accuracy: 0.9970\n",
            "Epoch 247: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0157 - accuracy: 0.9969 - val_loss: 0.6887 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 248/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.0162 - accuracy: 0.9974\n",
            "Epoch 248: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0160 - accuracy: 0.9975 - val_loss: 0.6901 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 249/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9969\n",
            "Epoch 249: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0163 - accuracy: 0.9969 - val_loss: 0.6906 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 250/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0164 - accuracy: 0.9972\n",
            "Epoch 250: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0164 - accuracy: 0.9971 - val_loss: 0.6917 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 251/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0157 - accuracy: 0.9976\n",
            "Epoch 251: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0159 - accuracy: 0.9975 - val_loss: 0.6917 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 252/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9969\n",
            "Epoch 252: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0159 - accuracy: 0.9969 - val_loss: 0.6918 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 253/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9973\n",
            "Epoch 253: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0159 - accuracy: 0.9973 - val_loss: 0.6932 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 254/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0152 - accuracy: 0.9978\n",
            "Epoch 254: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0152 - accuracy: 0.9978 - val_loss: 0.6935 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 255/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.0152 - accuracy: 0.9970\n",
            "Epoch 255: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0153 - accuracy: 0.9968 - val_loss: 0.6942 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 256/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0150 - accuracy: 0.9976\n",
            "Epoch 256: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0149 - accuracy: 0.9976 - val_loss: 0.6938 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 257/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.0154 - accuracy: 0.9977\n",
            "Epoch 257: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0156 - accuracy: 0.9976 - val_loss: 0.6950 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 258/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9981\n",
            "Epoch 258: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0145 - accuracy: 0.9981 - val_loss: 0.6949 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 259/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9971\n",
            "Epoch 259: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0147 - accuracy: 0.9971 - val_loss: 0.6954 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 260/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0151 - accuracy: 0.9981\n",
            "Epoch 260: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0151 - accuracy: 0.9981 - val_loss: 0.6965 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 261/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0153 - accuracy: 0.9974\n",
            "Epoch 261: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0152 - accuracy: 0.9975 - val_loss: 0.6982 - val_accuracy: 0.7437 - lr: 1.0000e-06\n",
            "Epoch 262/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0145 - accuracy: 0.9981\n",
            "Epoch 262: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0145 - accuracy: 0.9981 - val_loss: 0.6979 - val_accuracy: 0.7444 - lr: 1.0000e-06\n",
            "Epoch 263/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0143 - accuracy: 0.9972\n",
            "Epoch 263: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0147 - accuracy: 0.9971 - val_loss: 0.6980 - val_accuracy: 0.7444 - lr: 1.0000e-06\n",
            "Epoch 264/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0147 - accuracy: 0.9976\n",
            "Epoch 264: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0150 - accuracy: 0.9975 - val_loss: 0.6988 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 265/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0146 - accuracy: 0.9978\n",
            "Epoch 265: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0145 - accuracy: 0.9978 - val_loss: 0.6992 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 266/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0151 - accuracy: 0.9970\n",
            "Epoch 266: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0150 - accuracy: 0.9971 - val_loss: 0.7010 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 267/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0145 - accuracy: 0.9976\n",
            "Epoch 267: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0144 - accuracy: 0.9976 - val_loss: 0.7009 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 268/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0147 - accuracy: 0.9984\n",
            "Epoch 268: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0145 - accuracy: 0.9985 - val_loss: 0.7005 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 269/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0149 - accuracy: 0.9976\n",
            "Epoch 269: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0148 - accuracy: 0.9976 - val_loss: 0.7015 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 270/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0139 - accuracy: 0.9981\n",
            "Epoch 270: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0141 - accuracy: 0.9980 - val_loss: 0.7014 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 271/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0148 - accuracy: 0.9983\n",
            "Epoch 271: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0147 - accuracy: 0.9983 - val_loss: 0.7025 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 272/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0139 - accuracy: 0.9981\n",
            "Epoch 272: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0138 - accuracy: 0.9981 - val_loss: 0.7039 - val_accuracy: 0.7437 - lr: 1.0000e-06\n",
            "Epoch 273/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0148 - accuracy: 0.9976\n",
            "Epoch 273: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0148 - accuracy: 0.9976 - val_loss: 0.7039 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 274/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0143 - accuracy: 0.9974\n",
            "Epoch 274: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0144 - accuracy: 0.9975 - val_loss: 0.7058 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 275/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0146 - accuracy: 0.9977\n",
            "Epoch 275: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0144 - accuracy: 0.9978 - val_loss: 0.7052 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 276/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0138 - accuracy: 0.9980\n",
            "Epoch 276: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0138 - accuracy: 0.9980 - val_loss: 0.7055 - val_accuracy: 0.7444 - lr: 1.0000e-06\n",
            "Epoch 277/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0135 - accuracy: 0.9977\n",
            "Epoch 277: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0134 - accuracy: 0.9978 - val_loss: 0.7058 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 278/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0131 - accuracy: 0.9983\n",
            "Epoch 278: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0131 - accuracy: 0.9983 - val_loss: 0.7066 - val_accuracy: 0.7444 - lr: 1.0000e-06\n",
            "Epoch 279/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0144 - accuracy: 0.9976\n",
            "Epoch 279: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0143 - accuracy: 0.9976 - val_loss: 0.7073 - val_accuracy: 0.7444 - lr: 1.0000e-06\n",
            "Epoch 280/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0132 - accuracy: 0.9979\n",
            "Epoch 280: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0132 - accuracy: 0.9980 - val_loss: 0.7091 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 281/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0134 - accuracy: 0.9981\n",
            "Epoch 281: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0134 - accuracy: 0.9981 - val_loss: 0.7092 - val_accuracy: 0.7430 - lr: 1.0000e-06\n",
            "Epoch 282/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0143 - accuracy: 0.9968\n",
            "Epoch 282: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0142 - accuracy: 0.9968 - val_loss: 0.7090 - val_accuracy: 0.7444 - lr: 1.0000e-06\n",
            "Epoch 283/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9969\n",
            "Epoch 283: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0138 - accuracy: 0.9969 - val_loss: 0.7092 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 284/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9981\n",
            "Epoch 284: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0136 - accuracy: 0.9981 - val_loss: 0.7082 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 285/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0136 - accuracy: 0.9981\n",
            "Epoch 285: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0136 - accuracy: 0.9981 - val_loss: 0.7100 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 286/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0139 - accuracy: 0.9979\n",
            "Epoch 286: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0139 - accuracy: 0.9980 - val_loss: 0.7116 - val_accuracy: 0.7437 - lr: 1.0000e-06\n",
            "Epoch 287/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0134 - accuracy: 0.9978\n",
            "Epoch 287: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0135 - accuracy: 0.9976 - val_loss: 0.7131 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 288/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0129 - accuracy: 0.9984\n",
            "Epoch 288: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0134 - accuracy: 0.9981 - val_loss: 0.7125 - val_accuracy: 0.7437 - lr: 1.0000e-06\n",
            "Epoch 289/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0139 - accuracy: 0.9983\n",
            "Epoch 289: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0139 - accuracy: 0.9983 - val_loss: 0.7134 - val_accuracy: 0.7430 - lr: 1.0000e-06\n",
            "Epoch 290/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0133 - accuracy: 0.9984\n",
            "Epoch 290: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0134 - accuracy: 0.9985 - val_loss: 0.7136 - val_accuracy: 0.7430 - lr: 1.0000e-06\n",
            "Epoch 291/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0129 - accuracy: 0.9981\n",
            "Epoch 291: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0129 - accuracy: 0.9981 - val_loss: 0.7133 - val_accuracy: 0.7444 - lr: 1.0000e-06\n",
            "Epoch 292/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0125 - accuracy: 0.9991\n",
            "Epoch 292: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0125 - accuracy: 0.9992 - val_loss: 0.7141 - val_accuracy: 0.7437 - lr: 1.0000e-06\n",
            "Epoch 293/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0133 - accuracy: 0.9974\n",
            "Epoch 293: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0133 - accuracy: 0.9975 - val_loss: 0.7146 - val_accuracy: 0.7430 - lr: 1.0000e-06\n",
            "Epoch 294/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0134 - accuracy: 0.9979\n",
            "Epoch 294: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0133 - accuracy: 0.9980 - val_loss: 0.7167 - val_accuracy: 0.7437 - lr: 1.0000e-06\n",
            "Epoch 295/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0136 - accuracy: 0.9977\n",
            "Epoch 295: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0135 - accuracy: 0.9978 - val_loss: 0.7160 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 296/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0133 - accuracy: 0.9976\n",
            "Epoch 296: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0133 - accuracy: 0.9976 - val_loss: 0.7173 - val_accuracy: 0.7437 - lr: 1.0000e-06\n",
            "Epoch 297/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0128 - accuracy: 0.9985\n",
            "Epoch 297: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0127 - accuracy: 0.9985 - val_loss: 0.7175 - val_accuracy: 0.7430 - lr: 1.0000e-06\n",
            "Epoch 298/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0129 - accuracy: 0.9978\n",
            "Epoch 298: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0128 - accuracy: 0.9978 - val_loss: 0.7175 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 299/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0122 - accuracy: 0.9990\n",
            "Epoch 299: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0122 - accuracy: 0.9990 - val_loss: 0.7175 - val_accuracy: 0.7444 - lr: 1.0000e-06\n",
            "Epoch 300/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0128 - accuracy: 0.9978\n",
            "Epoch 300: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0127 - accuracy: 0.9978 - val_loss: 0.7193 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 301/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0125 - accuracy: 0.9978\n",
            "Epoch 301: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0125 - accuracy: 0.9978 - val_loss: 0.7198 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 302/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0121 - accuracy: 0.9991\n",
            "Epoch 302: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0122 - accuracy: 0.9990 - val_loss: 0.7200 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 303/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0127 - accuracy: 0.9981\n",
            "Epoch 303: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0126 - accuracy: 0.9981 - val_loss: 0.7202 - val_accuracy: 0.7430 - lr: 1.0000e-06\n",
            "Epoch 304/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0121 - accuracy: 0.9981\n",
            "Epoch 304: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0122 - accuracy: 0.9981 - val_loss: 0.7218 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 305/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0125 - accuracy: 0.9983\n",
            "Epoch 305: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0126 - accuracy: 0.9981 - val_loss: 0.7219 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 306/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0126 - accuracy: 0.9976\n",
            "Epoch 306: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0126 - accuracy: 0.9976 - val_loss: 0.7222 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 307/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0121 - accuracy: 0.9983\n",
            "Epoch 307: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0120 - accuracy: 0.9983 - val_loss: 0.7233 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 308/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0121 - accuracy: 0.9986\n",
            "Epoch 308: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0121 - accuracy: 0.9986 - val_loss: 0.7229 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 309/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0124 - accuracy: 0.9981\n",
            "Epoch 309: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0122 - accuracy: 0.9981 - val_loss: 0.7232 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 310/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0123 - accuracy: 0.9981\n",
            "Epoch 310: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0123 - accuracy: 0.9981 - val_loss: 0.7255 - val_accuracy: 0.7430 - lr: 1.0000e-06\n",
            "Epoch 311/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0126 - accuracy: 0.9979\n",
            "Epoch 311: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0126 - accuracy: 0.9980 - val_loss: 0.7248 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 312/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0121 - accuracy: 0.9979\n",
            "Epoch 312: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0120 - accuracy: 0.9980 - val_loss: 0.7266 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 313/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0116 - accuracy: 0.9986\n",
            "Epoch 313: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0117 - accuracy: 0.9986 - val_loss: 0.7270 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 314/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0116 - accuracy: 0.9981\n",
            "Epoch 314: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0120 - accuracy: 0.9980 - val_loss: 0.7269 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 315/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0116 - accuracy: 0.9976\n",
            "Epoch 315: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0118 - accuracy: 0.9976 - val_loss: 0.7282 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 316/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0119 - accuracy: 0.9977\n",
            "Epoch 316: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0119 - accuracy: 0.9978 - val_loss: 0.7281 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 317/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0114 - accuracy: 0.9986\n",
            "Epoch 317: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0114 - accuracy: 0.9986 - val_loss: 0.7300 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 318/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0119 - accuracy: 0.9978\n",
            "Epoch 318: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0120 - accuracy: 0.9978 - val_loss: 0.7296 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 319/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0116 - accuracy: 0.9986\n",
            "Epoch 319: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0116 - accuracy: 0.9986 - val_loss: 0.7298 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 320/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0113 - accuracy: 0.9983\n",
            "Epoch 320: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0113 - accuracy: 0.9983 - val_loss: 0.7294 - val_accuracy: 0.7444 - lr: 1.0000e-06\n",
            "Epoch 321/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0115 - accuracy: 0.9981\n",
            "Epoch 321: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0114 - accuracy: 0.9981 - val_loss: 0.7308 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 322/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0116 - accuracy: 0.9985\n",
            "Epoch 322: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0116 - accuracy: 0.9985 - val_loss: 0.7321 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 323/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0118 - accuracy: 0.9979\n",
            "Epoch 323: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0118 - accuracy: 0.9980 - val_loss: 0.7321 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 324/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0115 - accuracy: 0.9980\n",
            "Epoch 324: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0115 - accuracy: 0.9980 - val_loss: 0.7345 - val_accuracy: 0.7403 - lr: 1.0000e-06\n",
            "Epoch 325/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9981\n",
            "Epoch 325: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0117 - accuracy: 0.9981 - val_loss: 0.7346 - val_accuracy: 0.7403 - lr: 1.0000e-06\n",
            "Epoch 326/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0115 - accuracy: 0.9985\n",
            "Epoch 326: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0115 - accuracy: 0.9985 - val_loss: 0.7346 - val_accuracy: 0.7403 - lr: 1.0000e-06\n",
            "Epoch 327/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0117 - accuracy: 0.9981\n",
            "Epoch 327: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0117 - accuracy: 0.9981 - val_loss: 0.7360 - val_accuracy: 0.7403 - lr: 1.0000e-06\n",
            "Epoch 328/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0114 - accuracy: 0.9986\n",
            "Epoch 328: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0113 - accuracy: 0.9986 - val_loss: 0.7368 - val_accuracy: 0.7403 - lr: 1.0000e-06\n",
            "Epoch 329/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0112 - accuracy: 0.9985\n",
            "Epoch 329: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0112 - accuracy: 0.9985 - val_loss: 0.7372 - val_accuracy: 0.7403 - lr: 1.0000e-06\n",
            "Epoch 330/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0111 - accuracy: 0.9983\n",
            "Epoch 330: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0111 - accuracy: 0.9983 - val_loss: 0.7371 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 331/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0114 - accuracy: 0.9984\n",
            "Epoch 331: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0114 - accuracy: 0.9985 - val_loss: 0.7383 - val_accuracy: 0.7403 - lr: 1.0000e-06\n",
            "Epoch 332/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0116 - accuracy: 0.9978\n",
            "Epoch 332: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0116 - accuracy: 0.9978 - val_loss: 0.7388 - val_accuracy: 0.7403 - lr: 1.0000e-06\n",
            "Epoch 333/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9985\n",
            "Epoch 333: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0111 - accuracy: 0.9985 - val_loss: 0.7387 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 334/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.9986\n",
            "Epoch 334: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0110 - accuracy: 0.9986 - val_loss: 0.7393 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 335/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0108 - accuracy: 0.9988\n",
            "Epoch 335: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0108 - accuracy: 0.9988 - val_loss: 0.7402 - val_accuracy: 0.7403 - lr: 1.0000e-06\n",
            "Epoch 336/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0106 - accuracy: 0.9986\n",
            "Epoch 336: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0106 - accuracy: 0.9986 - val_loss: 0.7413 - val_accuracy: 0.7396 - lr: 1.0000e-06\n",
            "Epoch 337/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0112 - accuracy: 0.9984\n",
            "Epoch 337: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0112 - accuracy: 0.9985 - val_loss: 0.7408 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 338/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0108 - accuracy: 0.9983\n",
            "Epoch 338: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0110 - accuracy: 0.9981 - val_loss: 0.7408 - val_accuracy: 0.7430 - lr: 1.0000e-06\n",
            "Epoch 339/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0108 - accuracy: 0.9983\n",
            "Epoch 339: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0108 - accuracy: 0.9981 - val_loss: 0.7421 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 340/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0105 - accuracy: 0.9985\n",
            "Epoch 340: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0105 - accuracy: 0.9985 - val_loss: 0.7434 - val_accuracy: 0.7383 - lr: 1.0000e-06\n",
            "Epoch 341/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 0.9988\n",
            "Epoch 341: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0104 - accuracy: 0.9988 - val_loss: 0.7438 - val_accuracy: 0.7403 - lr: 1.0000e-06\n",
            "Epoch 342/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0111 - accuracy: 0.9976\n",
            "Epoch 342: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0111 - accuracy: 0.9976 - val_loss: 0.7442 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 343/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.9988\n",
            "Epoch 343: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0108 - accuracy: 0.9988 - val_loss: 0.7441 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 344/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0099 - accuracy: 0.9986\n",
            "Epoch 344: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0100 - accuracy: 0.9985 - val_loss: 0.7453 - val_accuracy: 0.7403 - lr: 1.0000e-06\n",
            "Epoch 345/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0104 - accuracy: 0.9985\n",
            "Epoch 345: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0104 - accuracy: 0.9985 - val_loss: 0.7449 - val_accuracy: 0.7430 - lr: 1.0000e-06\n",
            "Epoch 346/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0106 - accuracy: 0.9984\n",
            "Epoch 346: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0106 - accuracy: 0.9985 - val_loss: 0.7461 - val_accuracy: 0.7403 - lr: 1.0000e-06\n",
            "Epoch 347/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0109 - accuracy: 0.9981\n",
            "Epoch 347: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0109 - accuracy: 0.9981 - val_loss: 0.7473 - val_accuracy: 0.7403 - lr: 1.0000e-06\n",
            "Epoch 348/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0107 - accuracy: 0.9985\n",
            "Epoch 348: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0107 - accuracy: 0.9985 - val_loss: 0.7475 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 349/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0102 - accuracy: 0.9986\n",
            "Epoch 349: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0103 - accuracy: 0.9985 - val_loss: 0.7483 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 350/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0106 - accuracy: 0.9979\n",
            "Epoch 350: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0105 - accuracy: 0.9980 - val_loss: 0.7485 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 351/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0099 - accuracy: 0.9981\n",
            "Epoch 351: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0099 - accuracy: 0.9981 - val_loss: 0.7487 - val_accuracy: 0.7430 - lr: 1.0000e-06\n",
            "Epoch 352/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0105 - accuracy: 0.9984\n",
            "Epoch 352: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0104 - accuracy: 0.9985 - val_loss: 0.7487 - val_accuracy: 0.7437 - lr: 1.0000e-06\n",
            "Epoch 353/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0100 - accuracy: 0.9991\n",
            "Epoch 353: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0100 - accuracy: 0.9992 - val_loss: 0.7495 - val_accuracy: 0.7437 - lr: 1.0000e-06\n",
            "Epoch 354/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0098 - accuracy: 0.9986\n",
            "Epoch 354: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0098 - accuracy: 0.9986 - val_loss: 0.7507 - val_accuracy: 0.7430 - lr: 1.0000e-06\n",
            "Epoch 355/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 0.9981\n",
            "Epoch 355: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0101 - accuracy: 0.9981 - val_loss: 0.7494 - val_accuracy: 0.7444 - lr: 1.0000e-06\n",
            "Epoch 356/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0103 - accuracy: 0.9988\n",
            "Epoch 356: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0101 - accuracy: 0.9988 - val_loss: 0.7512 - val_accuracy: 0.7430 - lr: 1.0000e-06\n",
            "Epoch 357/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0099 - accuracy: 0.9991\n",
            "Epoch 357: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0103 - accuracy: 0.9988 - val_loss: 0.7511 - val_accuracy: 0.7444 - lr: 1.0000e-06\n",
            "Epoch 358/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0094 - accuracy: 0.9990\n",
            "Epoch 358: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0097 - accuracy: 0.9988 - val_loss: 0.7512 - val_accuracy: 0.7437 - lr: 1.0000e-06\n",
            "Epoch 359/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0106 - accuracy: 0.9981\n",
            "Epoch 359: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0106 - accuracy: 0.9981 - val_loss: 0.7512 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 360/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 0.9981\n",
            "Epoch 360: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0101 - accuracy: 0.9981 - val_loss: 0.7517 - val_accuracy: 0.7444 - lr: 1.0000e-06\n",
            "Epoch 361/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0096 - accuracy: 0.9988\n",
            "Epoch 361: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0097 - accuracy: 0.9988 - val_loss: 0.7535 - val_accuracy: 0.7437 - lr: 1.0000e-06\n",
            "Epoch 362/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9985\n",
            "Epoch 362: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0100 - accuracy: 0.9985 - val_loss: 0.7547 - val_accuracy: 0.7437 - lr: 1.0000e-06\n",
            "Epoch 363/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0105 - accuracy: 0.9979\n",
            "Epoch 363: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0104 - accuracy: 0.9980 - val_loss: 0.7565 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 364/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0096 - accuracy: 0.9983\n",
            "Epoch 364: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0097 - accuracy: 0.9981 - val_loss: 0.7562 - val_accuracy: 0.7430 - lr: 1.0000e-06\n",
            "Epoch 365/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0094 - accuracy: 0.9991\n",
            "Epoch 365: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0095 - accuracy: 0.9992 - val_loss: 0.7558 - val_accuracy: 0.7430 - lr: 1.0000e-06\n",
            "Epoch 366/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0099 - accuracy: 0.9983\n",
            "Epoch 366: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0099 - accuracy: 0.9983 - val_loss: 0.7568 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 367/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0097 - accuracy: 0.9990\n",
            "Epoch 367: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0096 - accuracy: 0.9990 - val_loss: 0.7578 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 368/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0095 - accuracy: 0.9984\n",
            "Epoch 368: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0094 - accuracy: 0.9985 - val_loss: 0.7598 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 369/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0100 - accuracy: 0.9988\n",
            "Epoch 369: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0099 - accuracy: 0.9988 - val_loss: 0.7602 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 370/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0092 - accuracy: 0.9991\n",
            "Epoch 370: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0092 - accuracy: 0.9992 - val_loss: 0.7610 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 371/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0099 - accuracy: 0.9988\n",
            "Epoch 371: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0099 - accuracy: 0.9988 - val_loss: 0.7609 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 372/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0094 - accuracy: 0.9983\n",
            "Epoch 372: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0094 - accuracy: 0.9983 - val_loss: 0.7593 - val_accuracy: 0.7437 - lr: 1.0000e-06\n",
            "Epoch 373/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0092 - accuracy: 0.9988\n",
            "Epoch 373: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0093 - accuracy: 0.9988 - val_loss: 0.7603 - val_accuracy: 0.7437 - lr: 1.0000e-06\n",
            "Epoch 374/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0088 - accuracy: 0.9990\n",
            "Epoch 374: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0088 - accuracy: 0.9990 - val_loss: 0.7615 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 375/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0092 - accuracy: 0.9983\n",
            "Epoch 375: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0091 - accuracy: 0.9983 - val_loss: 0.7619 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 376/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.0093 - accuracy: 0.9988\n",
            "Epoch 376: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0092 - accuracy: 0.9988 - val_loss: 0.7627 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 377/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0095 - accuracy: 0.9985\n",
            "Epoch 377: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0095 - accuracy: 0.9985 - val_loss: 0.7625 - val_accuracy: 0.7437 - lr: 1.0000e-06\n",
            "Epoch 378/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0091 - accuracy: 0.9988\n",
            "Epoch 378: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0091 - accuracy: 0.9988 - val_loss: 0.7639 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 379/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.9988\n",
            "Epoch 379: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0089 - accuracy: 0.9988 - val_loss: 0.7647 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 380/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0098 - accuracy: 0.9985\n",
            "Epoch 380: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0098 - accuracy: 0.9985 - val_loss: 0.7640 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 381/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0086 - accuracy: 0.9990\n",
            "Epoch 381: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0085 - accuracy: 0.9990 - val_loss: 0.7653 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 382/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 0.9990\n",
            "Epoch 382: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0093 - accuracy: 0.9990 - val_loss: 0.7653 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 383/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0090 - accuracy: 0.9983\n",
            "Epoch 383: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0092 - accuracy: 0.9981 - val_loss: 0.7664 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 384/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0087 - accuracy: 0.9990\n",
            "Epoch 384: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0088 - accuracy: 0.9990 - val_loss: 0.7689 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 385/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0091 - accuracy: 0.9986\n",
            "Epoch 385: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0091 - accuracy: 0.9986 - val_loss: 0.7681 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 386/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0089 - accuracy: 0.9984\n",
            "Epoch 386: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.7691 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 387/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0095 - accuracy: 0.9981\n",
            "Epoch 387: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0095 - accuracy: 0.9981 - val_loss: 0.7709 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 388/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0090 - accuracy: 0.9988\n",
            "Epoch 388: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0090 - accuracy: 0.9988 - val_loss: 0.7703 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 389/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0089 - accuracy: 0.9990\n",
            "Epoch 389: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0088 - accuracy: 0.9990 - val_loss: 0.7705 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 390/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0087 - accuracy: 0.9990\n",
            "Epoch 390: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0087 - accuracy: 0.9990 - val_loss: 0.7707 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 391/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.9985\n",
            "Epoch 391: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.7717 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 392/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0085 - accuracy: 0.9990\n",
            "Epoch 392: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0087 - accuracy: 0.9988 - val_loss: 0.7729 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 393/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.0083 - accuracy: 0.9991\n",
            "Epoch 393: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0082 - accuracy: 0.9992 - val_loss: 0.7738 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 394/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.9995\n",
            "Epoch 394: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0087 - accuracy: 0.9995 - val_loss: 0.7742 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 395/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0085 - accuracy: 0.9990\n",
            "Epoch 395: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0084 - accuracy: 0.9990 - val_loss: 0.7741 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 396/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0083 - accuracy: 0.9991\n",
            "Epoch 396: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0083 - accuracy: 0.9992 - val_loss: 0.7747 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 397/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0085 - accuracy: 0.9986\n",
            "Epoch 397: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0085 - accuracy: 0.9986 - val_loss: 0.7752 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 398/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0083 - accuracy: 0.9990\n",
            "Epoch 398: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0084 - accuracy: 0.9988 - val_loss: 0.7752 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 399/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0091 - accuracy: 0.9981\n",
            "Epoch 399: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0090 - accuracy: 0.9981 - val_loss: 0.7784 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 400/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0085 - accuracy: 0.9990\n",
            "Epoch 400: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0085 - accuracy: 0.9990 - val_loss: 0.7789 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 401/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0084 - accuracy: 0.9993\n",
            "Epoch 401: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0084 - accuracy: 0.9993 - val_loss: 0.7785 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 402/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0085 - accuracy: 0.9993\n",
            "Epoch 402: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0084 - accuracy: 0.9993 - val_loss: 0.7789 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 403/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0082 - accuracy: 0.9988\n",
            "Epoch 403: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0082 - accuracy: 0.9988 - val_loss: 0.7787 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 404/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0085 - accuracy: 0.9988\n",
            "Epoch 404: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0086 - accuracy: 0.9988 - val_loss: 0.7795 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 405/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0088 - accuracy: 0.9985\n",
            "Epoch 405: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.7807 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 406/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0084 - accuracy: 0.9984\n",
            "Epoch 406: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0083 - accuracy: 0.9985 - val_loss: 0.7807 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 407/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0081 - accuracy: 0.9990\n",
            "Epoch 407: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0084 - accuracy: 0.9988 - val_loss: 0.7820 - val_accuracy: 0.7403 - lr: 1.0000e-06\n",
            "Epoch 408/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.9985\n",
            "Epoch 408: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.7825 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 409/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.9988\n",
            "Epoch 409: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0084 - accuracy: 0.9988 - val_loss: 0.7834 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 410/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0081 - accuracy: 0.9990\n",
            "Epoch 410: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0081 - accuracy: 0.9990 - val_loss: 0.7830 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 411/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9991\n",
            "Epoch 411: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0075 - accuracy: 0.9992 - val_loss: 0.7839 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 412/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0084 - accuracy: 0.9993\n",
            "Epoch 412: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0085 - accuracy: 0.9993 - val_loss: 0.7844 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 413/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.9986\n",
            "Epoch 413: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0082 - accuracy: 0.9986 - val_loss: 0.7860 - val_accuracy: 0.7403 - lr: 1.0000e-06\n",
            "Epoch 414/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0077 - accuracy: 0.9992\n",
            "Epoch 414: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0077 - accuracy: 0.9992 - val_loss: 0.7863 - val_accuracy: 0.7403 - lr: 1.0000e-06\n",
            "Epoch 415/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0078 - accuracy: 0.9988\n",
            "Epoch 415: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0080 - accuracy: 0.9988 - val_loss: 0.7876 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 416/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0078 - accuracy: 0.9991\n",
            "Epoch 416: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0078 - accuracy: 0.9992 - val_loss: 0.7868 - val_accuracy: 0.7396 - lr: 1.0000e-06\n",
            "Epoch 417/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9997\n",
            "Epoch 417: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0077 - accuracy: 0.9993 - val_loss: 0.7879 - val_accuracy: 0.7403 - lr: 1.0000e-06\n",
            "Epoch 418/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.9986\n",
            "Epoch 418: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0082 - accuracy: 0.9986 - val_loss: 0.7888 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 419/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.9990\n",
            "Epoch 419: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0079 - accuracy: 0.9990 - val_loss: 0.7888 - val_accuracy: 0.7403 - lr: 1.0000e-06\n",
            "Epoch 420/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0076 - accuracy: 0.9992\n",
            "Epoch 420: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0076 - accuracy: 0.9992 - val_loss: 0.7895 - val_accuracy: 0.7396 - lr: 1.0000e-06\n",
            "Epoch 421/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0080 - accuracy: 0.9990\n",
            "Epoch 421: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0081 - accuracy: 0.9988 - val_loss: 0.7912 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 422/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0080 - accuracy: 0.9990\n",
            "Epoch 422: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0080 - accuracy: 0.9990 - val_loss: 0.7913 - val_accuracy: 0.7403 - lr: 1.0000e-06\n",
            "Epoch 423/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0077 - accuracy: 0.9988\n",
            "Epoch 423: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0077 - accuracy: 0.9988 - val_loss: 0.7918 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 424/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.9988\n",
            "Epoch 424: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0079 - accuracy: 0.9988 - val_loss: 0.7941 - val_accuracy: 0.7396 - lr: 1.0000e-06\n",
            "Epoch 425/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0078 - accuracy: 0.9983\n",
            "Epoch 425: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0079 - accuracy: 0.9983 - val_loss: 0.7933 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 426/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0077 - accuracy: 0.9990\n",
            "Epoch 426: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0078 - accuracy: 0.9988 - val_loss: 0.7932 - val_accuracy: 0.7403 - lr: 1.0000e-06\n",
            "Epoch 427/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.9988\n",
            "Epoch 427: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0079 - accuracy: 0.9988 - val_loss: 0.7930 - val_accuracy: 0.7396 - lr: 1.0000e-06\n",
            "Epoch 428/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9995\n",
            "Epoch 428: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0071 - accuracy: 0.9995 - val_loss: 0.7941 - val_accuracy: 0.7396 - lr: 1.0000e-06\n",
            "Epoch 429/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9995\n",
            "Epoch 429: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0075 - accuracy: 0.9995 - val_loss: 0.7947 - val_accuracy: 0.7396 - lr: 1.0000e-06\n",
            "Epoch 430/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0080 - accuracy: 0.9993\n",
            "Epoch 430: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0079 - accuracy: 0.9993 - val_loss: 0.7961 - val_accuracy: 0.7403 - lr: 1.0000e-06\n",
            "Epoch 431/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9990\n",
            "Epoch 431: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0075 - accuracy: 0.9990 - val_loss: 0.7967 - val_accuracy: 0.7396 - lr: 1.0000e-06\n",
            "Epoch 432/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 0.9988\n",
            "Epoch 432: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0078 - accuracy: 0.9988 - val_loss: 0.7973 - val_accuracy: 0.7396 - lr: 1.0000e-06\n",
            "Epoch 433/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9991\n",
            "Epoch 433: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0073 - accuracy: 0.9992 - val_loss: 0.7976 - val_accuracy: 0.7403 - lr: 1.0000e-06\n",
            "Epoch 434/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0070 - accuracy: 0.9993\n",
            "Epoch 434: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0070 - accuracy: 0.9993 - val_loss: 0.7982 - val_accuracy: 0.7403 - lr: 1.0000e-06\n",
            "Epoch 435/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0078 - accuracy: 0.9991\n",
            "Epoch 435: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0077 - accuracy: 0.9992 - val_loss: 0.8000 - val_accuracy: 0.7396 - lr: 1.0000e-06\n",
            "Epoch 436/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9988\n",
            "Epoch 436: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0074 - accuracy: 0.9988 - val_loss: 0.7997 - val_accuracy: 0.7396 - lr: 1.0000e-06\n",
            "Epoch 437/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0077 - accuracy: 0.9988\n",
            "Epoch 437: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0077 - accuracy: 0.9988 - val_loss: 0.7997 - val_accuracy: 0.7390 - lr: 1.0000e-06\n",
            "Epoch 438/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0070 - accuracy: 0.9997\n",
            "Epoch 438: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0069 - accuracy: 0.9997 - val_loss: 0.7997 - val_accuracy: 0.7369 - lr: 1.0000e-06\n",
            "Epoch 439/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9988\n",
            "Epoch 439: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0073 - accuracy: 0.9988 - val_loss: 0.8016 - val_accuracy: 0.7396 - lr: 1.0000e-06\n",
            "Epoch 440/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9991\n",
            "Epoch 440: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.0072 - accuracy: 0.9992 - val_loss: 0.8022 - val_accuracy: 0.7396 - lr: 1.0000e-06\n",
            "Epoch 441/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9991\n",
            "Epoch 441: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0071 - accuracy: 0.9992 - val_loss: 0.8017 - val_accuracy: 0.7376 - lr: 1.0000e-06\n",
            "Epoch 442/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9988\n",
            "Epoch 442: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0072 - accuracy: 0.9988 - val_loss: 0.8028 - val_accuracy: 0.7390 - lr: 1.0000e-06\n",
            "Epoch 443/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9990\n",
            "Epoch 443: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0074 - accuracy: 0.9990 - val_loss: 0.8042 - val_accuracy: 0.7390 - lr: 1.0000e-06\n",
            "Epoch 444/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9990\n",
            "Epoch 444: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0074 - accuracy: 0.9990 - val_loss: 0.8041 - val_accuracy: 0.7383 - lr: 1.0000e-06\n",
            "Epoch 445/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0070 - accuracy: 0.9990\n",
            "Epoch 445: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0070 - accuracy: 0.9990 - val_loss: 0.8040 - val_accuracy: 0.7383 - lr: 1.0000e-06\n",
            "Epoch 446/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0068 - accuracy: 0.9997\n",
            "Epoch 446: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0068 - accuracy: 0.9997 - val_loss: 0.8049 - val_accuracy: 0.7383 - lr: 1.0000e-06\n",
            "Epoch 447/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0068 - accuracy: 0.9995\n",
            "Epoch 447: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0068 - accuracy: 0.9995 - val_loss: 0.8051 - val_accuracy: 0.7376 - lr: 1.0000e-06\n",
            "Epoch 448/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0067 - accuracy: 0.9988\n",
            "Epoch 448: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0066 - accuracy: 0.9988 - val_loss: 0.8047 - val_accuracy: 0.7390 - lr: 1.0000e-06\n",
            "Epoch 449/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0074 - accuracy: 0.9990\n",
            "Epoch 449: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0073 - accuracy: 0.9990 - val_loss: 0.8068 - val_accuracy: 0.7376 - lr: 1.0000e-06\n",
            "Epoch 450/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9990\n",
            "Epoch 450: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0072 - accuracy: 0.9988 - val_loss: 0.8066 - val_accuracy: 0.7376 - lr: 1.0000e-06\n",
            "Epoch 451/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9990\n",
            "Epoch 451: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0072 - accuracy: 0.9990 - val_loss: 0.8073 - val_accuracy: 0.7376 - lr: 1.0000e-06\n",
            "Epoch 452/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0070 - accuracy: 0.9991\n",
            "Epoch 452: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0070 - accuracy: 0.9992 - val_loss: 0.8081 - val_accuracy: 0.7376 - lr: 1.0000e-06\n",
            "Epoch 453/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9988\n",
            "Epoch 453: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0071 - accuracy: 0.9988 - val_loss: 0.8092 - val_accuracy: 0.7376 - lr: 1.0000e-06\n",
            "Epoch 454/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0063 - accuracy: 0.9997\n",
            "Epoch 454: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0063 - accuracy: 0.9997 - val_loss: 0.8081 - val_accuracy: 0.7383 - lr: 1.0000e-06\n",
            "Epoch 455/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0069 - accuracy: 0.9988\n",
            "Epoch 455: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0069 - accuracy: 0.9988 - val_loss: 0.8110 - val_accuracy: 0.7376 - lr: 1.0000e-06\n",
            "Epoch 456/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0068 - accuracy: 0.9991\n",
            "Epoch 456: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0068 - accuracy: 0.9992 - val_loss: 0.8115 - val_accuracy: 0.7376 - lr: 1.0000e-06\n",
            "Epoch 457/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0066 - accuracy: 0.9993\n",
            "Epoch 457: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0066 - accuracy: 0.9993 - val_loss: 0.8112 - val_accuracy: 0.7376 - lr: 1.0000e-06\n",
            "Epoch 458/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0067 - accuracy: 0.9993\n",
            "Epoch 458: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0068 - accuracy: 0.9993 - val_loss: 0.8119 - val_accuracy: 0.7376 - lr: 1.0000e-06\n",
            "Epoch 459/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9991\n",
            "Epoch 459: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0072 - accuracy: 0.9992 - val_loss: 0.8142 - val_accuracy: 0.7376 - lr: 1.0000e-06\n",
            "Epoch 460/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0067 - accuracy: 0.9990\n",
            "Epoch 460: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0067 - accuracy: 0.9990 - val_loss: 0.8146 - val_accuracy: 0.7376 - lr: 1.0000e-06\n",
            "Epoch 461/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0068 - accuracy: 0.9993\n",
            "Epoch 461: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0068 - accuracy: 0.9993 - val_loss: 0.8148 - val_accuracy: 0.7376 - lr: 1.0000e-06\n",
            "Epoch 462/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0063 - accuracy: 0.9993\n",
            "Epoch 462: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0063 - accuracy: 0.9993 - val_loss: 0.8151 - val_accuracy: 0.7376 - lr: 1.0000e-06\n",
            "Epoch 463/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0061 - accuracy: 0.9997\n",
            "Epoch 463: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0062 - accuracy: 0.9995 - val_loss: 0.8160 - val_accuracy: 0.7376 - lr: 1.0000e-06\n",
            "Epoch 464/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0062 - accuracy: 0.9997\n",
            "Epoch 464: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0062 - accuracy: 0.9997 - val_loss: 0.8175 - val_accuracy: 0.7369 - lr: 1.0000e-06\n",
            "Epoch 465/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0065 - accuracy: 0.9993\n",
            "Epoch 465: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0065 - accuracy: 0.9993 - val_loss: 0.8168 - val_accuracy: 0.7376 - lr: 1.0000e-06\n",
            "Epoch 466/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0062 - accuracy: 0.9991\n",
            "Epoch 466: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0062 - accuracy: 0.9992 - val_loss: 0.8160 - val_accuracy: 0.7369 - lr: 1.0000e-06\n",
            "Epoch 467/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0063 - accuracy: 0.9993\n",
            "Epoch 467: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0063 - accuracy: 0.9993 - val_loss: 0.8163 - val_accuracy: 0.7369 - lr: 1.0000e-06\n",
            "Epoch 468/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0063 - accuracy: 0.9993\n",
            "Epoch 468: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0063 - accuracy: 0.9993 - val_loss: 0.8180 - val_accuracy: 0.7369 - lr: 1.0000e-06\n",
            "Epoch 469/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0064 - accuracy: 0.9990\n",
            "Epoch 469: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0065 - accuracy: 0.9990 - val_loss: 0.8199 - val_accuracy: 0.7369 - lr: 1.0000e-06\n",
            "Epoch 470/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0066 - accuracy: 0.9990\n",
            "Epoch 470: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0065 - accuracy: 0.9990 - val_loss: 0.8198 - val_accuracy: 0.7376 - lr: 1.0000e-06\n",
            "Epoch 471/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0061 - accuracy: 0.9995\n",
            "Epoch 471: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0060 - accuracy: 0.9995 - val_loss: 0.8211 - val_accuracy: 0.7376 - lr: 1.0000e-06\n",
            "Epoch 472/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0065 - accuracy: 0.9993\n",
            "Epoch 472: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0064 - accuracy: 0.9993 - val_loss: 0.8212 - val_accuracy: 0.7369 - lr: 1.0000e-06\n",
            "Epoch 473/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0066 - accuracy: 0.9991\n",
            "Epoch 473: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0065 - accuracy: 0.9992 - val_loss: 0.8223 - val_accuracy: 0.7376 - lr: 1.0000e-06\n",
            "Epoch 474/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0064 - accuracy: 0.9990\n",
            "Epoch 474: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0064 - accuracy: 0.9990 - val_loss: 0.8223 - val_accuracy: 0.7369 - lr: 1.0000e-06\n",
            "Epoch 475/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 0.9997\n",
            "Epoch 475: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0063 - accuracy: 0.9997 - val_loss: 0.8226 - val_accuracy: 0.7369 - lr: 1.0000e-06\n",
            "Epoch 476/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0058 - accuracy: 0.9995\n",
            "Epoch 476: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0058 - accuracy: 0.9995 - val_loss: 0.8219 - val_accuracy: 0.7376 - lr: 1.0000e-06\n",
            "Epoch 477/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0065 - accuracy: 0.9988\n",
            "Epoch 477: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0065 - accuracy: 0.9988 - val_loss: 0.8221 - val_accuracy: 0.7383 - lr: 1.0000e-06\n",
            "Epoch 478/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0063 - accuracy: 0.9993\n",
            "Epoch 478: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0062 - accuracy: 0.9993 - val_loss: 0.8233 - val_accuracy: 0.7362 - lr: 1.0000e-06\n",
            "Epoch 479/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0059 - accuracy: 0.9995\n",
            "Epoch 479: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0058 - accuracy: 0.9995 - val_loss: 0.8252 - val_accuracy: 0.7362 - lr: 1.0000e-06\n",
            "Epoch 480/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0061 - accuracy: 0.9993\n",
            "Epoch 480: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0061 - accuracy: 0.9993 - val_loss: 0.8244 - val_accuracy: 0.7356 - lr: 1.0000e-06\n",
            "Epoch 481/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0060 - accuracy: 0.9993\n",
            "Epoch 481: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0059 - accuracy: 0.9993 - val_loss: 0.8265 - val_accuracy: 0.7362 - lr: 1.0000e-06\n",
            "Epoch 482/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0061 - accuracy: 0.9990\n",
            "Epoch 482: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0062 - accuracy: 0.9990 - val_loss: 0.8266 - val_accuracy: 0.7356 - lr: 1.0000e-06\n",
            "Epoch 483/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0058 - accuracy: 0.9991\n",
            "Epoch 483: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0059 - accuracy: 0.9992 - val_loss: 0.8265 - val_accuracy: 0.7362 - lr: 1.0000e-06\n",
            "Epoch 484/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0055 - accuracy: 0.9995\n",
            "Epoch 484: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0055 - accuracy: 0.9995 - val_loss: 0.8266 - val_accuracy: 0.7362 - lr: 1.0000e-06\n",
            "Epoch 485/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0061 - accuracy: 0.9990\n",
            "Epoch 485: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0061 - accuracy: 0.9990 - val_loss: 0.8281 - val_accuracy: 0.7356 - lr: 1.0000e-06\n",
            "Epoch 486/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0061 - accuracy: 0.9995\n",
            "Epoch 486: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0060 - accuracy: 0.9995 - val_loss: 0.8290 - val_accuracy: 0.7356 - lr: 1.0000e-06\n",
            "Epoch 487/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0062 - accuracy: 0.9991\n",
            "Epoch 487: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0062 - accuracy: 0.9992 - val_loss: 0.8298 - val_accuracy: 0.7356 - lr: 1.0000e-06\n",
            "Epoch 488/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0057 - accuracy: 0.9995\n",
            "Epoch 488: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0057 - accuracy: 0.9995 - val_loss: 0.8300 - val_accuracy: 0.7362 - lr: 1.0000e-06\n",
            "Epoch 489/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0057 - accuracy: 0.9997\n",
            "Epoch 489: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0058 - accuracy: 0.9995 - val_loss: 0.8304 - val_accuracy: 0.7356 - lr: 1.0000e-06\n",
            "Epoch 490/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0057 - accuracy: 0.9993\n",
            "Epoch 490: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0057 - accuracy: 0.9993 - val_loss: 0.8324 - val_accuracy: 0.7356 - lr: 1.0000e-06\n",
            "Epoch 491/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0059 - accuracy: 0.9998\n",
            "Epoch 491: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0058 - accuracy: 0.9998 - val_loss: 0.8336 - val_accuracy: 0.7356 - lr: 1.0000e-06\n",
            "Epoch 492/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0061 - accuracy: 0.9993\n",
            "Epoch 492: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0060 - accuracy: 0.9993 - val_loss: 0.8342 - val_accuracy: 0.7356 - lr: 1.0000e-06\n",
            "Epoch 493/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.0060 - accuracy: 0.9995\n",
            "Epoch 493: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0059 - accuracy: 0.9995 - val_loss: 0.8344 - val_accuracy: 0.7349 - lr: 1.0000e-06\n",
            "Epoch 494/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.0062 - accuracy: 0.9990\n",
            "Epoch 494: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0061 - accuracy: 0.9990 - val_loss: 0.8347 - val_accuracy: 0.7349 - lr: 1.0000e-06\n",
            "Epoch 495/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0055 - accuracy: 0.9993\n",
            "Epoch 495: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0059 - accuracy: 0.9990 - val_loss: 0.8368 - val_accuracy: 0.7356 - lr: 1.0000e-06\n",
            "Epoch 496/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0057 - accuracy: 0.9991\n",
            "Epoch 496: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0057 - accuracy: 0.9992 - val_loss: 0.8370 - val_accuracy: 0.7349 - lr: 1.0000e-06\n",
            "Epoch 497/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.0053 - accuracy: 0.9995\n",
            "Epoch 497: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0053 - accuracy: 0.9995 - val_loss: 0.8361 - val_accuracy: 0.7349 - lr: 1.0000e-06\n",
            "Epoch 498/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.0057 - accuracy: 0.9995\n",
            "Epoch 498: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0057 - accuracy: 0.9995 - val_loss: 0.8364 - val_accuracy: 0.7349 - lr: 1.0000e-06\n",
            "Epoch 499/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.0057 - accuracy: 0.9997\n",
            "Epoch 499: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0057 - accuracy: 0.9997 - val_loss: 0.8380 - val_accuracy: 0.7349 - lr: 1.0000e-06\n",
            "Epoch 500/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.0057 - accuracy: 0.9995\n",
            "Epoch 500: val_loss did not improve from 0.48980\n",
            "184/184 [==============================] - 2s 10ms/step - loss: 0.0059 - accuracy: 0.9995 - val_loss: 0.8379 - val_accuracy: 0.7349 - lr: 1.0000e-06\n"
          ]
        }
      ],
      "source": [
        "hist = model.fit([train_X, train_X, train_X], train_Y, epochs = EPOCHS, batch_size = BS, validation_data = ([val_X, val_X, val_X], val_Y), callbacks = [callbacks_list], shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4xW5EeNpQ26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "7ff70f4a-3062-44f8-f058-3cef78fbc19d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"252e6fba-794e-45fd-8b46-5b7848e22804\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"252e6fba-794e-45fd-8b46-5b7848e22804\")) {                    Plotly.newPlot(                        \"252e6fba-794e-45fd-8b46-5b7848e22804\",                        [{\"line\":{\"color\":\"blue\",\"width\":2},\"mode\":\"lines\",\"name\":\"loss\",\"y\":[0.6804859638214111,0.6222001314163208,0.510380208492279,0.4224313497543335,0.3451899290084839,0.26258614659309387,0.19091449677944183,0.13545681536197662,0.09585490077733994,0.06751979887485504,0.04930583015084267,0.047477856278419495,0.0450933501124382,0.04418123513460159,0.043964143842458725,0.044211409986019135,0.04401184245944023,0.04308154433965683,0.04457724466919899,0.04350534826517105,0.04348594695329666,0.04416152834892273,0.042809806764125824,0.04258296266198158,0.042679499834775925,0.04160945117473602,0.042192913591861725,0.04183001443743706,0.041121263056993484,0.04118781164288521,0.0406801700592041,0.04097069054841995,0.04113127663731575,0.03982473537325859,0.03995058313012123,0.041495662182569504,0.04097888618707657,0.04000962898135185,0.03942882642149925,0.03894459828734398,0.039321981370449066,0.039013415575027466,0.03909263014793396,0.03825318440794945,0.038763441145420074,0.03933068364858627,0.03693553805351257,0.037596363574266434,0.037338707596063614,0.037489134818315506,0.03792218491435051,0.037709303200244904,0.03700852766633034,0.03698057308793068,0.03677504509687424,0.036853186786174774,0.036411769688129425,0.03579603135585785,0.03456416726112366,0.03616396710276604,0.03549785539507866,0.034446101635694504,0.03556090593338013,0.035032615065574646,0.03412299603223801,0.03465699777007103,0.034797318279743195,0.03335186466574669,0.03432948887348175,0.03410394489765167,0.03336517512798309,0.03398973494768143,0.034156959503889084,0.03353295475244522,0.03263874351978302,0.03308664634823799,0.03335791453719139,0.03252008557319641,0.03219351917505264,0.03291042149066925,0.03391599655151367,0.03241037204861641,0.03126818314194679,0.03222144395112991,0.031743645668029785,0.030990922823548317,0.031545087695121765,0.03105025924742222,0.030739957466721535,0.03173280507326126,0.03187818080186844,0.03090646304190159,0.030797239392995834,0.0302730742841959,0.030003536492586136,0.029449449852108955,0.02953064814209938,0.029797304421663284,0.030205782502889633,0.029196584597229958,0.029941456392407417,0.02928868494927883,0.02913413755595684,0.029189523309469223,0.02927403710782528,0.028844600543379784,0.029822716489434242,0.02859276905655861,0.02874133549630642,0.028387511149048805,0.02826407551765442,0.02841176465153694,0.027483919635415077,0.02748950570821762,0.028311654925346375,0.027666661888360977,0.02704445645213127,0.026883089914917946,0.026589294895529747,0.027154460549354553,0.026326725259423256,0.026972126215696335,0.02793516404926777,0.02610817179083824,0.02630782499909401,0.026791881769895554,0.025532757863402367,0.026841266080737114,0.025978120043873787,0.02622583881020546,0.025991275906562805,0.025791659951210022,0.025332145392894745,0.025161953642964363,0.025605682283639908,0.026140091940760612,0.024849051609635353,0.025017576292157173,0.024740301072597504,0.024398837238550186,0.02412375435233116,0.024748409166932106,0.024036066606640816,0.02376551367342472,0.024078086018562317,0.023777734488248825,0.023644350469112396,0.024486392736434937,0.023103272542357445,0.023823563009500504,0.022834032773971558,0.02317172847688198,0.023158634081482887,0.02352721616625786,0.022666186094284058,0.022836511954665184,0.02211599051952362,0.022795401513576508,0.0226480420678854,0.02240624651312828,0.023265887051820755,0.02213614620268345,0.022387558594346046,0.02238033339381218,0.021963829174637794,0.02239561825990677,0.022043513134121895,0.022046001628041267,0.022527391090989113,0.021494822576642036,0.022171134129166603,0.02137802168726921,0.02144467644393444,0.02186293713748455,0.021149149164557457,0.02160882204771042,0.02039237692952156,0.022012043744325638,0.0205075703561306,0.020941955968737602,0.02046503871679306,0.020213313400745392,0.021407507359981537,0.020448466762900352,0.020307140424847603,0.020203622058033943,0.019978467375040054,0.0196395106613636,0.01989198476076126,0.01976984739303589,0.020045746117830276,0.018874790519475937,0.019019244238734245,0.01949309930205345,0.019739383831620216,0.0197183545678854,0.02001975104212761,0.018683115020394325,0.019630134105682373,0.01969054341316223,0.019431930035352707,0.019299978390336037,0.018914861604571342,0.018303146585822105,0.019373655319213867,0.019270487129688263,0.01929684355854988,0.018299521878361702,0.018654098734259605,0.017513860017061234,0.018622709438204765,0.018375802785158157,0.018109507858753204,0.017578966915607452,0.01837330497801304,0.018220731988549232,0.018134817481040955,0.017455056309700012,0.01761036552488804,0.017403777688741684,0.017541611567139626,0.017638808116316795,0.016924863681197166,0.01717737503349781,0.016911720857024193,0.0172001663595438,0.017336765304207802,0.017302004620432854,0.016461851075291634,0.016724932938814163,0.016478994861245155,0.016240911558270454,0.01739496737718582,0.0172334685921669,0.016143780201673508,0.01608910784125328,0.016399655491113663,0.015745971351861954,0.01653820276260376,0.016497524455189705,0.016595451161265373,0.016321882605552673,0.015891574323177338,0.015281042084097862,0.015890398994088173,0.01605619490146637,0.015655647963285446,0.016014153137803078,0.016296561807394028,0.01642017625272274,0.015922438353300095,0.015924906358122826,0.01592891477048397,0.01515008695423603,0.015278617851436138,0.014949197880923748,0.015622567385435104,0.014478430151939392,0.014676309190690517,0.015109803527593613,0.015178623609244823,0.014515148475766182,0.014680806547403336,0.01502962876111269,0.014464059844613075,0.015007690526545048,0.014426272362470627,0.014510774053633213,0.014762463048100471,0.014120681211352348,0.014737113378942013,0.013752540573477745,0.014796887524425983,0.0143744433298707,0.014429762028157711,0.013811926357448101,0.013380242511630058,0.013080685399472713,0.014267602935433388,0.013235281221568584,0.013381785713136196,0.014237765222787857,0.013780447654426098,0.013594316318631172,0.013591543771326542,0.013887479901313782,0.01348467543721199,0.013362575322389603,0.013863475061953068,0.013380778022110462,0.01285945437848568,0.012454114854335785,0.013264404609799385,0.013332237489521503,0.013522018678486347,0.013272271491587162,0.012748684734106064,0.012786353938281536,0.012169379740953445,0.012688465416431427,0.012468120083212852,0.012157963588833809,0.012595921754837036,0.01215584296733141,0.012574275955557823,0.012586950324475765,0.012022852897644043,0.01207455713301897,0.012227154336869717,0.012255821377038956,0.012566151097416878,0.01201608870178461,0.011662228032946587,0.01201111264526844,0.011783281341195107,0.011856169439852238,0.011393893510103226,0.011990160681307316,0.011555307544767857,0.011263658292591572,0.01144539937376976,0.011642038822174072,0.011760464869439602,0.01153973862528801,0.011690805666148663,0.011535954661667347,0.011698197573423386,0.011322738602757454,0.011200327426195145,0.01107289269566536,0.011405527591705322,0.011565372347831726,0.011083042249083519,0.01096462644636631,0.010822617448866367,0.010605448856949806,0.011170544661581516,0.010951693169772625,0.010788257233798504,0.010546494275331497,0.01035429909825325,0.011086495593190193,0.010839343070983887,0.010042334906756878,0.010379740968346596,0.010572104714810848,0.01087205670773983,0.010677477344870567,0.01033942960202694,0.010480067692697048,0.00990915298461914,0.01039836648851633,0.009972526691854,0.009789413772523403,0.010075193829834461,0.010129989124834538,0.01027628593146801,0.00972982868552208,0.010561912320554256,0.010054312646389008,0.009699652902781963,0.010028899647295475,0.01042847242206335,0.009701329283416271,0.009549285285174847,0.00986213143914938,0.00963575765490532,0.00939835887402296,0.009939499199390411,0.009205863811075687,0.009854520671069622,0.009416940622031689,0.009251625277101994,0.008755451999604702,0.009147770702838898,0.009241766296327114,0.009508121758699417,0.0090926643460989,0.008942441083490849,0.009847602806985378,0.008540450595319271,0.00927360262721777,0.00915397796779871,0.008828552439808846,0.009148373268544674,0.008815847337245941,0.009506965056061745,0.008995919488370419,0.008801660500466824,0.008686527609825134,0.00862274318933487,0.00870677549391985,0.008201277814805508,0.008721289224922657,0.008402620442211628,0.008254514075815678,0.008537089452147484,0.008388944901525974,0.008993847295641899,0.00850278977304697,0.008396828547120094,0.008448760956525803,0.008160280995070934,0.008576198481023312,0.008835740387439728,0.008304616436362267,0.008378406055271626,0.008243137039244175,0.008374130353331566,0.008058859966695309,0.0074885631911456585,0.008484813384711742,0.008211015723645687,0.00768909091129899,0.007974597625434399,0.007764092180877924,0.007650025654584169,0.008236173540353775,0.007867460139095783,0.007578791119158268,0.008130593225359917,0.008029484190046787,0.007732665166258812,0.007852114737033844,0.007890213280916214,0.007824095897376537,0.00794178992509842,0.007118811830878258,0.007455732673406601,0.007898899726569653,0.007502786349505186,0.007846388034522533,0.007317526265978813,0.007043625693768263,0.00769530376419425,0.00744482409209013,0.007654142566025257,0.0069160135462880135,0.007251434028148651,0.007218328770250082,0.007104626856744289,0.00723717175424099,0.007419647183269262,0.007409338373690844,0.006972222588956356,0.00683343131095171,0.006798368878662586,0.00663716159760952,0.0073410202749073505,0.007221107836812735,0.007211801130324602,0.007015562150627375,0.007060898933559656,0.006309439893811941,0.006929231807589531,0.00677375216037035,0.006572461221367121,0.0067836325615644455,0.00719395512714982,0.006677250377833843,0.006793575827032328,0.006331567652523518,0.006228261161595583,0.00617351196706295,0.006461446639150381,0.0061676339246332645,0.006252623628824949,0.006276691798120737,0.006487526465207338,0.006549196317791939,0.005995688494294882,0.006439724005758762,0.006521853618323803,0.006441941950470209,0.006257590372115374,0.005847959313541651,0.00646331999450922,0.00624452531337738,0.005830802023410797,0.006112435832619667,0.005923980847001076,0.0062346248887479305,0.005858507938683033,0.005451797507703304,0.006089191883802414,0.006034275982528925,0.00616614380851388,0.0056924233213067055,0.005814072210341692,0.005684604402631521,0.005831826478242874,0.006049301475286484,0.005918878596276045,0.006115252617746592,0.005854891147464514,0.005693095736205578,0.005328320898115635,0.00568596925586462,0.005673953797668219,0.005863748025149107],\"type\":\"scatter\"},{\"line\":{\"color\":\"red\",\"width\":2},\"mode\":\"lines\",\"name\":\"val_loss\",\"y\":[0.6564651131629944,0.58917635679245,0.5223173499107361,0.4954928457736969,0.4905027747154236,0.49347400665283203,0.4898039400577545,0.4997714161872864,0.5357785820960999,0.5711463093757629,0.5600041151046753,0.5627928376197815,0.5717690587043762,0.5682157874107361,0.5667667984962463,0.5656871795654297,0.5659366846084595,0.5672640204429626,0.5667361617088318,0.5671765804290771,0.566892147064209,0.567798912525177,0.5686177015304565,0.569871723651886,0.5696892142295837,0.5697776675224304,0.5701913237571716,0.5712060928344727,0.5713083148002625,0.5710242390632629,0.5715179443359375,0.5723028182983398,0.5726568698883057,0.5735406279563904,0.5742327570915222,0.5739782452583313,0.5758911967277527,0.5762254595756531,0.5765992999076843,0.5764504671096802,0.5760897397994995,0.5771161317825317,0.5780370235443115,0.579300582408905,0.5794872641563416,0.5794551372528076,0.5802564024925232,0.580405592918396,0.581150233745575,0.5821727514266968,0.5824124217033386,0.5824689865112305,0.5836120843887329,0.5843227505683899,0.5848779678344727,0.5850396752357483,0.585323691368103,0.5854580998420715,0.5853416323661804,0.5856848359107971,0.5869829058647156,0.5867153406143188,0.5882270336151123,0.5881590247154236,0.5888911485671997,0.5896080732345581,0.5911799669265747,0.5907270908355713,0.5910784602165222,0.5915656089782715,0.5920712351799011,0.5932861566543579,0.5943143963813782,0.5944104194641113,0.5938827991485596,0.5956477522850037,0.596076250076294,0.5963512659072876,0.5962355136871338,0.5972417593002319,0.5983787775039673,0.5983618497848511,0.5994818210601807,0.5989975929260254,0.6000816226005554,0.599918007850647,0.6017451882362366,0.6004343628883362,0.6019763946533203,0.6025916934013367,0.6031338572502136,0.6036592125892639,0.6044100522994995,0.6049339771270752,0.6054382920265198,0.6055387258529663,0.60593181848526,0.6067013144493103,0.6077273488044739,0.6070072650909424,0.6078793406486511,0.608729898929596,0.6086298227310181,0.6105389595031738,0.6115434765815735,0.611201286315918,0.6122250556945801,0.6126957535743713,0.6137734055519104,0.6146698594093323,0.615543007850647,0.615045964717865,0.6151766180992126,0.6154158115386963,0.6156877279281616,0.6159769892692566,0.6169037222862244,0.6177899837493896,0.6184340715408325,0.6191449761390686,0.619674026966095,0.6208714842796326,0.621321439743042,0.6209651827812195,0.622418999671936,0.6216916441917419,0.622114360332489,0.6227730512619019,0.6225951313972473,0.6238137483596802,0.6250839233398438,0.6265226006507874,0.6260891556739807,0.6255531907081604,0.6276225447654724,0.6265758275985718,0.6275725364685059,0.6277734041213989,0.628594696521759,0.630159854888916,0.6297905445098877,0.6305719017982483,0.631249725818634,0.6310364007949829,0.6326887011528015,0.6334404349327087,0.6337119936943054,0.6344259977340698,0.6349899172782898,0.635810375213623,0.6354060769081116,0.6360774636268616,0.636199414730072,0.6365870833396912,0.637738823890686,0.6385837197303772,0.6384803652763367,0.6397034525871277,0.6396846771240234,0.6405942440032959,0.6402958035469055,0.6415864825248718,0.6422212719917297,0.6431469917297363,0.6438273191452026,0.6442438364028931,0.643770158290863,0.6454393267631531,0.6462820768356323,0.646344006061554,0.6471732258796692,0.6482948064804077,0.6478115320205688,0.6480699777603149,0.649778425693512,0.6484397649765015,0.6486648917198181,0.6505917906761169,0.6518093943595886,0.6512550115585327,0.6523247361183167,0.6526117324829102,0.6536782383918762,0.6550678610801697,0.6549380421638489,0.6555478572845459,0.656125009059906,0.6551265120506287,0.6557185053825378,0.6566891670227051,0.6580203771591187,0.6578447818756104,0.6576564311981201,0.6585162878036499,0.658645749092102,0.6603351831436157,0.6612620949745178,0.6605879664421082,0.6627975106239319,0.663750946521759,0.663570761680603,0.6638427376747131,0.664442241191864,0.6658428907394409,0.6667515635490417,0.6666854023933411,0.6672442555427551,0.6679208278656006,0.6689887642860413,0.6686683297157288,0.6695542335510254,0.6702367067337036,0.6699843406677246,0.6699572205543518,0.6722028851509094,0.6727968454360962,0.6720085144042969,0.6725209951400757,0.6728872656822205,0.6727795600891113,0.6737945675849915,0.6746956706047058,0.6754388213157654,0.6760793924331665,0.6769086718559265,0.6777850389480591,0.678484320640564,0.678301215171814,0.6799124479293823,0.6786366105079651,0.6800045967102051,0.6802526116371155,0.6814240217208862,0.6814055442810059,0.6817494630813599,0.6828839182853699,0.683784008026123,0.6837072968482971,0.6844221949577332,0.6848015785217285,0.6849735975265503,0.686383843421936,0.687210738658905,0.6873524785041809,0.6868188381195068,0.6884524822235107,0.6887344121932983,0.6900941133499146,0.6905762553215027,0.6916791796684265,0.6917329430580139,0.6917657852172852,0.6931561231613159,0.6935078501701355,0.694238007068634,0.6938026547431946,0.6949707269668579,0.6948580145835876,0.6954358816146851,0.6965331435203552,0.6982399225234985,0.697921872138977,0.6979780197143555,0.6988483667373657,0.6992384195327759,0.7009643316268921,0.7008626461029053,0.7004914879798889,0.7014745473861694,0.7013710141181946,0.7025499939918518,0.7039003968238831,0.7038863897323608,0.7058253288269043,0.705203652381897,0.705535888671875,0.7057924270629883,0.7066097855567932,0.7073175311088562,0.7090865969657898,0.7092177867889404,0.7090084552764893,0.7091665267944336,0.7081999182701111,0.7099941968917847,0.7116408944129944,0.7130646705627441,0.7124685645103455,0.7133787274360657,0.7136238217353821,0.7132945656776428,0.7141246199607849,0.7145759463310242,0.7166897654533386,0.71596759557724,0.7172762155532837,0.7174504399299622,0.7175319790840149,0.7174719572067261,0.7193377614021301,0.7198066711425781,0.7200075387954712,0.7202326059341431,0.7218461632728577,0.7218788266181946,0.7222251296043396,0.7232718467712402,0.7228682637214661,0.7231737971305847,0.7255271077156067,0.7247685194015503,0.7266463041305542,0.7270384430885315,0.7268840074539185,0.7282407879829407,0.728094220161438,0.7299531102180481,0.7295989990234375,0.7298255562782288,0.72944176197052,0.7308458089828491,0.7321212291717529,0.7321329116821289,0.7345012426376343,0.734550952911377,0.7345952391624451,0.73598313331604,0.7368094325065613,0.7371686697006226,0.7370937466621399,0.7382863163948059,0.738781750202179,0.7387160658836365,0.7393344044685364,0.7401880025863647,0.7412731647491455,0.7407680749893188,0.7408481240272522,0.7420859336853027,0.7433792948722839,0.7437642812728882,0.7441949248313904,0.7441201210021973,0.7452656626701355,0.7449458241462708,0.7460952997207642,0.7472524046897888,0.7474614381790161,0.7482888698577881,0.7484675049781799,0.7486680746078491,0.7486909031867981,0.7495244145393372,0.7506937384605408,0.7494154572486877,0.751171350479126,0.7511341571807861,0.7511714696884155,0.7511568665504456,0.7517244219779968,0.7535252571105957,0.7546883225440979,0.7565025687217712,0.7562230825424194,0.755788266658783,0.7568230628967285,0.7577850222587585,0.7597967982292175,0.7601869106292725,0.7610006928443909,0.7608650326728821,0.7593376040458679,0.7602887749671936,0.7615262866020203,0.7618646621704102,0.7627236843109131,0.7624520063400269,0.7639495134353638,0.7647407054901123,0.7640048265457153,0.7652581930160522,0.7653149962425232,0.7664427161216736,0.7689156532287598,0.7680943608283997,0.7691177129745483,0.7709482312202454,0.770279586315155,0.7704895734786987,0.7707323431968689,0.7717106938362122,0.7729058265686035,0.7737702131271362,0.7742438316345215,0.7740611433982849,0.7747111320495605,0.7751858234405518,0.77524334192276,0.7783512473106384,0.7788500785827637,0.7785053849220276,0.7788758277893066,0.7787084579467773,0.7794987559318542,0.780745804309845,0.7807257771492004,0.7819756865501404,0.7825080752372742,0.7834141254425049,0.7829979658126831,0.7838935852050781,0.7843562960624695,0.786002516746521,0.7863439917564392,0.7876467108726501,0.7868143916130066,0.7878788113594055,0.7887865900993347,0.7888304591178894,0.7894899249076843,0.7912351489067078,0.7912971377372742,0.7917680740356445,0.7941088676452637,0.7932716608047485,0.7931554913520813,0.7930474281311035,0.7941045761108398,0.7946942448616028,0.7960786819458008,0.796665608882904,0.7972636222839355,0.7975553274154663,0.7981879711151123,0.7999538779258728,0.7997207045555115,0.7997443675994873,0.7997111082077026,0.8015545606613159,0.802223265171051,0.8016543984413147,0.8027626872062683,0.8042123913764954,0.8040913939476013,0.8040125966072083,0.8048507571220398,0.8050776720046997,0.8047029376029968,0.8067538142204285,0.8065642714500427,0.807256281375885,0.8081247806549072,0.8092082738876343,0.808109700679779,0.8110296726226807,0.8115074038505554,0.8111616373062134,0.8118733763694763,0.8141889572143555,0.8146414756774902,0.8148489594459534,0.8151214122772217,0.8159871697425842,0.817542552947998,0.8168496489524841,0.8160427808761597,0.8163295984268188,0.8180456757545471,0.8199306726455688,0.8198072910308838,0.8211032748222351,0.821179211139679,0.8222537636756897,0.822257399559021,0.822638213634491,0.8218867182731628,0.8221489191055298,0.8233205080032349,0.8252048492431641,0.8244495391845703,0.8264674544334412,0.8265951871871948,0.8265167474746704,0.8265564441680908,0.8281250596046448,0.8289571404457092,0.8298351168632507,0.8300115466117859,0.8303734660148621,0.8324382901191711,0.8335840702056885,0.8342031836509705,0.8344144821166992,0.8347073197364807,0.8367570638656616,0.8369876146316528,0.8361455202102661,0.8363548517227173,0.8380006551742554,0.8378555774688721],\"type\":\"scatter\"}],                        {\"title\":{\"text\":\"Loss\"},\"xaxis\":{\"title\":{\"text\":\"epochs\"}},\"yaxis\":{\"title\":{\"text\":\"\"}},\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('252e6fba-794e-45fd-8b46-5b7848e22804');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "h1 = go.Scatter(y=hist.history['loss'], \n",
        "                    mode=\"lines\", line=dict(\n",
        "                    width=2,\n",
        "                    color='blue'),\n",
        "                    name=\"loss\"\n",
        "                   )\n",
        "h2 = go.Scatter(y=hist.history['val_loss'], \n",
        "                    mode=\"lines\", line=dict(\n",
        "                    width=2,\n",
        "                    color='red'),\n",
        "                    name=\"val_loss\"\n",
        "                   )\n",
        "\n",
        "data = [h1,h2]\n",
        "layout1 = go.Layout(title='Loss',\n",
        "                   xaxis=dict(title='epochs'),\n",
        "                   yaxis=dict(title=''))\n",
        "fig1 = go.Figure(data = data, layout=layout1)\n",
        "fig1.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywkelIbjpX5k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "7562e74e-4f85-4533-f612-e25ed7a8d06a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"63764d9c-5c3e-4055-8c03-81e60293cde8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"63764d9c-5c3e-4055-8c03-81e60293cde8\")) {                    Plotly.newPlot(                        \"63764d9c-5c3e-4055-8c03-81e60293cde8\",                        [{\"line\":{\"color\":\"blue\",\"width\":2},\"mode\":\"lines\",\"name\":\"acc\",\"y\":[0.5561788082122803,0.655957818031311,0.7627061009407043,0.8075811862945557,0.860275387763977,0.9036206007003784,0.9391466975212097,0.9602243900299072,0.9760326147079468,0.9857215881347656,0.9913309812545776,0.9918408989906311,0.9918408989906311,0.9923508167266846,0.9918408989906311,0.9915009140968323,0.9921808838844299,0.9926908016204834,0.99065101146698,0.9926908016204834,0.992520809173584,0.9903110861778259,0.9913309812545776,0.9916709065437317,0.9923508167266846,0.992520809173584,0.9918408989906311,0.9928607940673828,0.9928607940673828,0.9930307865142822,0.9926908016204834,0.9932007193565369,0.992520809173584,0.9935407042503357,0.9932007193565369,0.9933707118034363,0.9930307865142822,0.9930307865142822,0.9940506815910339,0.9930307865142822,0.9933707118034363,0.9932007193565369,0.992520809173584,0.9935407042503357,0.9932007193565369,0.9926908016204834,0.9937106966972351,0.9940506815910339,0.9938806891441345,0.9933707118034363,0.9928607940673828,0.9932007193565369,0.9933707118034363,0.9918408989906311,0.9930307865142822,0.9921808838844299,0.9938806891441345,0.9930307865142822,0.9942206144332886,0.992520809173584,0.9930307865142822,0.9938806891441345,0.9938806891441345,0.9930307865142822,0.9935407042503357,0.9945605993270874,0.9928607940673828,0.9949005842208862,0.9940506815910339,0.9938806891441345,0.9947305917739868,0.9932007193565369,0.9938806891441345,0.9938806891441345,0.9954105019569397,0.9940506815910339,0.9950705170631409,0.994390606880188,0.9937106966972351,0.9932007193565369,0.9935407042503357,0.9945605993270874,0.9955804944038391,0.9947305917739868,0.9950705170631409,0.9950705170631409,0.9945605993270874,0.994390606880188,0.9952405095100403,0.9935407042503357,0.9937106966972351,0.9945605993270874,0.9949005842208862,0.9955804944038391,0.9952405095100403,0.9952405095100403,0.9952405095100403,0.9937106966972351,0.994390606880188,0.9950705170631409,0.9947305917739868,0.9949005842208862,0.9950705170631409,0.994390606880188,0.9952405095100403,0.996260404586792,0.9938806891441345,0.9954105019569397,0.9947305917739868,0.9949005842208862,0.9952405095100403,0.9950705170631409,0.9959204196929932,0.9952405095100403,0.994390606880188,0.9950705170631409,0.9950705170631409,0.9960904121398926,0.9954105019569397,0.9960904121398926,0.9957504868507385,0.994390606880188,0.9942206144332886,0.996260404586792,0.996260404586792,0.9949005842208862,0.9960904121398926,0.9952405095100403,0.9954105019569397,0.9952405095100403,0.9950705170631409,0.9955804944038391,0.9949005842208862,0.9959204196929932,0.9955804944038391,0.9950705170631409,0.9959204196929932,0.9960904121398926,0.9957504868507385,0.9952405095100403,0.9966003894805908,0.9950705170631409,0.9967703819274902,0.9960904121398926,0.9959204196929932,0.9969403147697449,0.9959204196929932,0.9957504868507385,0.9952405095100403,0.9964303970336914,0.9966003894805908,0.996260404586792,0.9974502921104431,0.9960904121398926,0.9960904121398926,0.9967703819274902,0.9967703819274902,0.9957504868507385,0.9955804944038391,0.9960904121398926,0.996260404586792,0.9967703819274902,0.9967703819274902,0.9954105019569397,0.9954105019569397,0.9966003894805908,0.9964303970336914,0.996260404586792,0.9959204196929932,0.9969403147697449,0.9964303970336914,0.9955804944038391,0.9969403147697449,0.9964303970336914,0.9971103072166443,0.9967703819274902,0.9969403147697449,0.9952405095100403,0.9976202845573425,0.996260404586792,0.9969403147697449,0.9967703819274902,0.9957504868507385,0.996260404586792,0.9972802996635437,0.996260404586792,0.9964303970336914,0.9971103072166443,0.9967703819274902,0.9969403147697449,0.9966003894805908,0.9974502921104431,0.9976202845573425,0.9966003894805908,0.996260404586792,0.9967703819274902,0.9971103072166443,0.9971103072166443,0.9969403147697449,0.9960904121398926,0.9969403147697449,0.9967703819274902,0.9966003894805908,0.9977902173995972,0.9959204196929932,0.9959204196929932,0.9964303970336914,0.9967703819274902,0.9971103072166443,0.9977902173995972,0.9966003894805908,0.9969403147697449,0.9967703819274902,0.9967703819274902,0.9960904121398926,0.9974502921104431,0.9974502921104431,0.996260404586792,0.9974502921104431,0.9964303970336914,0.9969403147697449,0.996260404586792,0.9972802996635437,0.9974502921104431,0.9972802996635437,0.9971103072166443,0.9969403147697449,0.9972802996635437,0.9977902173995972,0.9972802996635437,0.9979602098464966,0.9972802996635437,0.9977902173995972,0.9969403147697449,0.9972802996635437,0.998130202293396,0.9969403147697449,0.998130202293396,0.9972802996635437,0.9972802996635437,0.9972802996635437,0.9969403147697449,0.998130202293396,0.9977902173995972,0.9971103072166443,0.9974502921104431,0.9969403147697449,0.9974502921104431,0.9969403147697449,0.9971103072166443,0.9974502921104431,0.9969403147697449,0.9972802996635437,0.9977902173995972,0.9967703819274902,0.9976202845573425,0.9976202845573425,0.998130202293396,0.9971103072166443,0.998130202293396,0.9974502921104431,0.998130202293396,0.9971103072166443,0.9974502921104431,0.9977902173995972,0.9971103072166443,0.9976202845573425,0.9984701871871948,0.9976202845573425,0.9979602098464966,0.9983001947402954,0.998130202293396,0.9976202845573425,0.9974502921104431,0.9977902173995972,0.9979602098464966,0.9977902173995972,0.9983001947402954,0.9976202845573425,0.9979602098464966,0.998130202293396,0.9967703819274902,0.9969403147697449,0.998130202293396,0.998130202293396,0.9979602098464966,0.9976202845573425,0.998130202293396,0.9983001947402954,0.9984701871871948,0.998130202293396,0.9991500973701477,0.9974502921104431,0.9979602098464966,0.9977902173995972,0.9976202845573425,0.9984701871871948,0.9977902173995972,0.9989801049232483,0.9977902173995972,0.9977902173995972,0.9989801049232483,0.998130202293396,0.998130202293396,0.998130202293396,0.9976202845573425,0.9983001947402954,0.9986401200294495,0.998130202293396,0.998130202293396,0.9979602098464966,0.9979602098464966,0.9986401200294495,0.9979602098464966,0.9976202845573425,0.9977902173995972,0.9986401200294495,0.9977902173995972,0.9986401200294495,0.9983001947402954,0.998130202293396,0.9984701871871948,0.9979602098464966,0.9979602098464966,0.998130202293396,0.9984701871871948,0.998130202293396,0.9986401200294495,0.9984701871871948,0.9983001947402954,0.9984701871871948,0.9977902173995972,0.9984701871871948,0.9986401200294495,0.9988101124763489,0.9986401200294495,0.9984701871871948,0.998130202293396,0.998130202293396,0.9984701871871948,0.9988101124763489,0.9976202845573425,0.9988101124763489,0.9984701871871948,0.9984701871871948,0.9984701871871948,0.998130202293396,0.9984701871871948,0.9984701871871948,0.9979602098464966,0.998130202293396,0.9984701871871948,0.9991500973701477,0.9986401200294495,0.998130202293396,0.9988101124763489,0.9988101124763489,0.9988101124763489,0.998130202293396,0.998130202293396,0.9988101124763489,0.9984701871871948,0.9979602098464966,0.998130202293396,0.9991500973701477,0.9983001947402954,0.9989801049232483,0.9984701871871948,0.9988101124763489,0.9991500973701477,0.9988101124763489,0.9983001947402954,0.9988101124763489,0.9989801049232483,0.9983001947402954,0.9988101124763489,0.9984701871871948,0.9988101124763489,0.9988101124763489,0.9984701871871948,0.9989801049232483,0.9989801049232483,0.998130202293396,0.9989801049232483,0.9986401200294495,0.9984701871871948,0.998130202293396,0.9988101124763489,0.9989801049232483,0.9989801049232483,0.9984701871871948,0.9988101124763489,0.9991500973701477,0.9994900822639465,0.9989801049232483,0.9991500973701477,0.9986401200294495,0.9988101124763489,0.998130202293396,0.9989801049232483,0.9993200898170471,0.9993200898170471,0.9988101124763489,0.9988101124763489,0.9984701871871948,0.9984701871871948,0.9988101124763489,0.9984701871871948,0.9988101124763489,0.9989801049232483,0.9991500973701477,0.9993200898170471,0.9986401200294495,0.9991500973701477,0.9988101124763489,0.9991500973701477,0.9993200898170471,0.9986401200294495,0.9989801049232483,0.9991500973701477,0.9988101124763489,0.9989801049232483,0.9988101124763489,0.9988101124763489,0.9983001947402954,0.9988101124763489,0.9988101124763489,0.9994900822639465,0.9994900822639465,0.9993200898170471,0.9989801049232483,0.9988101124763489,0.9991500973701477,0.9993200898170471,0.9991500973701477,0.9988101124763489,0.9988101124763489,0.9996600151062012,0.9988101124763489,0.9991500973701477,0.9991500973701477,0.9988101124763489,0.9989801049232483,0.9989801049232483,0.9989801049232483,0.9996600151062012,0.9994900822639465,0.9988101124763489,0.9989801049232483,0.9988101124763489,0.9989801049232483,0.9991500973701477,0.9988101124763489,0.9996600151062012,0.9988101124763489,0.9991500973701477,0.9993200898170471,0.9993200898170471,0.9991500973701477,0.9989801049232483,0.9993200898170471,0.9993200898170471,0.9994900822639465,0.9996600151062012,0.9993200898170471,0.9991500973701477,0.9993200898170471,0.9993200898170471,0.9989801049232483,0.9989801049232483,0.9994900822639465,0.9993200898170471,0.9991500973701477,0.9989801049232483,0.9996600151062012,0.9994900822639465,0.9988101124763489,0.9993200898170471,0.9994900822639465,0.9993200898170471,0.9993200898170471,0.9989801049232483,0.9991500973701477,0.9994900822639465,0.9989801049232483,0.9994900822639465,0.9991500973701477,0.9994900822639465,0.9994900822639465,0.9993200898170471,0.9998300075531006,0.9993200898170471,0.9994900822639465,0.9989801049232483,0.9989801049232483,0.9991500973701477,0.9994900822639465,0.9994900822639465,0.9996600151062012,0.9994900822639465],\"type\":\"scatter\"},{\"line\":{\"color\":\"red\",\"width\":2},\"mode\":\"lines\",\"name\":\"val_acc\",\"y\":[0.6220258474349976,0.7205982208251953,0.7579877376556396,0.7709041237831116,0.7634262442588806,0.7532290816307068,0.7607070207595825,0.7518694996833801,0.7403126955032349,0.7382732629776001,0.7450713515281677,0.7471107840538025,0.7457512021064758,0.7464309930801392,0.7464309930801392,0.7450713515281677,0.7450713515281677,0.7464309930801392,0.7450713515281677,0.7450713515281677,0.7450713515281677,0.7450713515281677,0.7457512021064758,0.7471107840538025,0.7457512021064758,0.7457512021064758,0.7450713515281677,0.7450713515281677,0.7464309930801392,0.7443915605545044,0.7443915605545044,0.7457512021064758,0.7464309930801392,0.7457512021064758,0.7464309930801392,0.7457512021064758,0.7464309930801392,0.7450713515281677,0.7457512021064758,0.7471107840538025,0.7464309930801392,0.7464309930801392,0.7457512021064758,0.7471107840538025,0.7477906346321106,0.7464309930801392,0.7471107840538025,0.7471107840538025,0.7471107840538025,0.7464309930801392,0.7464309930801392,0.7464309930801392,0.7471107840538025,0.7477906346321106,0.7477906346321106,0.7471107840538025,0.7471107840538025,0.7471107840538025,0.7464309930801392,0.7464309930801392,0.7464309930801392,0.7471107840538025,0.7471107840538025,0.7471107840538025,0.7471107840538025,0.7471107840538025,0.7471107840538025,0.7471107840538025,0.7471107840538025,0.7471107840538025,0.7471107840538025,0.7477906346321106,0.7471107840538025,0.7471107840538025,0.7484704256057739,0.7477906346321106,0.7471107840538025,0.7471107840538025,0.7477906346321106,0.7484704256057739,0.7477906346321106,0.7477906346321106,0.7484704256057739,0.7484704256057739,0.7471107840538025,0.7484704256057739,0.7464309930801392,0.7498300671577454,0.7491502165794373,0.7484704256057739,0.7484704256057739,0.7477906346321106,0.7477906346321106,0.7477906346321106,0.7471107840538025,0.7477906346321106,0.7477906346321106,0.7484704256057739,0.7477906346321106,0.7484704256057739,0.7484704256057739,0.7477906346321106,0.7484704256057739,0.7471107840538025,0.7457512021064758,0.7464309930801392,0.7464309930801392,0.7464309930801392,0.7471107840538025,0.7471107840538025,0.7457512021064758,0.7464309930801392,0.7477906346321106,0.7464309930801392,0.7464309930801392,0.7457512021064758,0.7457512021064758,0.7457512021064758,0.7457512021064758,0.7457512021064758,0.7450713515281677,0.7450713515281677,0.7443915605545044,0.7450713515281677,0.7443915605545044,0.7450713515281677,0.7457512021064758,0.7457512021064758,0.7477906346321106,0.7450713515281677,0.7450713515281677,0.7457512021064758,0.7443915605545044,0.7464309930801392,0.7464309930801392,0.7464309930801392,0.7464309930801392,0.7457512021064758,0.7457512021064758,0.7464309930801392,0.7457512021064758,0.7464309930801392,0.7464309930801392,0.7471107840538025,0.7477906346321106,0.7471107840538025,0.7477906346321106,0.7484704256057739,0.7484704256057739,0.7477906346321106,0.7484704256057739,0.7484704256057739,0.7491502165794373,0.7491502165794373,0.7477906346321106,0.7484704256057739,0.7491502165794373,0.7491502165794373,0.7484704256057739,0.7484704256057739,0.7484704256057739,0.7484704256057739,0.7477906346321106,0.7484704256057739,0.7484704256057739,0.7477906346321106,0.7484704256057739,0.7484704256057739,0.7477906346321106,0.7491502165794373,0.7477906346321106,0.7471107840538025,0.7484704256057739,0.7477906346321106,0.7477906346321106,0.7484704256057739,0.7484704256057739,0.7477906346321106,0.7484704256057739,0.7477906346321106,0.7484704256057739,0.7477906346321106,0.7484704256057739,0.7471107840538025,0.7477906346321106,0.7484704256057739,0.7484704256057739,0.7477906346321106,0.7484704256057739,0.7484704256057739,0.7477906346321106,0.7484704256057739,0.7498300671577454,0.7484704256057739,0.7498300671577454,0.7471107840538025,0.7477906346321106,0.7498300671577454,0.7471107840538025,0.7464309930801392,0.7471107840538025,0.7471107840538025,0.7477906346321106,0.7471107840538025,0.7464309930801392,0.7477906346321106,0.7484704256057739,0.7477906346321106,0.7484704256057739,0.7464309930801392,0.7464309930801392,0.7464309930801392,0.7464309930801392,0.7477906346321106,0.7450713515281677,0.7450713515281677,0.7464309930801392,0.7464309930801392,0.7471107840538025,0.7477906346321106,0.7477906346321106,0.7471107840538025,0.7471107840538025,0.7471107840538025,0.7471107840538025,0.7464309930801392,0.7464309930801392,0.7471107840538025,0.7457512021064758,0.7471107840538025,0.7471107840538025,0.7471107840538025,0.7471107840538025,0.7471107840538025,0.7471107840538025,0.7471107840538025,0.7457512021064758,0.7457512021064758,0.7464309930801392,0.7464309930801392,0.7464309930801392,0.7457512021064758,0.7457512021064758,0.7464309930801392,0.7471107840538025,0.7457512021064758,0.7464309930801392,0.7457512021064758,0.7457512021064758,0.7457512021064758,0.7457512021064758,0.7471107840538025,0.7450713515281677,0.7450713515281677,0.7450713515281677,0.7464309930801392,0.7450713515281677,0.7464309930801392,0.7464309930801392,0.7450713515281677,0.7437117695808411,0.7443915605545044,0.7443915605545044,0.7450713515281677,0.7457512021064758,0.7423521280288696,0.7450713515281677,0.7457512021064758,0.7464309930801392,0.7457512021064758,0.7450713515281677,0.7437117695808411,0.7450713515281677,0.740992546081543,0.7450713515281677,0.7443915605545044,0.7450713515281677,0.7443915605545044,0.7443915605545044,0.7423521280288696,0.7430319786071777,0.7443915605545044,0.7450713515281677,0.7464309930801392,0.7457512021064758,0.7437117695808411,0.7423521280288696,0.7437117695808411,0.7430319786071777,0.7430319786071777,0.7443915605545044,0.7437117695808411,0.7430319786071777,0.7437117695808411,0.7423521280288696,0.7437117695808411,0.7430319786071777,0.7423521280288696,0.7443915605545044,0.7423521280288696,0.7423521280288696,0.7423521280288696,0.7430319786071777,0.7423521280288696,0.7423521280288696,0.7423521280288696,0.7423521280288696,0.7423521280288696,0.7423521280288696,0.7430319786071777,0.7416723370552063,0.7423521280288696,0.7416723370552063,0.7416723370552063,0.740992546081543,0.7416723370552063,0.7416723370552063,0.7416723370552063,0.740992546081543,0.7443915605545044,0.7416723370552063,0.740992546081543,0.740992546081543,0.7403126955032349,0.7403126955032349,0.7403126955032349,0.7403126955032349,0.7403126955032349,0.7403126955032349,0.740992546081543,0.7403126955032349,0.7403126955032349,0.740992546081543,0.740992546081543,0.7403126955032349,0.7396329045295715,0.7416723370552063,0.7430319786071777,0.740992546081543,0.7382732629776001,0.7403126955032349,0.740992546081543,0.7423521280288696,0.7403126955032349,0.7430319786071777,0.7403126955032349,0.7403126955032349,0.740992546081543,0.740992546081543,0.7416723370552063,0.7430319786071777,0.7437117695808411,0.7437117695808411,0.7430319786071777,0.7443915605545044,0.7430319786071777,0.7443915605545044,0.7437117695808411,0.7450713515281677,0.7443915605545044,0.7437117695808411,0.7437117695808411,0.7416723370552063,0.7430319786071777,0.7430319786071777,0.7423521280288696,0.7423521280288696,0.740992546081543,0.7416723370552063,0.740992546081543,0.7416723370552063,0.7437117695808411,0.7437117695808411,0.7416723370552063,0.7423521280288696,0.7416723370552063,0.7437117695808411,0.7423521280288696,0.7423521280288696,0.7423521280288696,0.7416723370552063,0.7423521280288696,0.7423521280288696,0.7423521280288696,0.7423521280288696,0.7423521280288696,0.7423521280288696,0.7423521280288696,0.7416723370552063,0.7416723370552063,0.7416723370552063,0.7416723370552063,0.7416723370552063,0.7416723370552063,0.7416723370552063,0.7416723370552063,0.7416723370552063,0.7416723370552063,0.740992546081543,0.740992546081543,0.740992546081543,0.7416723370552063,0.7416723370552063,0.7416723370552063,0.740992546081543,0.7416723370552063,0.7403126955032349,0.740992546081543,0.740992546081543,0.7416723370552063,0.740992546081543,0.740992546081543,0.7403126955032349,0.7403126955032349,0.740992546081543,0.7396329045295715,0.7403126955032349,0.740992546081543,0.7403126955032349,0.7396329045295715,0.740992546081543,0.7403126955032349,0.740992546081543,0.7396329045295715,0.740992546081543,0.7403126955032349,0.7396329045295715,0.7396329045295715,0.7396329045295715,0.7403126955032349,0.7396329045295715,0.7396329045295715,0.7403126955032349,0.7403126955032349,0.7396329045295715,0.7396329045295715,0.7389531135559082,0.7369136810302734,0.7396329045295715,0.7396329045295715,0.7375934720039368,0.7389531135559082,0.7389531135559082,0.7382732629776001,0.7382732629776001,0.7382732629776001,0.7375934720039368,0.7389531135559082,0.7375934720039368,0.7375934720039368,0.7375934720039368,0.7375934720039368,0.7375934720039368,0.7382732629776001,0.7375934720039368,0.7375934720039368,0.7375934720039368,0.7375934720039368,0.7375934720039368,0.7375934720039368,0.7375934720039368,0.7375934720039368,0.7375934720039368,0.7369136810302734,0.7375934720039368,0.7369136810302734,0.7369136810302734,0.7369136810302734,0.7369136810302734,0.7375934720039368,0.7375934720039368,0.7369136810302734,0.7375934720039368,0.7369136810302734,0.7369136810302734,0.7375934720039368,0.7382732629776001,0.7362338304519653,0.7362338304519653,0.735554039478302,0.7362338304519653,0.735554039478302,0.7362338304519653,0.7362338304519653,0.735554039478302,0.735554039478302,0.735554039478302,0.7362338304519653,0.735554039478302,0.735554039478302,0.735554039478302,0.735554039478302,0.7348742485046387,0.7348742485046387,0.735554039478302,0.7348742485046387,0.7348742485046387,0.7348742485046387,0.7348742485046387,0.7348742485046387],\"type\":\"scatter\"}],                        {\"title\":{\"text\":\"Accuracy\"},\"xaxis\":{\"title\":{\"text\":\"epochs\"}},\"yaxis\":{\"title\":{\"text\":\"\"}},\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('63764d9c-5c3e-4055-8c03-81e60293cde8');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "h1 = go.Scatter(y=hist.history['accuracy'], \n",
        "                    mode=\"lines\", line=dict(\n",
        "                    width=2,\n",
        "                    color='blue'),\n",
        "                    name=\"acc\"\n",
        "                   )\n",
        "h2 = go.Scatter(y=hist.history['val_accuracy'], \n",
        "                    mode=\"lines\", line=dict(\n",
        "                    width=2,\n",
        "                    color='red'),\n",
        "                    name=\"val_acc\"\n",
        "                   )\n",
        "\n",
        "data = [h1,h2]\n",
        "layout1 = go.Layout(title='Accuracy',\n",
        "                   xaxis=dict(title='epochs'),\n",
        "                   yaxis=dict(title=''))\n",
        "fig1 = go.Figure(data = data, layout=layout1)\n",
        "fig1.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFJfy4HDpX8I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c80c2c3e-9dad-40b3-a65c-ce475751aa2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 128, 100)     2269800     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 128, 100)     2269800     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, 128, 100)     2269800     ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 125, 32)      12832       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 123, 32)      19232       ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 121, 32)      25632       ['embedding_2[0][0]']            \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 125, 32)      0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 123, 32)      0           ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 121, 32)      0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 62, 32)       0           ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 61, 32)      0           ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 60, 32)      0           ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 1984)         0           ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 1952)         0           ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 1920)         0           ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 5856)         0           ['flatten[0][0]',                \n",
            "                                                                  'flatten_1[0][0]',              \n",
            "                                                                  'flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 10)           58570       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 2)            22          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,925,688\n",
            "Trainable params: 6,925,688\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "predict_model = load_model(filename) \n",
        "predict_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VehcUBITpX-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fac6666-134a-48d1-b1fd-0dda7f7d4022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validate loss: 0.4898039400577545\n",
            "Validate accuracy: 0.7607070207595825\n"
          ]
        }
      ],
      "source": [
        "score = predict_model.evaluate([val_X, val_X, val_X], val_Y, verbose=0)\n",
        "print('Validate loss:', score[0])\n",
        "print('Validate accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkVeAYS4pYBb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec4e9f4a-24f6-4984-ff1f-7485fdbeb8eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1471,)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "predicted_classes = np.argmax(predict_model.predict([val_X, val_X, val_X]), axis=-1)\n",
        "predicted_classes.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsusqncFpfrw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69ac98a5-ce89-4200-d89c-0aaef892980a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1.]\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "y_true = np.argmax(val_Y,axis = 1)\n",
        "print(val_Y[0])\n",
        "print(y_true[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5y5EzMUpfuT"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_true, predicted_classes)\n",
        "np.savetxt(\"confusion_matrix.csv\", cm, delimiter=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cc9hecMlpfw6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f47ce9ec-8dd6-4ed1-aff0-6e4d94419ccd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x1008 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "df_cm = pd.DataFrame(cm, range(2), range(2))\n",
        "plt.figure(figsize=(20,14))\n",
        "sn.set(font_scale=1.2) # for label size\n",
        "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 14}, fmt='g') # for num predict size\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zvr-185ipks1"
      },
      "outputs": [],
      "source": [
        "label_dict = output_tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymNS46w9pkvx"
      },
      "outputs": [],
      "source": [
        "label = [key for key, value in label_dict.items()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHsqF4Kkpkzd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cab129ad-b39f-4c9a-cbb7-ac51baaa7a93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "ไม่เดือดร้อน     0.7376    0.8098    0.7720       736\n",
            "   เดือดร้อน     0.7888    0.7116    0.7482       735\n",
            "\n",
            "    accuracy                         0.7607      1471\n",
            "   macro avg     0.7632    0.7607    0.7601      1471\n",
            "weighted avg     0.7632    0.7607    0.7601      1471\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_true, predicted_classes, target_names=label, digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CX2F6qGXiYz6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "enkB131liX-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "F62wZoiPiYPK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddL9MtFMpk59"
      },
      "outputs": [],
      "source": [
        "sentences = [st.split() for st in cleaned_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCBFuoEFpuUC"
      },
      "outputs": [],
      "source": [
        "w2v_model = Word2Vec(sentences, min_count=1, size=DIMENSION, workers=6, sg=1, iter=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKZtG-wrpuXt"
      },
      "outputs": [],
      "source": [
        "w2v_model.save('w2v_model.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVRtkWdhpuZz"
      },
      "outputs": [],
      "source": [
        "new_model = Word2Vec.load('w2v_model.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ArPQFM1pucY"
      },
      "outputs": [],
      "source": [
        "embedding_matrix = np.zeros((vocab_size, DIMENSION))\n",
        "\n",
        "for word, i in train_word_tokenizer.word_index.items():\n",
        "    if word in new_model.wv.vocab:\n",
        "        embedding_vector = new_model.wv[word]\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QriCXIDdpueX"
      },
      "outputs": [],
      "source": [
        "# define the model\n",
        "def define_w2v_model(length, vocab_size, embedding_matrix):\n",
        "    # channel 1\n",
        "    inputs1 = tf.keras.layers.Input(shape=(length,))\n",
        "    embedding1 = tf.keras.layers.Embedding(vocab_size, DIMENSION, trainable = False, weights=[embedding_matrix])(inputs1)\n",
        "    conv1 = tf.keras.layers.Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
        "    drop1 = tf.keras.layers.Dropout(0.5)(conv1)\n",
        "    pool1 = tf.keras.layers.MaxPooling1D(pool_size=2)(drop1)\n",
        "    flat1 = tf.keras.layers.Flatten()(pool1)\n",
        "    # channel 2\n",
        "    inputs2 = tf.keras.layers.Input(shape=(length,))\n",
        "    embedding2 = tf.keras.layers.Embedding(vocab_size, DIMENSION, trainable = False, weights=[embedding_matrix])(inputs2)\n",
        "    conv2 = tf.keras.layers.Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
        "    drop2 = tf.keras.layers.Dropout(0.5)(conv2)\n",
        "    pool2 = tf.keras.layers.MaxPooling1D(pool_size=2)(drop2)\n",
        "    flat2 = tf.keras.layers.Flatten()(pool2)\n",
        "    # channel 3\n",
        "    inputs3 = tf.keras.layers.Input(shape=(length,))\n",
        "    embedding3 = tf.keras.layers.Embedding(vocab_size, DIMENSION, trainable = False, weights=[embedding_matrix])(inputs3)\n",
        "    conv3 = tf.keras.layers.Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
        "    drop3 = tf.keras.layers.Dropout(0.5)(conv3)\n",
        "    pool3 = tf.keras.layers.MaxPooling1D(pool_size=2)(drop3)\n",
        "    flat3 = tf.keras.layers.Flatten()(pool3)\n",
        "    # merge\n",
        "    merged = tf.keras.layers.concatenate([flat1, flat2, flat3])\n",
        "    # interpretation\n",
        "    dense1 = tf.keras.layers.Dense(10, activation='relu')(merged)\n",
        "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(dense1)\n",
        "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "    # compile\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "    # summarize\n",
        "    print(model.summary())\n",
        "#     plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ztra9q_Spugt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc7443c5-78b5-4676-85ac-9c60db3f1e88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " input_5 (InputLayer)           [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, 128, 100)     2269800     ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_4 (Embedding)        (None, 128, 100)     2269800     ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_5 (Embedding)        (None, 128, 100)     2269800     ['input_6[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 125, 32)      12832       ['embedding_3[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 123, 32)      19232       ['embedding_4[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 121, 32)      25632       ['embedding_5[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 125, 32)      0           ['conv1d_3[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 123, 32)      0           ['conv1d_4[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 121, 32)      0           ['conv1d_5[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_3 (MaxPooling1D)  (None, 62, 32)      0           ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_4 (MaxPooling1D)  (None, 61, 32)      0           ['dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_5 (MaxPooling1D)  (None, 60, 32)      0           ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_3 (Flatten)            (None, 1984)         0           ['max_pooling1d_3[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_4 (Flatten)            (None, 1952)         0           ['max_pooling1d_4[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_5 (Flatten)            (None, 1920)         0           ['max_pooling1d_5[0][0]']        \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 5856)         0           ['flatten_3[0][0]',              \n",
            "                                                                  'flatten_4[0][0]',              \n",
            "                                                                  'flatten_5[0][0]']              \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 10)           58570       ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 2)            22          ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,925,688\n",
            "Trainable params: 116,288\n",
            "Non-trainable params: 6,809,400\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model2 = define_w2v_model(max_length, vocab_size, embedding_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INe9P-s2Jlg1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4f3ae84-f84e-44db-f9e1-bdda01931492"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(model2, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TT5MvXmlpui5"
      },
      "outputs": [],
      "source": [
        "filename = 'model2.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrbuu_Yqp5Ox"
      },
      "outputs": [],
      "source": [
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience = 3, verbose=1,factor=0.1, min_lr=0.000001)\n",
        "callbacks_list = [checkpoint, learning_rate_reduction]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7z4tl3op5RS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1df7da74-97cc-4989-9529-56680602113d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.7288 - accuracy: 0.5065\n",
            "Epoch 1: val_loss improved from inf to 0.69219, saving model to model2.h5\n",
            "184/184 [==============================] - 2s 7ms/step - loss: 0.7280 - accuracy: 0.5077 - val_loss: 0.6922 - val_accuracy: 0.5391 - lr: 1.0000e-06\n",
            "Epoch 2/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.7120 - accuracy: 0.5252\n",
            "Epoch 2: val_loss improved from 0.69219 to 0.68673, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.7119 - accuracy: 0.5252 - val_loss: 0.6867 - val_accuracy: 0.5479 - lr: 1.0000e-06\n",
            "Epoch 3/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.7102 - accuracy: 0.5208\n",
            "Epoch 3: val_loss improved from 0.68673 to 0.68268, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.7093 - accuracy: 0.5222 - val_loss: 0.6827 - val_accuracy: 0.5561 - lr: 1.0000e-06\n",
            "Epoch 4/500\n",
            "173/184 [===========================>..] - ETA: 0s - loss: 0.7056 - accuracy: 0.5343\n",
            "Epoch 4: val_loss improved from 0.68268 to 0.67945, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.7063 - accuracy: 0.5341 - val_loss: 0.6794 - val_accuracy: 0.5602 - lr: 1.0000e-06\n",
            "Epoch 5/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.7007 - accuracy: 0.5433\n",
            "Epoch 5: val_loss improved from 0.67945 to 0.67670, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.7011 - accuracy: 0.5431 - val_loss: 0.6767 - val_accuracy: 0.5710 - lr: 1.0000e-06\n",
            "Epoch 6/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.7009 - accuracy: 0.5315\n",
            "Epoch 6: val_loss improved from 0.67670 to 0.67438, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.7009 - accuracy: 0.5315 - val_loss: 0.6744 - val_accuracy: 0.5785 - lr: 1.0000e-06\n",
            "Epoch 7/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.6914 - accuracy: 0.5545\n",
            "Epoch 7: val_loss improved from 0.67438 to 0.67230, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6906 - accuracy: 0.5545 - val_loss: 0.6723 - val_accuracy: 0.5840 - lr: 1.0000e-06\n",
            "Epoch 8/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.6895 - accuracy: 0.5540\n",
            "Epoch 8: val_loss improved from 0.67230 to 0.67024, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6898 - accuracy: 0.5540 - val_loss: 0.6702 - val_accuracy: 0.5860 - lr: 1.0000e-06\n",
            "Epoch 9/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.6909 - accuracy: 0.5535\n",
            "Epoch 9: val_loss improved from 0.67024 to 0.66838, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6909 - accuracy: 0.5535 - val_loss: 0.6684 - val_accuracy: 0.5914 - lr: 1.0000e-06\n",
            "Epoch 10/500\n",
            "173/184 [===========================>..] - ETA: 0s - loss: 0.6874 - accuracy: 0.5517\n",
            "Epoch 10: val_loss improved from 0.66838 to 0.66666, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6874 - accuracy: 0.5528 - val_loss: 0.6667 - val_accuracy: 0.5928 - lr: 1.0000e-06\n",
            "Epoch 11/500\n",
            "173/184 [===========================>..] - ETA: 0s - loss: 0.6848 - accuracy: 0.5605\n",
            "Epoch 11: val_loss improved from 0.66666 to 0.66499, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6831 - accuracy: 0.5628 - val_loss: 0.6650 - val_accuracy: 0.5921 - lr: 1.0000e-06\n",
            "Epoch 12/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.6811 - accuracy: 0.5681\n",
            "Epoch 12: val_loss improved from 0.66499 to 0.66339, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6795 - accuracy: 0.5715 - val_loss: 0.6634 - val_accuracy: 0.5921 - lr: 1.0000e-06\n",
            "Epoch 13/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.6795 - accuracy: 0.5641\n",
            "Epoch 13: val_loss improved from 0.66339 to 0.66172, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6789 - accuracy: 0.5650 - val_loss: 0.6617 - val_accuracy: 0.5928 - lr: 1.0000e-06\n",
            "Epoch 14/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.6769 - accuracy: 0.5670\n",
            "Epoch 14: val_loss improved from 0.66172 to 0.66024, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6768 - accuracy: 0.5662 - val_loss: 0.6602 - val_accuracy: 0.5976 - lr: 1.0000e-06\n",
            "Epoch 15/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.6741 - accuracy: 0.5737\n",
            "Epoch 15: val_loss improved from 0.66024 to 0.65874, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6747 - accuracy: 0.5722 - val_loss: 0.6587 - val_accuracy: 0.6023 - lr: 1.0000e-06\n",
            "Epoch 16/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.6735 - accuracy: 0.5839\n",
            "Epoch 16: val_loss improved from 0.65874 to 0.65728, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6735 - accuracy: 0.5839 - val_loss: 0.6573 - val_accuracy: 0.6050 - lr: 1.0000e-06\n",
            "Epoch 17/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.6691 - accuracy: 0.5796\n",
            "Epoch 17: val_loss improved from 0.65728 to 0.65586, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6698 - accuracy: 0.5791 - val_loss: 0.6559 - val_accuracy: 0.6091 - lr: 1.0000e-06\n",
            "Epoch 18/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.6674 - accuracy: 0.5821\n",
            "Epoch 18: val_loss improved from 0.65586 to 0.65446, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6677 - accuracy: 0.5812 - val_loss: 0.6545 - val_accuracy: 0.6118 - lr: 1.0000e-06\n",
            "Epoch 19/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.6688 - accuracy: 0.5849\n",
            "Epoch 19: val_loss improved from 0.65446 to 0.65305, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6687 - accuracy: 0.5854 - val_loss: 0.6530 - val_accuracy: 0.6118 - lr: 1.0000e-06\n",
            "Epoch 20/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.6701 - accuracy: 0.5871\n",
            "Epoch 20: val_loss improved from 0.65305 to 0.65168, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6701 - accuracy: 0.5871 - val_loss: 0.6517 - val_accuracy: 0.6125 - lr: 1.0000e-06\n",
            "Epoch 21/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.6653 - accuracy: 0.5872\n",
            "Epoch 21: val_loss improved from 0.65168 to 0.65030, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6651 - accuracy: 0.5873 - val_loss: 0.6503 - val_accuracy: 0.6139 - lr: 1.0000e-06\n",
            "Epoch 22/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.6618 - accuracy: 0.5845\n",
            "Epoch 22: val_loss improved from 0.65030 to 0.64898, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6630 - accuracy: 0.5844 - val_loss: 0.6490 - val_accuracy: 0.6145 - lr: 1.0000e-06\n",
            "Epoch 23/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.6580 - accuracy: 0.5952\n",
            "Epoch 23: val_loss improved from 0.64898 to 0.64779, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6572 - accuracy: 0.5963 - val_loss: 0.6478 - val_accuracy: 0.6186 - lr: 1.0000e-06\n",
            "Epoch 24/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.6614 - accuracy: 0.5939\n",
            "Epoch 24: val_loss improved from 0.64779 to 0.64649, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6614 - accuracy: 0.5943 - val_loss: 0.6465 - val_accuracy: 0.6213 - lr: 1.0000e-06\n",
            "Epoch 25/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.6583 - accuracy: 0.5968\n",
            "Epoch 25: val_loss improved from 0.64649 to 0.64514, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6565 - accuracy: 0.5983 - val_loss: 0.6451 - val_accuracy: 0.6241 - lr: 1.0000e-06\n",
            "Epoch 26/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.6580 - accuracy: 0.5931\n",
            "Epoch 26: val_loss improved from 0.64514 to 0.64390, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6580 - accuracy: 0.5937 - val_loss: 0.6439 - val_accuracy: 0.6227 - lr: 1.0000e-06\n",
            "Epoch 27/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.6517 - accuracy: 0.6041\n",
            "Epoch 27: val_loss improved from 0.64390 to 0.64259, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6519 - accuracy: 0.6051 - val_loss: 0.6426 - val_accuracy: 0.6254 - lr: 1.0000e-06\n",
            "Epoch 28/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.6516 - accuracy: 0.6016\n",
            "Epoch 28: val_loss improved from 0.64259 to 0.64131, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6514 - accuracy: 0.6016 - val_loss: 0.6413 - val_accuracy: 0.6261 - lr: 1.0000e-06\n",
            "Epoch 29/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.6507 - accuracy: 0.6114\n",
            "Epoch 29: val_loss improved from 0.64131 to 0.63999, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6506 - accuracy: 0.6114 - val_loss: 0.6400 - val_accuracy: 0.6281 - lr: 1.0000e-06\n",
            "Epoch 30/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.6509 - accuracy: 0.6068\n",
            "Epoch 30: val_loss improved from 0.63999 to 0.63880, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6500 - accuracy: 0.6075 - val_loss: 0.6388 - val_accuracy: 0.6281 - lr: 1.0000e-06\n",
            "Epoch 31/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.6494 - accuracy: 0.6085\n",
            "Epoch 31: val_loss improved from 0.63880 to 0.63755, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6495 - accuracy: 0.6075 - val_loss: 0.6376 - val_accuracy: 0.6295 - lr: 1.0000e-06\n",
            "Epoch 32/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.6477 - accuracy: 0.6128\n",
            "Epoch 32: val_loss improved from 0.63755 to 0.63630, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6477 - accuracy: 0.6121 - val_loss: 0.6363 - val_accuracy: 0.6343 - lr: 1.0000e-06\n",
            "Epoch 33/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.6463 - accuracy: 0.6193\n",
            "Epoch 33: val_loss improved from 0.63630 to 0.63505, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6468 - accuracy: 0.6189 - val_loss: 0.6350 - val_accuracy: 0.6356 - lr: 1.0000e-06\n",
            "Epoch 34/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.6418 - accuracy: 0.6148\n",
            "Epoch 34: val_loss improved from 0.63505 to 0.63379, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6411 - accuracy: 0.6158 - val_loss: 0.6338 - val_accuracy: 0.6377 - lr: 1.0000e-06\n",
            "Epoch 35/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.6391 - accuracy: 0.6190\n",
            "Epoch 35: val_loss improved from 0.63379 to 0.63255, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6390 - accuracy: 0.6192 - val_loss: 0.6326 - val_accuracy: 0.6397 - lr: 1.0000e-06\n",
            "Epoch 36/500\n",
            "172/184 [===========================>..] - ETA: 0s - loss: 0.6388 - accuracy: 0.6277\n",
            "Epoch 36: val_loss improved from 0.63255 to 0.63134, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6388 - accuracy: 0.6266 - val_loss: 0.6313 - val_accuracy: 0.6431 - lr: 1.0000e-06\n",
            "Epoch 37/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.6345 - accuracy: 0.6310\n",
            "Epoch 37: val_loss improved from 0.63134 to 0.62996, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6355 - accuracy: 0.6296 - val_loss: 0.6300 - val_accuracy: 0.6472 - lr: 1.0000e-06\n",
            "Epoch 38/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.6374 - accuracy: 0.6328\n",
            "Epoch 38: val_loss improved from 0.62996 to 0.62869, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6374 - accuracy: 0.6330 - val_loss: 0.6287 - val_accuracy: 0.6479 - lr: 1.0000e-06\n",
            "Epoch 39/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.6335 - accuracy: 0.6273\n",
            "Epoch 39: val_loss improved from 0.62869 to 0.62740, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6339 - accuracy: 0.6257 - val_loss: 0.6274 - val_accuracy: 0.6526 - lr: 1.0000e-06\n",
            "Epoch 40/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.6308 - accuracy: 0.6361\n",
            "Epoch 40: val_loss improved from 0.62740 to 0.62622, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6312 - accuracy: 0.6354 - val_loss: 0.6262 - val_accuracy: 0.6533 - lr: 1.0000e-06\n",
            "Epoch 41/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.6322 - accuracy: 0.6286\n",
            "Epoch 41: val_loss improved from 0.62622 to 0.62501, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6323 - accuracy: 0.6279 - val_loss: 0.6250 - val_accuracy: 0.6553 - lr: 1.0000e-06\n",
            "Epoch 42/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.6296 - accuracy: 0.6397\n",
            "Epoch 42: val_loss improved from 0.62501 to 0.62375, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6292 - accuracy: 0.6410 - val_loss: 0.6237 - val_accuracy: 0.6567 - lr: 1.0000e-06\n",
            "Epoch 43/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.6290 - accuracy: 0.6369\n",
            "Epoch 43: val_loss improved from 0.62375 to 0.62245, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6279 - accuracy: 0.6388 - val_loss: 0.6225 - val_accuracy: 0.6553 - lr: 1.0000e-06\n",
            "Epoch 44/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.6268 - accuracy: 0.6422\n",
            "Epoch 44: val_loss improved from 0.62245 to 0.62122, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6264 - accuracy: 0.6434 - val_loss: 0.6212 - val_accuracy: 0.6601 - lr: 1.0000e-06\n",
            "Epoch 45/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.6280 - accuracy: 0.6343\n",
            "Epoch 45: val_loss improved from 0.62122 to 0.62002, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6273 - accuracy: 0.6356 - val_loss: 0.6200 - val_accuracy: 0.6608 - lr: 1.0000e-06\n",
            "Epoch 46/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.6244 - accuracy: 0.6437\n",
            "Epoch 46: val_loss improved from 0.62002 to 0.61875, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6242 - accuracy: 0.6447 - val_loss: 0.6187 - val_accuracy: 0.6621 - lr: 1.0000e-06\n",
            "Epoch 47/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.6232 - accuracy: 0.6402\n",
            "Epoch 47: val_loss improved from 0.61875 to 0.61754, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6235 - accuracy: 0.6415 - val_loss: 0.6175 - val_accuracy: 0.6635 - lr: 1.0000e-06\n",
            "Epoch 48/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.6243 - accuracy: 0.6458\n",
            "Epoch 48: val_loss improved from 0.61754 to 0.61635, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6243 - accuracy: 0.6459 - val_loss: 0.6163 - val_accuracy: 0.6662 - lr: 1.0000e-06\n",
            "Epoch 49/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.6201 - accuracy: 0.6499\n",
            "Epoch 49: val_loss improved from 0.61635 to 0.61506, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6203 - accuracy: 0.6493 - val_loss: 0.6151 - val_accuracy: 0.6676 - lr: 1.0000e-06\n",
            "Epoch 50/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.6167 - accuracy: 0.6665\n",
            "Epoch 50: val_loss improved from 0.61506 to 0.61382, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6158 - accuracy: 0.6667 - val_loss: 0.6138 - val_accuracy: 0.6730 - lr: 1.0000e-06\n",
            "Epoch 51/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.6205 - accuracy: 0.6429\n",
            "Epoch 51: val_loss improved from 0.61382 to 0.61256, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6201 - accuracy: 0.6432 - val_loss: 0.6126 - val_accuracy: 0.6744 - lr: 1.0000e-06\n",
            "Epoch 52/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.6175 - accuracy: 0.6514\n",
            "Epoch 52: val_loss improved from 0.61256 to 0.61130, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6171 - accuracy: 0.6529 - val_loss: 0.6113 - val_accuracy: 0.6764 - lr: 1.0000e-06\n",
            "Epoch 53/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.6145 - accuracy: 0.6552\n",
            "Epoch 53: val_loss improved from 0.61130 to 0.61012, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6145 - accuracy: 0.6561 - val_loss: 0.6101 - val_accuracy: 0.6805 - lr: 1.0000e-06\n",
            "Epoch 54/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.6138 - accuracy: 0.6564\n",
            "Epoch 54: val_loss improved from 0.61012 to 0.60891, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6131 - accuracy: 0.6573 - val_loss: 0.6089 - val_accuracy: 0.6812 - lr: 1.0000e-06\n",
            "Epoch 55/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.6113 - accuracy: 0.6584\n",
            "Epoch 55: val_loss improved from 0.60891 to 0.60777, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6134 - accuracy: 0.6577 - val_loss: 0.6078 - val_accuracy: 0.6832 - lr: 1.0000e-06\n",
            "Epoch 56/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.6101 - accuracy: 0.6624\n",
            "Epoch 56: val_loss improved from 0.60777 to 0.60657, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6105 - accuracy: 0.6628 - val_loss: 0.6066 - val_accuracy: 0.6818 - lr: 1.0000e-06\n",
            "Epoch 57/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.6071 - accuracy: 0.6629\n",
            "Epoch 57: val_loss improved from 0.60657 to 0.60543, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6073 - accuracy: 0.6624 - val_loss: 0.6054 - val_accuracy: 0.6852 - lr: 1.0000e-06\n",
            "Epoch 58/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.6106 - accuracy: 0.6645\n",
            "Epoch 58: val_loss improved from 0.60543 to 0.60433, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6100 - accuracy: 0.6648 - val_loss: 0.6043 - val_accuracy: 0.6873 - lr: 1.0000e-06\n",
            "Epoch 59/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.6027 - accuracy: 0.6679\n",
            "Epoch 59: val_loss improved from 0.60433 to 0.60313, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6026 - accuracy: 0.6684 - val_loss: 0.6031 - val_accuracy: 0.6893 - lr: 1.0000e-06\n",
            "Epoch 60/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.6025 - accuracy: 0.6704\n",
            "Epoch 60: val_loss improved from 0.60313 to 0.60193, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6030 - accuracy: 0.6699 - val_loss: 0.6019 - val_accuracy: 0.6900 - lr: 1.0000e-06\n",
            "Epoch 61/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.6011 - accuracy: 0.6695\n",
            "Epoch 61: val_loss improved from 0.60193 to 0.60081, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5997 - accuracy: 0.6721 - val_loss: 0.6008 - val_accuracy: 0.6941 - lr: 1.0000e-06\n",
            "Epoch 62/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.6052 - accuracy: 0.6688\n",
            "Epoch 62: val_loss improved from 0.60081 to 0.59963, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.6057 - accuracy: 0.6679 - val_loss: 0.5996 - val_accuracy: 0.6948 - lr: 1.0000e-06\n",
            "Epoch 63/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.5968 - accuracy: 0.6785\n",
            "Epoch 63: val_loss improved from 0.59963 to 0.59864, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5975 - accuracy: 0.6775 - val_loss: 0.5986 - val_accuracy: 0.6968 - lr: 1.0000e-06\n",
            "Epoch 64/500\n",
            "171/184 [==========================>...] - ETA: 0s - loss: 0.6016 - accuracy: 0.6714\n",
            "Epoch 64: val_loss improved from 0.59864 to 0.59750, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5994 - accuracy: 0.6735 - val_loss: 0.5975 - val_accuracy: 0.6975 - lr: 1.0000e-06\n",
            "Epoch 65/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.5984 - accuracy: 0.6735\n",
            "Epoch 65: val_loss improved from 0.59750 to 0.59641, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5987 - accuracy: 0.6735 - val_loss: 0.5964 - val_accuracy: 0.7016 - lr: 1.0000e-06\n",
            "Epoch 66/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.5982 - accuracy: 0.6700\n",
            "Epoch 66: val_loss improved from 0.59641 to 0.59533, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5975 - accuracy: 0.6714 - val_loss: 0.5953 - val_accuracy: 0.7016 - lr: 1.0000e-06\n",
            "Epoch 67/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.5901 - accuracy: 0.6840\n",
            "Epoch 67: val_loss improved from 0.59533 to 0.59422, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5904 - accuracy: 0.6837 - val_loss: 0.5942 - val_accuracy: 0.7036 - lr: 1.0000e-06\n",
            "Epoch 68/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.5910 - accuracy: 0.6805\n",
            "Epoch 68: val_loss improved from 0.59422 to 0.59310, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5913 - accuracy: 0.6798 - val_loss: 0.5931 - val_accuracy: 0.7043 - lr: 1.0000e-06\n",
            "Epoch 69/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.5936 - accuracy: 0.6776\n",
            "Epoch 69: val_loss improved from 0.59310 to 0.59217, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5938 - accuracy: 0.6772 - val_loss: 0.5922 - val_accuracy: 0.7063 - lr: 1.0000e-06\n",
            "Epoch 70/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.5884 - accuracy: 0.6786\n",
            "Epoch 70: val_loss improved from 0.59217 to 0.59097, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5884 - accuracy: 0.6770 - val_loss: 0.5910 - val_accuracy: 0.7063 - lr: 1.0000e-06\n",
            "Epoch 71/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.5910 - accuracy: 0.6762\n",
            "Epoch 71: val_loss improved from 0.59097 to 0.58993, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5914 - accuracy: 0.6762 - val_loss: 0.5899 - val_accuracy: 0.7070 - lr: 1.0000e-06\n",
            "Epoch 72/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.5871 - accuracy: 0.6941\n",
            "Epoch 72: val_loss improved from 0.58993 to 0.58884, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5866 - accuracy: 0.6940 - val_loss: 0.5888 - val_accuracy: 0.7077 - lr: 1.0000e-06\n",
            "Epoch 73/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.5868 - accuracy: 0.6852\n",
            "Epoch 73: val_loss improved from 0.58884 to 0.58777, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5873 - accuracy: 0.6843 - val_loss: 0.5878 - val_accuracy: 0.7070 - lr: 1.0000e-06\n",
            "Epoch 74/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.5868 - accuracy: 0.6802\n",
            "Epoch 74: val_loss improved from 0.58777 to 0.58684, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5874 - accuracy: 0.6799 - val_loss: 0.5868 - val_accuracy: 0.7077 - lr: 1.0000e-06\n",
            "Epoch 75/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.5840 - accuracy: 0.6887\n",
            "Epoch 75: val_loss improved from 0.58684 to 0.58596, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5857 - accuracy: 0.6871 - val_loss: 0.5860 - val_accuracy: 0.7077 - lr: 1.0000e-06\n",
            "Epoch 76/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.5780 - accuracy: 0.6940\n",
            "Epoch 76: val_loss improved from 0.58596 to 0.58476, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5785 - accuracy: 0.6918 - val_loss: 0.5848 - val_accuracy: 0.7097 - lr: 1.0000e-06\n",
            "Epoch 77/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.5881 - accuracy: 0.6849\n",
            "Epoch 77: val_loss improved from 0.58476 to 0.58382, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5876 - accuracy: 0.6850 - val_loss: 0.5838 - val_accuracy: 0.7097 - lr: 1.0000e-06\n",
            "Epoch 78/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.5809 - accuracy: 0.6949\n",
            "Epoch 78: val_loss improved from 0.58382 to 0.58285, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5807 - accuracy: 0.6934 - val_loss: 0.5829 - val_accuracy: 0.7111 - lr: 1.0000e-06\n",
            "Epoch 79/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.5780 - accuracy: 0.6924\n",
            "Epoch 79: val_loss improved from 0.58285 to 0.58191, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5804 - accuracy: 0.6913 - val_loss: 0.5819 - val_accuracy: 0.7131 - lr: 1.0000e-06\n",
            "Epoch 80/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.5746 - accuracy: 0.6944\n",
            "Epoch 80: val_loss improved from 0.58191 to 0.58110, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5746 - accuracy: 0.6939 - val_loss: 0.5811 - val_accuracy: 0.7118 - lr: 1.0000e-06\n",
            "Epoch 81/500\n",
            "173/184 [===========================>..] - ETA: 0s - loss: 0.5747 - accuracy: 0.6868\n",
            "Epoch 81: val_loss improved from 0.58110 to 0.58028, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5757 - accuracy: 0.6862 - val_loss: 0.5803 - val_accuracy: 0.7111 - lr: 1.0000e-06\n",
            "Epoch 82/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.5775 - accuracy: 0.6916\n",
            "Epoch 82: val_loss improved from 0.58028 to 0.57943, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5777 - accuracy: 0.6910 - val_loss: 0.5794 - val_accuracy: 0.7111 - lr: 1.0000e-06\n",
            "Epoch 83/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.5777 - accuracy: 0.6925\n",
            "Epoch 83: val_loss improved from 0.57943 to 0.57870, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5774 - accuracy: 0.6934 - val_loss: 0.5787 - val_accuracy: 0.7090 - lr: 1.0000e-06\n",
            "Epoch 84/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.5742 - accuracy: 0.6977\n",
            "Epoch 84: val_loss improved from 0.57870 to 0.57780, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5747 - accuracy: 0.6959 - val_loss: 0.5778 - val_accuracy: 0.7104 - lr: 1.0000e-06\n",
            "Epoch 85/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.5708 - accuracy: 0.6991\n",
            "Epoch 85: val_loss improved from 0.57780 to 0.57691, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5706 - accuracy: 0.6990 - val_loss: 0.5769 - val_accuracy: 0.7104 - lr: 1.0000e-06\n",
            "Epoch 86/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.5741 - accuracy: 0.6930\n",
            "Epoch 86: val_loss improved from 0.57691 to 0.57619, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5730 - accuracy: 0.6945 - val_loss: 0.5762 - val_accuracy: 0.7111 - lr: 1.0000e-06\n",
            "Epoch 87/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.5715 - accuracy: 0.7010\n",
            "Epoch 87: val_loss improved from 0.57619 to 0.57537, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5717 - accuracy: 0.7007 - val_loss: 0.5754 - val_accuracy: 0.7111 - lr: 1.0000e-06\n",
            "Epoch 88/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.5706 - accuracy: 0.7036\n",
            "Epoch 88: val_loss improved from 0.57537 to 0.57455, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5706 - accuracy: 0.7037 - val_loss: 0.5746 - val_accuracy: 0.7104 - lr: 1.0000e-06\n",
            "Epoch 89/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.5700 - accuracy: 0.7012\n",
            "Epoch 89: val_loss improved from 0.57455 to 0.57392, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7015 - val_loss: 0.5739 - val_accuracy: 0.7104 - lr: 1.0000e-06\n",
            "Epoch 90/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.5720 - accuracy: 0.6994\n",
            "Epoch 90: val_loss improved from 0.57392 to 0.57314, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5717 - accuracy: 0.6990 - val_loss: 0.5731 - val_accuracy: 0.7111 - lr: 1.0000e-06\n",
            "Epoch 91/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.5670 - accuracy: 0.7076\n",
            "Epoch 91: val_loss improved from 0.57314 to 0.57244, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5670 - accuracy: 0.7071 - val_loss: 0.5724 - val_accuracy: 0.7118 - lr: 1.0000e-06\n",
            "Epoch 92/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.5674 - accuracy: 0.7087\n",
            "Epoch 92: val_loss improved from 0.57244 to 0.57178, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5674 - accuracy: 0.7087 - val_loss: 0.5718 - val_accuracy: 0.7118 - lr: 1.0000e-06\n",
            "Epoch 93/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.5664 - accuracy: 0.7048\n",
            "Epoch 93: val_loss improved from 0.57178 to 0.57111, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5666 - accuracy: 0.7039 - val_loss: 0.5711 - val_accuracy: 0.7131 - lr: 1.0000e-06\n",
            "Epoch 94/500\n",
            "172/184 [===========================>..] - ETA: 0s - loss: 0.5640 - accuracy: 0.7015\n",
            "Epoch 94: val_loss improved from 0.57111 to 0.57034, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5639 - accuracy: 0.7034 - val_loss: 0.5703 - val_accuracy: 0.7104 - lr: 1.0000e-06\n",
            "Epoch 95/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.5642 - accuracy: 0.6973\n",
            "Epoch 95: val_loss improved from 0.57034 to 0.56966, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5657 - accuracy: 0.6959 - val_loss: 0.5697 - val_accuracy: 0.7124 - lr: 1.0000e-06\n",
            "Epoch 96/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.5609 - accuracy: 0.7097\n",
            "Epoch 96: val_loss improved from 0.56966 to 0.56885, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5612 - accuracy: 0.7095 - val_loss: 0.5688 - val_accuracy: 0.7131 - lr: 1.0000e-06\n",
            "Epoch 97/500\n",
            "173/184 [===========================>..] - ETA: 0s - loss: 0.5616 - accuracy: 0.7025\n",
            "Epoch 97: val_loss improved from 0.56885 to 0.56829, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5638 - accuracy: 0.7015 - val_loss: 0.5683 - val_accuracy: 0.7138 - lr: 1.0000e-06\n",
            "Epoch 98/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.5631 - accuracy: 0.6989\n",
            "Epoch 98: val_loss improved from 0.56829 to 0.56772, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5633 - accuracy: 0.6976 - val_loss: 0.5677 - val_accuracy: 0.7138 - lr: 1.0000e-06\n",
            "Epoch 99/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.5629 - accuracy: 0.7022\n",
            "Epoch 99: val_loss improved from 0.56772 to 0.56697, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5632 - accuracy: 0.7017 - val_loss: 0.5670 - val_accuracy: 0.7145 - lr: 1.0000e-06\n",
            "Epoch 100/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.5600 - accuracy: 0.7046\n",
            "Epoch 100: val_loss improved from 0.56697 to 0.56641, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5619 - accuracy: 0.7039 - val_loss: 0.5664 - val_accuracy: 0.7145 - lr: 1.0000e-06\n",
            "Epoch 101/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.5573 - accuracy: 0.7097\n",
            "Epoch 101: val_loss improved from 0.56641 to 0.56582, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5570 - accuracy: 0.7095 - val_loss: 0.5658 - val_accuracy: 0.7152 - lr: 1.0000e-06\n",
            "Epoch 102/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.5574 - accuracy: 0.7069\n",
            "Epoch 102: val_loss improved from 0.56582 to 0.56524, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5556 - accuracy: 0.7080 - val_loss: 0.5652 - val_accuracy: 0.7158 - lr: 1.0000e-06\n",
            "Epoch 103/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.5563 - accuracy: 0.7160\n",
            "Epoch 103: val_loss improved from 0.56524 to 0.56444, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5565 - accuracy: 0.7156 - val_loss: 0.5644 - val_accuracy: 0.7152 - lr: 1.0000e-06\n",
            "Epoch 104/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.5557 - accuracy: 0.7134\n",
            "Epoch 104: val_loss improved from 0.56444 to 0.56399, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5556 - accuracy: 0.7139 - val_loss: 0.5640 - val_accuracy: 0.7152 - lr: 1.0000e-06\n",
            "Epoch 105/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.5540 - accuracy: 0.7115\n",
            "Epoch 105: val_loss improved from 0.56399 to 0.56344, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5545 - accuracy: 0.7112 - val_loss: 0.5634 - val_accuracy: 0.7172 - lr: 1.0000e-06\n",
            "Epoch 106/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.5549 - accuracy: 0.7124\n",
            "Epoch 106: val_loss improved from 0.56344 to 0.56295, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5550 - accuracy: 0.7122 - val_loss: 0.5629 - val_accuracy: 0.7179 - lr: 1.0000e-06\n",
            "Epoch 107/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.5583 - accuracy: 0.7079\n",
            "Epoch 107: val_loss improved from 0.56295 to 0.56243, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5586 - accuracy: 0.7087 - val_loss: 0.5624 - val_accuracy: 0.7179 - lr: 1.0000e-06\n",
            "Epoch 108/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.5534 - accuracy: 0.7132\n",
            "Epoch 108: val_loss improved from 0.56243 to 0.56206, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5538 - accuracy: 0.7131 - val_loss: 0.5621 - val_accuracy: 0.7172 - lr: 1.0000e-06\n",
            "Epoch 109/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.5564 - accuracy: 0.7100\n",
            "Epoch 109: val_loss improved from 0.56206 to 0.56149, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5557 - accuracy: 0.7112 - val_loss: 0.5615 - val_accuracy: 0.7179 - lr: 1.0000e-06\n",
            "Epoch 110/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.5541 - accuracy: 0.7086\n",
            "Epoch 110: val_loss improved from 0.56149 to 0.56100, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5529 - accuracy: 0.7092 - val_loss: 0.5610 - val_accuracy: 0.7179 - lr: 1.0000e-06\n",
            "Epoch 111/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.5555 - accuracy: 0.7097\n",
            "Epoch 111: val_loss improved from 0.56100 to 0.56039, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5553 - accuracy: 0.7100 - val_loss: 0.5604 - val_accuracy: 0.7199 - lr: 1.0000e-06\n",
            "Epoch 112/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.5553 - accuracy: 0.7126\n",
            "Epoch 112: val_loss improved from 0.56039 to 0.55995, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5546 - accuracy: 0.7124 - val_loss: 0.5599 - val_accuracy: 0.7220 - lr: 1.0000e-06\n",
            "Epoch 113/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.5550 - accuracy: 0.7155\n",
            "Epoch 113: val_loss improved from 0.55995 to 0.55943, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5534 - accuracy: 0.7165 - val_loss: 0.5594 - val_accuracy: 0.7206 - lr: 1.0000e-06\n",
            "Epoch 114/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.5552 - accuracy: 0.7099\n",
            "Epoch 114: val_loss improved from 0.55943 to 0.55899, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5525 - accuracy: 0.7117 - val_loss: 0.5590 - val_accuracy: 0.7220 - lr: 1.0000e-06\n",
            "Epoch 115/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.5493 - accuracy: 0.7160\n",
            "Epoch 115: val_loss improved from 0.55899 to 0.55855, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5492 - accuracy: 0.7160 - val_loss: 0.5586 - val_accuracy: 0.7213 - lr: 1.0000e-06\n",
            "Epoch 116/500\n",
            "173/184 [===========================>..] - ETA: 0s - loss: 0.5512 - accuracy: 0.7169\n",
            "Epoch 116: val_loss improved from 0.55855 to 0.55818, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5496 - accuracy: 0.7175 - val_loss: 0.5582 - val_accuracy: 0.7213 - lr: 1.0000e-06\n",
            "Epoch 117/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.5510 - accuracy: 0.7189\n",
            "Epoch 117: val_loss improved from 0.55818 to 0.55777, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5520 - accuracy: 0.7175 - val_loss: 0.5578 - val_accuracy: 0.7220 - lr: 1.0000e-06\n",
            "Epoch 118/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.5503 - accuracy: 0.7163\n",
            "Epoch 118: val_loss improved from 0.55777 to 0.55730, saving model to model2.h5\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.5494 - accuracy: 0.7165 - val_loss: 0.5573 - val_accuracy: 0.7226 - lr: 1.0000e-06\n",
            "Epoch 119/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.5406 - accuracy: 0.7258\n",
            "Epoch 119: val_loss improved from 0.55730 to 0.55688, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5406 - accuracy: 0.7258 - val_loss: 0.5569 - val_accuracy: 0.7226 - lr: 1.0000e-06\n",
            "Epoch 120/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.5468 - accuracy: 0.7189\n",
            "Epoch 120: val_loss improved from 0.55688 to 0.55647, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5466 - accuracy: 0.7190 - val_loss: 0.5565 - val_accuracy: 0.7226 - lr: 1.0000e-06\n",
            "Epoch 121/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.5443 - accuracy: 0.7198\n",
            "Epoch 121: val_loss improved from 0.55647 to 0.55602, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5439 - accuracy: 0.7204 - val_loss: 0.5560 - val_accuracy: 0.7240 - lr: 1.0000e-06\n",
            "Epoch 122/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.5435 - accuracy: 0.7195\n",
            "Epoch 122: val_loss improved from 0.55602 to 0.55563, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5440 - accuracy: 0.7199 - val_loss: 0.5556 - val_accuracy: 0.7240 - lr: 1.0000e-06\n",
            "Epoch 123/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.5428 - accuracy: 0.7268\n",
            "Epoch 123: val_loss improved from 0.55563 to 0.55527, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5422 - accuracy: 0.7279 - val_loss: 0.5553 - val_accuracy: 0.7247 - lr: 1.0000e-06\n",
            "Epoch 124/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.5396 - accuracy: 0.7224\n",
            "Epoch 124: val_loss improved from 0.55527 to 0.55490, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5391 - accuracy: 0.7223 - val_loss: 0.5549 - val_accuracy: 0.7260 - lr: 1.0000e-06\n",
            "Epoch 125/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.5422 - accuracy: 0.7212\n",
            "Epoch 125: val_loss improved from 0.55490 to 0.55462, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5423 - accuracy: 0.7217 - val_loss: 0.5546 - val_accuracy: 0.7274 - lr: 1.0000e-06\n",
            "Epoch 126/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.5403 - accuracy: 0.7228\n",
            "Epoch 126: val_loss improved from 0.55462 to 0.55422, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5396 - accuracy: 0.7221 - val_loss: 0.5542 - val_accuracy: 0.7267 - lr: 1.0000e-06\n",
            "Epoch 127/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.5447 - accuracy: 0.7216\n",
            "Epoch 127: val_loss improved from 0.55422 to 0.55375, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5435 - accuracy: 0.7229 - val_loss: 0.5538 - val_accuracy: 0.7274 - lr: 1.0000e-06\n",
            "Epoch 128/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.5432 - accuracy: 0.7232\n",
            "Epoch 128: val_loss improved from 0.55375 to 0.55341, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5428 - accuracy: 0.7236 - val_loss: 0.5534 - val_accuracy: 0.7274 - lr: 1.0000e-06\n",
            "Epoch 129/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.5427 - accuracy: 0.7253\n",
            "Epoch 129: val_loss improved from 0.55341 to 0.55305, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5428 - accuracy: 0.7243 - val_loss: 0.5530 - val_accuracy: 0.7281 - lr: 1.0000e-06\n",
            "Epoch 130/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.5405 - accuracy: 0.7245\n",
            "Epoch 130: val_loss improved from 0.55305 to 0.55270, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5405 - accuracy: 0.7245 - val_loss: 0.5527 - val_accuracy: 0.7274 - lr: 1.0000e-06\n",
            "Epoch 131/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.5368 - accuracy: 0.7252\n",
            "Epoch 131: val_loss improved from 0.55270 to 0.55242, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5372 - accuracy: 0.7253 - val_loss: 0.5524 - val_accuracy: 0.7288 - lr: 1.0000e-06\n",
            "Epoch 132/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.5361 - accuracy: 0.7281\n",
            "Epoch 132: val_loss improved from 0.55242 to 0.55207, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5378 - accuracy: 0.7265 - val_loss: 0.5521 - val_accuracy: 0.7281 - lr: 1.0000e-06\n",
            "Epoch 133/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.5383 - accuracy: 0.7262\n",
            "Epoch 133: val_loss improved from 0.55207 to 0.55182, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5374 - accuracy: 0.7267 - val_loss: 0.5518 - val_accuracy: 0.7294 - lr: 1.0000e-06\n",
            "Epoch 134/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.5384 - accuracy: 0.7233\n",
            "Epoch 134: val_loss improved from 0.55182 to 0.55149, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5384 - accuracy: 0.7233 - val_loss: 0.5515 - val_accuracy: 0.7294 - lr: 1.0000e-06\n",
            "Epoch 135/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.5396 - accuracy: 0.7265\n",
            "Epoch 135: val_loss improved from 0.55149 to 0.55122, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5380 - accuracy: 0.7282 - val_loss: 0.5512 - val_accuracy: 0.7288 - lr: 1.0000e-06\n",
            "Epoch 136/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.5354 - accuracy: 0.7287\n",
            "Epoch 136: val_loss improved from 0.55122 to 0.55093, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5354 - accuracy: 0.7289 - val_loss: 0.5509 - val_accuracy: 0.7294 - lr: 1.0000e-06\n",
            "Epoch 137/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.5374 - accuracy: 0.7222\n",
            "Epoch 137: val_loss improved from 0.55093 to 0.55054, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5370 - accuracy: 0.7228 - val_loss: 0.5505 - val_accuracy: 0.7294 - lr: 1.0000e-06\n",
            "Epoch 138/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.5369 - accuracy: 0.7307\n",
            "Epoch 138: val_loss improved from 0.55054 to 0.55022, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5381 - accuracy: 0.7297 - val_loss: 0.5502 - val_accuracy: 0.7301 - lr: 1.0000e-06\n",
            "Epoch 139/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.5378 - accuracy: 0.7263\n",
            "Epoch 139: val_loss improved from 0.55022 to 0.54992, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5378 - accuracy: 0.7263 - val_loss: 0.5499 - val_accuracy: 0.7294 - lr: 1.0000e-06\n",
            "Epoch 140/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.5397 - accuracy: 0.7229\n",
            "Epoch 140: val_loss improved from 0.54992 to 0.54964, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5408 - accuracy: 0.7221 - val_loss: 0.5496 - val_accuracy: 0.7294 - lr: 1.0000e-06\n",
            "Epoch 141/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.5343 - accuracy: 0.7323\n",
            "Epoch 141: val_loss improved from 0.54964 to 0.54935, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5353 - accuracy: 0.7307 - val_loss: 0.5494 - val_accuracy: 0.7288 - lr: 1.0000e-06\n",
            "Epoch 142/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.5384 - accuracy: 0.7237\n",
            "Epoch 142: val_loss improved from 0.54935 to 0.54915, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5379 - accuracy: 0.7246 - val_loss: 0.5492 - val_accuracy: 0.7288 - lr: 1.0000e-06\n",
            "Epoch 143/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.5332 - accuracy: 0.7299\n",
            "Epoch 143: val_loss improved from 0.54915 to 0.54888, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5335 - accuracy: 0.7304 - val_loss: 0.5489 - val_accuracy: 0.7281 - lr: 1.0000e-06\n",
            "Epoch 144/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.5376 - accuracy: 0.7218\n",
            "Epoch 144: val_loss improved from 0.54888 to 0.54865, saving model to model2.h5\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.5362 - accuracy: 0.7219 - val_loss: 0.5487 - val_accuracy: 0.7288 - lr: 1.0000e-06\n",
            "Epoch 145/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.5313 - accuracy: 0.7374\n",
            "Epoch 145: val_loss improved from 0.54865 to 0.54840, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5319 - accuracy: 0.7375 - val_loss: 0.5484 - val_accuracy: 0.7288 - lr: 1.0000e-06\n",
            "Epoch 146/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.5338 - accuracy: 0.7289\n",
            "Epoch 146: val_loss improved from 0.54840 to 0.54816, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5340 - accuracy: 0.7290 - val_loss: 0.5482 - val_accuracy: 0.7294 - lr: 1.0000e-06\n",
            "Epoch 147/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.5332 - accuracy: 0.7313\n",
            "Epoch 147: val_loss improved from 0.54816 to 0.54796, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5336 - accuracy: 0.7292 - val_loss: 0.5480 - val_accuracy: 0.7301 - lr: 1.0000e-06\n",
            "Epoch 148/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.5286 - accuracy: 0.7367\n",
            "Epoch 148: val_loss improved from 0.54796 to 0.54762, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 8ms/step - loss: 0.5286 - accuracy: 0.7372 - val_loss: 0.5476 - val_accuracy: 0.7301 - lr: 1.0000e-06\n",
            "Epoch 149/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.5340 - accuracy: 0.7308\n",
            "Epoch 149: val_loss improved from 0.54762 to 0.54733, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5346 - accuracy: 0.7301 - val_loss: 0.5473 - val_accuracy: 0.7308 - lr: 1.0000e-06\n",
            "Epoch 150/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.5346 - accuracy: 0.7309\n",
            "Epoch 150: val_loss improved from 0.54733 to 0.54712, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5339 - accuracy: 0.7316 - val_loss: 0.5471 - val_accuracy: 0.7308 - lr: 1.0000e-06\n",
            "Epoch 151/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.5303 - accuracy: 0.7340\n",
            "Epoch 151: val_loss improved from 0.54712 to 0.54685, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5297 - accuracy: 0.7364 - val_loss: 0.5469 - val_accuracy: 0.7315 - lr: 1.0000e-06\n",
            "Epoch 152/500\n",
            "173/184 [===========================>..] - ETA: 0s - loss: 0.5307 - accuracy: 0.7325\n",
            "Epoch 152: val_loss improved from 0.54685 to 0.54659, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5321 - accuracy: 0.7319 - val_loss: 0.5466 - val_accuracy: 0.7315 - lr: 1.0000e-06\n",
            "Epoch 153/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.5329 - accuracy: 0.7360\n",
            "Epoch 153: val_loss improved from 0.54659 to 0.54630, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5327 - accuracy: 0.7362 - val_loss: 0.5463 - val_accuracy: 0.7308 - lr: 1.0000e-06\n",
            "Epoch 154/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.5306 - accuracy: 0.7290\n",
            "Epoch 154: val_loss improved from 0.54630 to 0.54616, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5297 - accuracy: 0.7311 - val_loss: 0.5462 - val_accuracy: 0.7308 - lr: 1.0000e-06\n",
            "Epoch 155/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.5283 - accuracy: 0.7345\n",
            "Epoch 155: val_loss improved from 0.54616 to 0.54586, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5290 - accuracy: 0.7338 - val_loss: 0.5459 - val_accuracy: 0.7315 - lr: 1.0000e-06\n",
            "Epoch 156/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.5315 - accuracy: 0.7350\n",
            "Epoch 156: val_loss improved from 0.54586 to 0.54565, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5306 - accuracy: 0.7357 - val_loss: 0.5456 - val_accuracy: 0.7322 - lr: 1.0000e-06\n",
            "Epoch 157/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.5277 - accuracy: 0.7376\n",
            "Epoch 157: val_loss improved from 0.54565 to 0.54547, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5295 - accuracy: 0.7362 - val_loss: 0.5455 - val_accuracy: 0.7315 - lr: 1.0000e-06\n",
            "Epoch 158/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.5281 - accuracy: 0.7333\n",
            "Epoch 158: val_loss improved from 0.54547 to 0.54529, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5291 - accuracy: 0.7326 - val_loss: 0.5453 - val_accuracy: 0.7322 - lr: 1.0000e-06\n",
            "Epoch 159/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.5301 - accuracy: 0.7332\n",
            "Epoch 159: val_loss improved from 0.54529 to 0.54507, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5300 - accuracy: 0.7328 - val_loss: 0.5451 - val_accuracy: 0.7328 - lr: 1.0000e-06\n",
            "Epoch 160/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.5278 - accuracy: 0.7363\n",
            "Epoch 160: val_loss improved from 0.54507 to 0.54483, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5284 - accuracy: 0.7355 - val_loss: 0.5448 - val_accuracy: 0.7328 - lr: 1.0000e-06\n",
            "Epoch 161/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.5224 - accuracy: 0.7394\n",
            "Epoch 161: val_loss improved from 0.54483 to 0.54450, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5228 - accuracy: 0.7392 - val_loss: 0.5445 - val_accuracy: 0.7335 - lr: 1.0000e-06\n",
            "Epoch 162/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.5227 - accuracy: 0.7392\n",
            "Epoch 162: val_loss improved from 0.54450 to 0.54430, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5228 - accuracy: 0.7401 - val_loss: 0.5443 - val_accuracy: 0.7328 - lr: 1.0000e-06\n",
            "Epoch 163/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.5275 - accuracy: 0.7327\n",
            "Epoch 163: val_loss improved from 0.54430 to 0.54417, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5258 - accuracy: 0.7341 - val_loss: 0.5442 - val_accuracy: 0.7335 - lr: 1.0000e-06\n",
            "Epoch 164/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.5253 - accuracy: 0.7362\n",
            "Epoch 164: val_loss improved from 0.54417 to 0.54398, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5272 - accuracy: 0.7343 - val_loss: 0.5440 - val_accuracy: 0.7342 - lr: 1.0000e-06\n",
            "Epoch 165/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.5222 - accuracy: 0.7393\n",
            "Epoch 165: val_loss improved from 0.54398 to 0.54382, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 8ms/step - loss: 0.5236 - accuracy: 0.7377 - val_loss: 0.5438 - val_accuracy: 0.7342 - lr: 1.0000e-06\n",
            "Epoch 166/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.5227 - accuracy: 0.7385\n",
            "Epoch 166: val_loss improved from 0.54382 to 0.54356, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5232 - accuracy: 0.7389 - val_loss: 0.5436 - val_accuracy: 0.7342 - lr: 1.0000e-06\n",
            "Epoch 167/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.5259 - accuracy: 0.7422\n",
            "Epoch 167: val_loss improved from 0.54356 to 0.54333, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5268 - accuracy: 0.7415 - val_loss: 0.5433 - val_accuracy: 0.7362 - lr: 1.0000e-06\n",
            "Epoch 168/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.5279 - accuracy: 0.7326\n",
            "Epoch 168: val_loss improved from 0.54333 to 0.54313, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5272 - accuracy: 0.7330 - val_loss: 0.5431 - val_accuracy: 0.7356 - lr: 1.0000e-06\n",
            "Epoch 169/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.5269 - accuracy: 0.7363\n",
            "Epoch 169: val_loss improved from 0.54313 to 0.54290, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5268 - accuracy: 0.7358 - val_loss: 0.5429 - val_accuracy: 0.7356 - lr: 1.0000e-06\n",
            "Epoch 170/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.5228 - accuracy: 0.7344\n",
            "Epoch 170: val_loss improved from 0.54290 to 0.54274, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5237 - accuracy: 0.7345 - val_loss: 0.5427 - val_accuracy: 0.7362 - lr: 1.0000e-06\n",
            "Epoch 171/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.5259 - accuracy: 0.7368\n",
            "Epoch 171: val_loss improved from 0.54274 to 0.54252, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5264 - accuracy: 0.7360 - val_loss: 0.5425 - val_accuracy: 0.7362 - lr: 1.0000e-06\n",
            "Epoch 172/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.5248 - accuracy: 0.7376\n",
            "Epoch 172: val_loss improved from 0.54252 to 0.54232, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5250 - accuracy: 0.7375 - val_loss: 0.5423 - val_accuracy: 0.7356 - lr: 1.0000e-06\n",
            "Epoch 173/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.5254 - accuracy: 0.7333\n",
            "Epoch 173: val_loss improved from 0.54232 to 0.54222, saving model to model2.h5\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.5234 - accuracy: 0.7347 - val_loss: 0.5422 - val_accuracy: 0.7362 - lr: 1.0000e-06\n",
            "Epoch 174/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.5273 - accuracy: 0.7364\n",
            "Epoch 174: val_loss improved from 0.54222 to 0.54202, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5263 - accuracy: 0.7367 - val_loss: 0.5420 - val_accuracy: 0.7376 - lr: 1.0000e-06\n",
            "Epoch 175/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.5180 - accuracy: 0.7425\n",
            "Epoch 175: val_loss improved from 0.54202 to 0.54185, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5190 - accuracy: 0.7413 - val_loss: 0.5418 - val_accuracy: 0.7376 - lr: 1.0000e-06\n",
            "Epoch 176/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.5245 - accuracy: 0.7329\n",
            "Epoch 176: val_loss improved from 0.54185 to 0.54159, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5264 - accuracy: 0.7307 - val_loss: 0.5416 - val_accuracy: 0.7376 - lr: 1.0000e-06\n",
            "Epoch 177/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.5207 - accuracy: 0.7365\n",
            "Epoch 177: val_loss improved from 0.54159 to 0.54146, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5230 - accuracy: 0.7340 - val_loss: 0.5415 - val_accuracy: 0.7369 - lr: 1.0000e-06\n",
            "Epoch 178/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.5190 - accuracy: 0.7435\n",
            "Epoch 178: val_loss improved from 0.54146 to 0.54126, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5205 - accuracy: 0.7415 - val_loss: 0.5413 - val_accuracy: 0.7390 - lr: 1.0000e-06\n",
            "Epoch 179/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.5189 - accuracy: 0.7413\n",
            "Epoch 179: val_loss improved from 0.54126 to 0.54110, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5189 - accuracy: 0.7409 - val_loss: 0.5411 - val_accuracy: 0.7390 - lr: 1.0000e-06\n",
            "Epoch 180/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.5226 - accuracy: 0.7407\n",
            "Epoch 180: val_loss improved from 0.54110 to 0.54090, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5235 - accuracy: 0.7396 - val_loss: 0.5409 - val_accuracy: 0.7390 - lr: 1.0000e-06\n",
            "Epoch 181/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.5167 - accuracy: 0.7412\n",
            "Epoch 181: val_loss improved from 0.54090 to 0.54073, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5181 - accuracy: 0.7406 - val_loss: 0.5407 - val_accuracy: 0.7369 - lr: 1.0000e-06\n",
            "Epoch 182/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.5198 - accuracy: 0.7374\n",
            "Epoch 182: val_loss improved from 0.54073 to 0.54056, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5203 - accuracy: 0.7375 - val_loss: 0.5406 - val_accuracy: 0.7390 - lr: 1.0000e-06\n",
            "Epoch 183/500\n",
            "172/184 [===========================>..] - ETA: 0s - loss: 0.5198 - accuracy: 0.7418\n",
            "Epoch 183: val_loss improved from 0.54056 to 0.54041, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5201 - accuracy: 0.7411 - val_loss: 0.5404 - val_accuracy: 0.7376 - lr: 1.0000e-06\n",
            "Epoch 184/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.5241 - accuracy: 0.7378\n",
            "Epoch 184: val_loss improved from 0.54041 to 0.54023, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5247 - accuracy: 0.7367 - val_loss: 0.5402 - val_accuracy: 0.7383 - lr: 1.0000e-06\n",
            "Epoch 185/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.5215 - accuracy: 0.7360\n",
            "Epoch 185: val_loss improved from 0.54023 to 0.54010, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5215 - accuracy: 0.7360 - val_loss: 0.5401 - val_accuracy: 0.7383 - lr: 1.0000e-06\n",
            "Epoch 186/500\n",
            "173/184 [===========================>..] - ETA: 0s - loss: 0.5162 - accuracy: 0.7442\n",
            "Epoch 186: val_loss improved from 0.54010 to 0.53989, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5173 - accuracy: 0.7443 - val_loss: 0.5399 - val_accuracy: 0.7396 - lr: 1.0000e-06\n",
            "Epoch 187/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.5115 - accuracy: 0.7385\n",
            "Epoch 187: val_loss improved from 0.53989 to 0.53966, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5121 - accuracy: 0.7375 - val_loss: 0.5397 - val_accuracy: 0.7376 - lr: 1.0000e-06\n",
            "Epoch 188/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.5187 - accuracy: 0.7443\n",
            "Epoch 188: val_loss improved from 0.53966 to 0.53951, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5169 - accuracy: 0.7457 - val_loss: 0.5395 - val_accuracy: 0.7396 - lr: 1.0000e-06\n",
            "Epoch 189/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.5192 - accuracy: 0.7445\n",
            "Epoch 189: val_loss improved from 0.53951 to 0.53937, saving model to model2.h5\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.5183 - accuracy: 0.7443 - val_loss: 0.5394 - val_accuracy: 0.7396 - lr: 1.0000e-06\n",
            "Epoch 190/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.5207 - accuracy: 0.7387\n",
            "Epoch 190: val_loss improved from 0.53937 to 0.53922, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5202 - accuracy: 0.7401 - val_loss: 0.5392 - val_accuracy: 0.7390 - lr: 1.0000e-06\n",
            "Epoch 191/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.5196 - accuracy: 0.7434\n",
            "Epoch 191: val_loss improved from 0.53922 to 0.53911, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5181 - accuracy: 0.7443 - val_loss: 0.5391 - val_accuracy: 0.7390 - lr: 1.0000e-06\n",
            "Epoch 192/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.5122 - accuracy: 0.7461\n",
            "Epoch 192: val_loss improved from 0.53911 to 0.53893, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5142 - accuracy: 0.7438 - val_loss: 0.5389 - val_accuracy: 0.7390 - lr: 1.0000e-06\n",
            "Epoch 193/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.5176 - accuracy: 0.7401\n",
            "Epoch 193: val_loss improved from 0.53893 to 0.53879, saving model to model2.h5\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.5178 - accuracy: 0.7396 - val_loss: 0.5388 - val_accuracy: 0.7396 - lr: 1.0000e-06\n",
            "Epoch 194/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.5158 - accuracy: 0.7416\n",
            "Epoch 194: val_loss improved from 0.53879 to 0.53866, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5167 - accuracy: 0.7420 - val_loss: 0.5387 - val_accuracy: 0.7396 - lr: 1.0000e-06\n",
            "Epoch 195/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.5156 - accuracy: 0.7435\n",
            "Epoch 195: val_loss improved from 0.53866 to 0.53846, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5136 - accuracy: 0.7462 - val_loss: 0.5385 - val_accuracy: 0.7403 - lr: 1.0000e-06\n",
            "Epoch 196/500\n",
            "173/184 [===========================>..] - ETA: 0s - loss: 0.5168 - accuracy: 0.7411\n",
            "Epoch 196: val_loss improved from 0.53846 to 0.53835, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5173 - accuracy: 0.7409 - val_loss: 0.5383 - val_accuracy: 0.7403 - lr: 1.0000e-06\n",
            "Epoch 197/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.5140 - accuracy: 0.7443\n",
            "Epoch 197: val_loss improved from 0.53835 to 0.53819, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 8ms/step - loss: 0.5140 - accuracy: 0.7443 - val_loss: 0.5382 - val_accuracy: 0.7403 - lr: 1.0000e-06\n",
            "Epoch 198/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.5142 - accuracy: 0.7415\n",
            "Epoch 198: val_loss improved from 0.53819 to 0.53805, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5140 - accuracy: 0.7416 - val_loss: 0.5381 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 199/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.5133 - accuracy: 0.7437\n",
            "Epoch 199: val_loss improved from 0.53805 to 0.53787, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5139 - accuracy: 0.7432 - val_loss: 0.5379 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 200/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.5132 - accuracy: 0.7415\n",
            "Epoch 200: val_loss improved from 0.53787 to 0.53775, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5135 - accuracy: 0.7413 - val_loss: 0.5377 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 201/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.5104 - accuracy: 0.7481\n",
            "Epoch 201: val_loss improved from 0.53775 to 0.53759, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5108 - accuracy: 0.7481 - val_loss: 0.5376 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 202/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.5110 - accuracy: 0.7483\n",
            "Epoch 202: val_loss improved from 0.53759 to 0.53743, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5102 - accuracy: 0.7483 - val_loss: 0.5374 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 203/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.5121 - accuracy: 0.7469\n",
            "Epoch 203: val_loss improved from 0.53743 to 0.53724, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5128 - accuracy: 0.7460 - val_loss: 0.5372 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 204/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.5120 - accuracy: 0.7437\n",
            "Epoch 204: val_loss improved from 0.53724 to 0.53710, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5117 - accuracy: 0.7440 - val_loss: 0.5371 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 205/500\n",
            "172/184 [===========================>..] - ETA: 0s - loss: 0.5147 - accuracy: 0.7396\n",
            "Epoch 205: val_loss improved from 0.53710 to 0.53692, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 8ms/step - loss: 0.5141 - accuracy: 0.7411 - val_loss: 0.5369 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 206/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.5134 - accuracy: 0.7427\n",
            "Epoch 206: val_loss improved from 0.53692 to 0.53678, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5122 - accuracy: 0.7437 - val_loss: 0.5368 - val_accuracy: 0.7396 - lr: 1.0000e-06\n",
            "Epoch 207/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.5129 - accuracy: 0.7471\n",
            "Epoch 207: val_loss improved from 0.53678 to 0.53663, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5142 - accuracy: 0.7464 - val_loss: 0.5366 - val_accuracy: 0.7403 - lr: 1.0000e-06\n",
            "Epoch 208/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.5100 - accuracy: 0.7437\n",
            "Epoch 208: val_loss improved from 0.53663 to 0.53649, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5099 - accuracy: 0.7438 - val_loss: 0.5365 - val_accuracy: 0.7396 - lr: 1.0000e-06\n",
            "Epoch 209/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.5128 - accuracy: 0.7428\n",
            "Epoch 209: val_loss improved from 0.53649 to 0.53638, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5121 - accuracy: 0.7435 - val_loss: 0.5364 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 210/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.5123 - accuracy: 0.7437\n",
            "Epoch 210: val_loss improved from 0.53638 to 0.53627, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5124 - accuracy: 0.7438 - val_loss: 0.5363 - val_accuracy: 0.7410 - lr: 1.0000e-06\n",
            "Epoch 211/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.5089 - accuracy: 0.7479\n",
            "Epoch 211: val_loss improved from 0.53627 to 0.53615, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5087 - accuracy: 0.7471 - val_loss: 0.5362 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 212/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.5113 - accuracy: 0.7462\n",
            "Epoch 212: val_loss improved from 0.53615 to 0.53604, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5109 - accuracy: 0.7474 - val_loss: 0.5360 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 213/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.5107 - accuracy: 0.7454\n",
            "Epoch 213: val_loss improved from 0.53604 to 0.53591, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5105 - accuracy: 0.7454 - val_loss: 0.5359 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 214/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.5148 - accuracy: 0.7464\n",
            "Epoch 214: val_loss improved from 0.53591 to 0.53583, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5149 - accuracy: 0.7466 - val_loss: 0.5358 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 215/500\n",
            "172/184 [===========================>..] - ETA: 0s - loss: 0.5106 - accuracy: 0.7442\n",
            "Epoch 215: val_loss improved from 0.53583 to 0.53567, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5099 - accuracy: 0.7450 - val_loss: 0.5357 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 216/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.5058 - accuracy: 0.7468\n",
            "Epoch 216: val_loss improved from 0.53567 to 0.53555, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5077 - accuracy: 0.7452 - val_loss: 0.5355 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 217/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.5087 - accuracy: 0.7475\n",
            "Epoch 217: val_loss improved from 0.53555 to 0.53550, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5066 - accuracy: 0.7483 - val_loss: 0.5355 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 218/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.5086 - accuracy: 0.7509\n",
            "Epoch 218: val_loss improved from 0.53550 to 0.53539, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5083 - accuracy: 0.7506 - val_loss: 0.5354 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 219/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.5098 - accuracy: 0.7451\n",
            "Epoch 219: val_loss improved from 0.53539 to 0.53525, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5092 - accuracy: 0.7459 - val_loss: 0.5353 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 220/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.5089 - accuracy: 0.7462\n",
            "Epoch 220: val_loss improved from 0.53525 to 0.53516, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5082 - accuracy: 0.7472 - val_loss: 0.5352 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 221/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.5073 - accuracy: 0.7457\n",
            "Epoch 221: val_loss improved from 0.53516 to 0.53502, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5074 - accuracy: 0.7460 - val_loss: 0.5350 - val_accuracy: 0.7430 - lr: 1.0000e-06\n",
            "Epoch 222/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.5102 - accuracy: 0.7383\n",
            "Epoch 222: val_loss improved from 0.53502 to 0.53494, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5097 - accuracy: 0.7379 - val_loss: 0.5349 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 223/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.5023 - accuracy: 0.7531\n",
            "Epoch 223: val_loss improved from 0.53494 to 0.53481, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5028 - accuracy: 0.7520 - val_loss: 0.5348 - val_accuracy: 0.7417 - lr: 1.0000e-06\n",
            "Epoch 224/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.5074 - accuracy: 0.7564\n",
            "Epoch 224: val_loss improved from 0.53481 to 0.53474, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5090 - accuracy: 0.7537 - val_loss: 0.5347 - val_accuracy: 0.7437 - lr: 1.0000e-06\n",
            "Epoch 225/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.5046 - accuracy: 0.7524\n",
            "Epoch 225: val_loss improved from 0.53474 to 0.53459, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 8ms/step - loss: 0.5056 - accuracy: 0.7522 - val_loss: 0.5346 - val_accuracy: 0.7437 - lr: 1.0000e-06\n",
            "Epoch 226/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.5067 - accuracy: 0.7528\n",
            "Epoch 226: val_loss improved from 0.53459 to 0.53446, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5064 - accuracy: 0.7527 - val_loss: 0.5345 - val_accuracy: 0.7437 - lr: 1.0000e-06\n",
            "Epoch 227/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.5059 - accuracy: 0.7519\n",
            "Epoch 227: val_loss improved from 0.53446 to 0.53436, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5056 - accuracy: 0.7515 - val_loss: 0.5344 - val_accuracy: 0.7437 - lr: 1.0000e-06\n",
            "Epoch 228/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.4995 - accuracy: 0.7523\n",
            "Epoch 228: val_loss improved from 0.53436 to 0.53418, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5003 - accuracy: 0.7517 - val_loss: 0.5342 - val_accuracy: 0.7430 - lr: 1.0000e-06\n",
            "Epoch 229/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.5088 - accuracy: 0.7481\n",
            "Epoch 229: val_loss improved from 0.53418 to 0.53407, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5071 - accuracy: 0.7493 - val_loss: 0.5341 - val_accuracy: 0.7430 - lr: 1.0000e-06\n",
            "Epoch 230/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.5004 - accuracy: 0.7463\n",
            "Epoch 230: val_loss improved from 0.53407 to 0.53394, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5027 - accuracy: 0.7454 - val_loss: 0.5339 - val_accuracy: 0.7430 - lr: 1.0000e-06\n",
            "Epoch 231/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.5020 - accuracy: 0.7514\n",
            "Epoch 231: val_loss improved from 0.53394 to 0.53380, saving model to model2.h5\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.5015 - accuracy: 0.7515 - val_loss: 0.5338 - val_accuracy: 0.7430 - lr: 1.0000e-06\n",
            "Epoch 232/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.5077 - accuracy: 0.7535\n",
            "Epoch 232: val_loss improved from 0.53380 to 0.53372, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5060 - accuracy: 0.7539 - val_loss: 0.5337 - val_accuracy: 0.7437 - lr: 1.0000e-06\n",
            "Epoch 233/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.5032 - accuracy: 0.7483\n",
            "Epoch 233: val_loss improved from 0.53372 to 0.53364, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 8ms/step - loss: 0.5034 - accuracy: 0.7483 - val_loss: 0.5336 - val_accuracy: 0.7437 - lr: 1.0000e-06\n",
            "Epoch 234/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.5017 - accuracy: 0.7536\n",
            "Epoch 234: val_loss improved from 0.53364 to 0.53352, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.5031 - accuracy: 0.7527 - val_loss: 0.5335 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 235/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.5034 - accuracy: 0.7521\n",
            "Epoch 235: val_loss improved from 0.53352 to 0.53343, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5065 - accuracy: 0.7501 - val_loss: 0.5334 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 236/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.5049 - accuracy: 0.7568\n",
            "Epoch 236: val_loss improved from 0.53343 to 0.53334, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5050 - accuracy: 0.7566 - val_loss: 0.5333 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 237/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.5048 - accuracy: 0.7568\n",
            "Epoch 237: val_loss improved from 0.53334 to 0.53325, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 8ms/step - loss: 0.5064 - accuracy: 0.7552 - val_loss: 0.5332 - val_accuracy: 0.7451 - lr: 1.0000e-06\n",
            "Epoch 238/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.5013 - accuracy: 0.7581\n",
            "Epoch 238: val_loss improved from 0.53325 to 0.53310, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5008 - accuracy: 0.7583 - val_loss: 0.5331 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 239/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.5060 - accuracy: 0.7452\n",
            "Epoch 239: val_loss improved from 0.53310 to 0.53305, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5059 - accuracy: 0.7450 - val_loss: 0.5330 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 240/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.5040 - accuracy: 0.7514\n",
            "Epoch 240: val_loss improved from 0.53305 to 0.53296, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5032 - accuracy: 0.7525 - val_loss: 0.5330 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 241/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.5002 - accuracy: 0.7517\n",
            "Epoch 241: val_loss improved from 0.53296 to 0.53285, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5002 - accuracy: 0.7517 - val_loss: 0.5329 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 242/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.4961 - accuracy: 0.7557\n",
            "Epoch 242: val_loss improved from 0.53285 to 0.53272, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4973 - accuracy: 0.7551 - val_loss: 0.5327 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 243/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4984 - accuracy: 0.7559\n",
            "Epoch 243: val_loss improved from 0.53272 to 0.53259, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4993 - accuracy: 0.7554 - val_loss: 0.5326 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 244/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.5029 - accuracy: 0.7514\n",
            "Epoch 244: val_loss improved from 0.53259 to 0.53251, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5009 - accuracy: 0.7525 - val_loss: 0.5325 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 245/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4985 - accuracy: 0.7585\n",
            "Epoch 245: val_loss improved from 0.53251 to 0.53235, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4983 - accuracy: 0.7585 - val_loss: 0.5324 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 246/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.4998 - accuracy: 0.7528\n",
            "Epoch 246: val_loss improved from 0.53235 to 0.53223, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4998 - accuracy: 0.7528 - val_loss: 0.5322 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 247/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.4980 - accuracy: 0.7553\n",
            "Epoch 247: val_loss improved from 0.53223 to 0.53212, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4973 - accuracy: 0.7552 - val_loss: 0.5321 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 248/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4982 - accuracy: 0.7527\n",
            "Epoch 248: val_loss improved from 0.53212 to 0.53203, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5000 - accuracy: 0.7513 - val_loss: 0.5320 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 249/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.5004 - accuracy: 0.7513\n",
            "Epoch 249: val_loss improved from 0.53203 to 0.53191, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5015 - accuracy: 0.7505 - val_loss: 0.5319 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 250/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4994 - accuracy: 0.7569\n",
            "Epoch 250: val_loss improved from 0.53191 to 0.53184, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4994 - accuracy: 0.7561 - val_loss: 0.5318 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 251/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.5001 - accuracy: 0.7553\n",
            "Epoch 251: val_loss improved from 0.53184 to 0.53175, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5001 - accuracy: 0.7552 - val_loss: 0.5318 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 252/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4987 - accuracy: 0.7564\n",
            "Epoch 252: val_loss improved from 0.53175 to 0.53166, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4973 - accuracy: 0.7573 - val_loss: 0.5317 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 253/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.4969 - accuracy: 0.7503\n",
            "Epoch 253: val_loss improved from 0.53166 to 0.53158, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4965 - accuracy: 0.7508 - val_loss: 0.5316 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 254/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.5022 - accuracy: 0.7514\n",
            "Epoch 254: val_loss improved from 0.53158 to 0.53150, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5016 - accuracy: 0.7508 - val_loss: 0.5315 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 255/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.4957 - accuracy: 0.7531\n",
            "Epoch 255: val_loss improved from 0.53150 to 0.53140, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4943 - accuracy: 0.7540 - val_loss: 0.5314 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 256/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.5007 - accuracy: 0.7483\n",
            "Epoch 256: val_loss improved from 0.53140 to 0.53135, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5003 - accuracy: 0.7488 - val_loss: 0.5313 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 257/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.5005 - accuracy: 0.7527\n",
            "Epoch 257: val_loss improved from 0.53135 to 0.53128, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5002 - accuracy: 0.7527 - val_loss: 0.5313 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 258/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.5023 - accuracy: 0.7529\n",
            "Epoch 258: val_loss improved from 0.53128 to 0.53120, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5025 - accuracy: 0.7527 - val_loss: 0.5312 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 259/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4954 - accuracy: 0.7532\n",
            "Epoch 259: val_loss improved from 0.53120 to 0.53114, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4954 - accuracy: 0.7539 - val_loss: 0.5311 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 260/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.4945 - accuracy: 0.7571\n",
            "Epoch 260: val_loss improved from 0.53114 to 0.53105, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4945 - accuracy: 0.7571 - val_loss: 0.5310 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 261/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.5030 - accuracy: 0.7541\n",
            "Epoch 261: val_loss improved from 0.53105 to 0.53098, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 8ms/step - loss: 0.5016 - accuracy: 0.7545 - val_loss: 0.5310 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 262/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.5003 - accuracy: 0.7568\n",
            "Epoch 262: val_loss improved from 0.53098 to 0.53090, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.5007 - accuracy: 0.7569 - val_loss: 0.5309 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 263/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4964 - accuracy: 0.7531\n",
            "Epoch 263: val_loss improved from 0.53090 to 0.53083, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4975 - accuracy: 0.7518 - val_loss: 0.5308 - val_accuracy: 0.7512 - lr: 1.0000e-06\n",
            "Epoch 264/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.4947 - accuracy: 0.7524\n",
            "Epoch 264: val_loss improved from 0.53083 to 0.53073, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4955 - accuracy: 0.7520 - val_loss: 0.5307 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 265/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.4951 - accuracy: 0.7558\n",
            "Epoch 265: val_loss improved from 0.53073 to 0.53070, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4956 - accuracy: 0.7559 - val_loss: 0.5307 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 266/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.4946 - accuracy: 0.7542\n",
            "Epoch 266: val_loss improved from 0.53070 to 0.53059, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4946 - accuracy: 0.7542 - val_loss: 0.5306 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 267/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4913 - accuracy: 0.7592\n",
            "Epoch 267: val_loss improved from 0.53059 to 0.53053, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4909 - accuracy: 0.7595 - val_loss: 0.5305 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 268/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.4979 - accuracy: 0.7538\n",
            "Epoch 268: val_loss improved from 0.53053 to 0.53047, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4966 - accuracy: 0.7545 - val_loss: 0.5305 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 269/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.4946 - accuracy: 0.7610\n",
            "Epoch 269: val_loss improved from 0.53047 to 0.53037, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4956 - accuracy: 0.7612 - val_loss: 0.5304 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 270/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.4943 - accuracy: 0.7544\n",
            "Epoch 270: val_loss improved from 0.53037 to 0.53031, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4930 - accuracy: 0.7564 - val_loss: 0.5303 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 271/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4964 - accuracy: 0.7570\n",
            "Epoch 271: val_loss improved from 0.53031 to 0.53026, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4950 - accuracy: 0.7573 - val_loss: 0.5303 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 272/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.4946 - accuracy: 0.7552\n",
            "Epoch 272: val_loss improved from 0.53026 to 0.53016, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4944 - accuracy: 0.7554 - val_loss: 0.5302 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 273/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.4966 - accuracy: 0.7531\n",
            "Epoch 273: val_loss improved from 0.53016 to 0.53011, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4963 - accuracy: 0.7540 - val_loss: 0.5301 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 274/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.4944 - accuracy: 0.7562\n",
            "Epoch 274: val_loss improved from 0.53011 to 0.53002, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4953 - accuracy: 0.7557 - val_loss: 0.5300 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 275/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.4936 - accuracy: 0.7598\n",
            "Epoch 275: val_loss improved from 0.53002 to 0.52991, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4940 - accuracy: 0.7591 - val_loss: 0.5299 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 276/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4889 - accuracy: 0.7635\n",
            "Epoch 276: val_loss improved from 0.52991 to 0.52982, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4912 - accuracy: 0.7617 - val_loss: 0.5298 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 277/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.4971 - accuracy: 0.7509\n",
            "Epoch 277: val_loss improved from 0.52982 to 0.52975, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4979 - accuracy: 0.7505 - val_loss: 0.5298 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 278/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4981 - accuracy: 0.7522\n",
            "Epoch 278: val_loss improved from 0.52975 to 0.52973, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4977 - accuracy: 0.7527 - val_loss: 0.5297 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 279/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.4927 - accuracy: 0.7572\n",
            "Epoch 279: val_loss improved from 0.52973 to 0.52966, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4928 - accuracy: 0.7568 - val_loss: 0.5297 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 280/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.4917 - accuracy: 0.7605\n",
            "Epoch 280: val_loss improved from 0.52966 to 0.52962, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4921 - accuracy: 0.7598 - val_loss: 0.5296 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 281/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4881 - accuracy: 0.7647\n",
            "Epoch 281: val_loss improved from 0.52962 to 0.52956, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4887 - accuracy: 0.7642 - val_loss: 0.5296 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 282/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.4907 - accuracy: 0.7571\n",
            "Epoch 282: val_loss improved from 0.52956 to 0.52950, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4919 - accuracy: 0.7561 - val_loss: 0.5295 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 283/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4875 - accuracy: 0.7667\n",
            "Epoch 283: val_loss improved from 0.52950 to 0.52942, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4880 - accuracy: 0.7663 - val_loss: 0.5294 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 284/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4870 - accuracy: 0.7678\n",
            "Epoch 284: val_loss improved from 0.52942 to 0.52930, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4873 - accuracy: 0.7676 - val_loss: 0.5293 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 285/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4853 - accuracy: 0.7650\n",
            "Epoch 285: val_loss improved from 0.52930 to 0.52924, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4877 - accuracy: 0.7637 - val_loss: 0.5292 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 286/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.4895 - accuracy: 0.7577\n",
            "Epoch 286: val_loss improved from 0.52924 to 0.52916, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4893 - accuracy: 0.7581 - val_loss: 0.5292 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 287/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4964 - accuracy: 0.7569\n",
            "Epoch 287: val_loss improved from 0.52916 to 0.52910, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4961 - accuracy: 0.7571 - val_loss: 0.5291 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 288/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4917 - accuracy: 0.7584\n",
            "Epoch 288: val_loss improved from 0.52910 to 0.52902, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4917 - accuracy: 0.7576 - val_loss: 0.5290 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 289/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4854 - accuracy: 0.7610\n",
            "Epoch 289: val_loss improved from 0.52902 to 0.52895, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4866 - accuracy: 0.7608 - val_loss: 0.5290 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 290/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.4895 - accuracy: 0.7620\n",
            "Epoch 290: val_loss improved from 0.52895 to 0.52889, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4914 - accuracy: 0.7600 - val_loss: 0.5289 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 291/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.4918 - accuracy: 0.7585\n",
            "Epoch 291: val_loss improved from 0.52889 to 0.52881, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4918 - accuracy: 0.7576 - val_loss: 0.5288 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 292/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4911 - accuracy: 0.7608\n",
            "Epoch 292: val_loss improved from 0.52881 to 0.52876, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4906 - accuracy: 0.7603 - val_loss: 0.5288 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 293/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.4921 - accuracy: 0.7543\n",
            "Epoch 293: val_loss improved from 0.52876 to 0.52870, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4924 - accuracy: 0.7545 - val_loss: 0.5287 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 294/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.4875 - accuracy: 0.7590\n",
            "Epoch 294: val_loss improved from 0.52870 to 0.52865, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4862 - accuracy: 0.7605 - val_loss: 0.5287 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 295/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4886 - accuracy: 0.7586\n",
            "Epoch 295: val_loss improved from 0.52865 to 0.52858, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4888 - accuracy: 0.7588 - val_loss: 0.5286 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 296/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.4869 - accuracy: 0.7655\n",
            "Epoch 296: val_loss improved from 0.52858 to 0.52855, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4891 - accuracy: 0.7630 - val_loss: 0.5285 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 297/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.4847 - accuracy: 0.7639\n",
            "Epoch 297: val_loss improved from 0.52855 to 0.52846, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4853 - accuracy: 0.7639 - val_loss: 0.5285 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 298/500\n",
            "173/184 [===========================>..] - ETA: 0s - loss: 0.4849 - accuracy: 0.7648\n",
            "Epoch 298: val_loss improved from 0.52846 to 0.52842, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4880 - accuracy: 0.7615 - val_loss: 0.5284 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 299/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.4839 - accuracy: 0.7686\n",
            "Epoch 299: val_loss improved from 0.52842 to 0.52834, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4851 - accuracy: 0.7676 - val_loss: 0.5283 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 300/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4870 - accuracy: 0.7617\n",
            "Epoch 300: val_loss improved from 0.52834 to 0.52829, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4874 - accuracy: 0.7603 - val_loss: 0.5283 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 301/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.4886 - accuracy: 0.7600\n",
            "Epoch 301: val_loss improved from 0.52829 to 0.52824, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4886 - accuracy: 0.7600 - val_loss: 0.5282 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 302/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4894 - accuracy: 0.7621\n",
            "Epoch 302: val_loss improved from 0.52824 to 0.52820, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4889 - accuracy: 0.7625 - val_loss: 0.5282 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 303/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4857 - accuracy: 0.7628\n",
            "Epoch 303: val_loss improved from 0.52820 to 0.52813, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4854 - accuracy: 0.7629 - val_loss: 0.5281 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 304/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.4857 - accuracy: 0.7616\n",
            "Epoch 304: val_loss improved from 0.52813 to 0.52810, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4873 - accuracy: 0.7612 - val_loss: 0.5281 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 305/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4899 - accuracy: 0.7577\n",
            "Epoch 305: val_loss improved from 0.52810 to 0.52804, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4877 - accuracy: 0.7590 - val_loss: 0.5280 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 306/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4873 - accuracy: 0.7618\n",
            "Epoch 306: val_loss improved from 0.52804 to 0.52798, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4870 - accuracy: 0.7619 - val_loss: 0.5280 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 307/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4869 - accuracy: 0.7605\n",
            "Epoch 307: val_loss improved from 0.52798 to 0.52792, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4859 - accuracy: 0.7613 - val_loss: 0.5279 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 308/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4886 - accuracy: 0.7642\n",
            "Epoch 308: val_loss improved from 0.52792 to 0.52785, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4864 - accuracy: 0.7661 - val_loss: 0.5279 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 309/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4827 - accuracy: 0.7613\n",
            "Epoch 309: val_loss improved from 0.52785 to 0.52781, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4826 - accuracy: 0.7613 - val_loss: 0.5278 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 310/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4874 - accuracy: 0.7594\n",
            "Epoch 310: val_loss improved from 0.52781 to 0.52777, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4868 - accuracy: 0.7596 - val_loss: 0.5278 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 311/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4860 - accuracy: 0.7620\n",
            "Epoch 311: val_loss improved from 0.52777 to 0.52771, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4858 - accuracy: 0.7629 - val_loss: 0.5277 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 312/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.4767 - accuracy: 0.7742\n",
            "Epoch 312: val_loss improved from 0.52771 to 0.52766, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4784 - accuracy: 0.7727 - val_loss: 0.5277 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 313/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4847 - accuracy: 0.7654\n",
            "Epoch 313: val_loss improved from 0.52766 to 0.52761, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4840 - accuracy: 0.7656 - val_loss: 0.5276 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 314/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.4855 - accuracy: 0.7627\n",
            "Epoch 314: val_loss improved from 0.52761 to 0.52757, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4855 - accuracy: 0.7627 - val_loss: 0.5276 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 315/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4854 - accuracy: 0.7627\n",
            "Epoch 315: val_loss improved from 0.52757 to 0.52751, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4844 - accuracy: 0.7634 - val_loss: 0.5275 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 316/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.4841 - accuracy: 0.7648\n",
            "Epoch 316: val_loss improved from 0.52751 to 0.52748, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4848 - accuracy: 0.7646 - val_loss: 0.5275 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 317/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.4850 - accuracy: 0.7595\n",
            "Epoch 317: val_loss improved from 0.52748 to 0.52742, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4852 - accuracy: 0.7596 - val_loss: 0.5274 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 318/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4829 - accuracy: 0.7587\n",
            "Epoch 318: val_loss improved from 0.52742 to 0.52734, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4838 - accuracy: 0.7588 - val_loss: 0.5273 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 319/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.4852 - accuracy: 0.7628\n",
            "Epoch 319: val_loss improved from 0.52734 to 0.52728, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4832 - accuracy: 0.7646 - val_loss: 0.5273 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 320/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.4826 - accuracy: 0.7600\n",
            "Epoch 320: val_loss improved from 0.52728 to 0.52721, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4826 - accuracy: 0.7600 - val_loss: 0.5272 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 321/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.4812 - accuracy: 0.7716\n",
            "Epoch 321: val_loss improved from 0.52721 to 0.52715, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4817 - accuracy: 0.7704 - val_loss: 0.5272 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 322/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4772 - accuracy: 0.7663\n",
            "Epoch 322: val_loss improved from 0.52715 to 0.52710, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4796 - accuracy: 0.7646 - val_loss: 0.5271 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 323/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.4849 - accuracy: 0.7656\n",
            "Epoch 323: val_loss improved from 0.52710 to 0.52703, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4844 - accuracy: 0.7658 - val_loss: 0.5270 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 324/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.4843 - accuracy: 0.7683\n",
            "Epoch 324: val_loss improved from 0.52703 to 0.52698, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4849 - accuracy: 0.7673 - val_loss: 0.5270 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 325/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4846 - accuracy: 0.7619\n",
            "Epoch 325: val_loss improved from 0.52698 to 0.52696, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 8ms/step - loss: 0.4841 - accuracy: 0.7624 - val_loss: 0.5270 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 326/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4809 - accuracy: 0.7609\n",
            "Epoch 326: val_loss improved from 0.52696 to 0.52690, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4809 - accuracy: 0.7610 - val_loss: 0.5269 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 327/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4816 - accuracy: 0.7700\n",
            "Epoch 327: val_loss improved from 0.52690 to 0.52686, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4798 - accuracy: 0.7714 - val_loss: 0.5269 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 328/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4808 - accuracy: 0.7690\n",
            "Epoch 328: val_loss improved from 0.52686 to 0.52682, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4796 - accuracy: 0.7697 - val_loss: 0.5268 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 329/500\n",
            "173/184 [===========================>..] - ETA: 0s - loss: 0.4811 - accuracy: 0.7598\n",
            "Epoch 329: val_loss improved from 0.52682 to 0.52676, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4815 - accuracy: 0.7607 - val_loss: 0.5268 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 330/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.4824 - accuracy: 0.7606\n",
            "Epoch 330: val_loss improved from 0.52676 to 0.52671, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4826 - accuracy: 0.7608 - val_loss: 0.5267 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 331/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4782 - accuracy: 0.7700\n",
            "Epoch 331: val_loss improved from 0.52671 to 0.52666, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4783 - accuracy: 0.7700 - val_loss: 0.5267 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 332/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.4844 - accuracy: 0.7623\n",
            "Epoch 332: val_loss did not improve from 0.52666\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4843 - accuracy: 0.7629 - val_loss: 0.5267 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 333/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.4823 - accuracy: 0.7707\n",
            "Epoch 333: val_loss improved from 0.52666 to 0.52658, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 8ms/step - loss: 0.4809 - accuracy: 0.7698 - val_loss: 0.5266 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 334/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.4806 - accuracy: 0.7649\n",
            "Epoch 334: val_loss improved from 0.52658 to 0.52653, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4820 - accuracy: 0.7654 - val_loss: 0.5265 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 335/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4808 - accuracy: 0.7698\n",
            "Epoch 335: val_loss improved from 0.52653 to 0.52650, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4802 - accuracy: 0.7702 - val_loss: 0.5265 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 336/500\n",
            "173/184 [===========================>..] - ETA: 0s - loss: 0.4800 - accuracy: 0.7670\n",
            "Epoch 336: val_loss improved from 0.52650 to 0.52645, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4785 - accuracy: 0.7680 - val_loss: 0.5265 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 337/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.4788 - accuracy: 0.7677\n",
            "Epoch 337: val_loss improved from 0.52645 to 0.52640, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4787 - accuracy: 0.7675 - val_loss: 0.5264 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 338/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.4797 - accuracy: 0.7664\n",
            "Epoch 338: val_loss did not improve from 0.52640\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4797 - accuracy: 0.7664 - val_loss: 0.5264 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 339/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.4783 - accuracy: 0.7705\n",
            "Epoch 339: val_loss improved from 0.52640 to 0.52637, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4791 - accuracy: 0.7687 - val_loss: 0.5264 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 340/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.4785 - accuracy: 0.7692\n",
            "Epoch 340: val_loss improved from 0.52637 to 0.52631, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4773 - accuracy: 0.7695 - val_loss: 0.5263 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 341/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4806 - accuracy: 0.7639\n",
            "Epoch 341: val_loss improved from 0.52631 to 0.52627, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 8ms/step - loss: 0.4811 - accuracy: 0.7639 - val_loss: 0.5263 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 342/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4778 - accuracy: 0.7621\n",
            "Epoch 342: val_loss improved from 0.52627 to 0.52624, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4779 - accuracy: 0.7627 - val_loss: 0.5262 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 343/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4771 - accuracy: 0.7679\n",
            "Epoch 343: val_loss improved from 0.52624 to 0.52622, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4769 - accuracy: 0.7680 - val_loss: 0.5262 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 344/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4756 - accuracy: 0.7701\n",
            "Epoch 344: val_loss improved from 0.52622 to 0.52619, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4761 - accuracy: 0.7693 - val_loss: 0.5262 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 345/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.4805 - accuracy: 0.7644\n",
            "Epoch 345: val_loss improved from 0.52619 to 0.52613, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4806 - accuracy: 0.7649 - val_loss: 0.5261 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 346/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4752 - accuracy: 0.7695\n",
            "Epoch 346: val_loss improved from 0.52613 to 0.52611, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4748 - accuracy: 0.7702 - val_loss: 0.5261 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 347/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.4749 - accuracy: 0.7680\n",
            "Epoch 347: val_loss improved from 0.52611 to 0.52606, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4749 - accuracy: 0.7680 - val_loss: 0.5261 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 348/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.4790 - accuracy: 0.7650\n",
            "Epoch 348: val_loss improved from 0.52606 to 0.52605, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4793 - accuracy: 0.7647 - val_loss: 0.5260 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 349/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.4765 - accuracy: 0.7660\n",
            "Epoch 349: val_loss improved from 0.52605 to 0.52602, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 8ms/step - loss: 0.4766 - accuracy: 0.7659 - val_loss: 0.5260 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 350/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.4763 - accuracy: 0.7692\n",
            "Epoch 350: val_loss improved from 0.52602 to 0.52598, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4767 - accuracy: 0.7690 - val_loss: 0.5260 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 351/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4750 - accuracy: 0.7662\n",
            "Epoch 351: val_loss improved from 0.52598 to 0.52592, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4778 - accuracy: 0.7632 - val_loss: 0.5259 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 352/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4715 - accuracy: 0.7698\n",
            "Epoch 352: val_loss improved from 0.52592 to 0.52589, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4717 - accuracy: 0.7695 - val_loss: 0.5259 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 353/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4788 - accuracy: 0.7715\n",
            "Epoch 353: val_loss improved from 0.52589 to 0.52583, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4789 - accuracy: 0.7712 - val_loss: 0.5258 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 354/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4728 - accuracy: 0.7658\n",
            "Epoch 354: val_loss improved from 0.52583 to 0.52578, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4729 - accuracy: 0.7658 - val_loss: 0.5258 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 355/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4758 - accuracy: 0.7697\n",
            "Epoch 355: val_loss improved from 0.52578 to 0.52575, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4766 - accuracy: 0.7697 - val_loss: 0.5257 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 356/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4755 - accuracy: 0.7662\n",
            "Epoch 356: val_loss improved from 0.52575 to 0.52573, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4749 - accuracy: 0.7663 - val_loss: 0.5257 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 357/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4781 - accuracy: 0.7649\n",
            "Epoch 357: val_loss improved from 0.52573 to 0.52570, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4772 - accuracy: 0.7668 - val_loss: 0.5257 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 358/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4755 - accuracy: 0.7676\n",
            "Epoch 358: val_loss improved from 0.52570 to 0.52563, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4742 - accuracy: 0.7683 - val_loss: 0.5256 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 359/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.4792 - accuracy: 0.7674\n",
            "Epoch 359: val_loss did not improve from 0.52563\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4783 - accuracy: 0.7678 - val_loss: 0.5256 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 360/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4752 - accuracy: 0.7687\n",
            "Epoch 360: val_loss improved from 0.52563 to 0.52563, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4755 - accuracy: 0.7688 - val_loss: 0.5256 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 361/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4732 - accuracy: 0.7740\n",
            "Epoch 361: val_loss improved from 0.52563 to 0.52556, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4745 - accuracy: 0.7731 - val_loss: 0.5256 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 362/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.4741 - accuracy: 0.7705\n",
            "Epoch 362: val_loss improved from 0.52556 to 0.52553, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4746 - accuracy: 0.7704 - val_loss: 0.5255 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 363/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.4769 - accuracy: 0.7698\n",
            "Epoch 363: val_loss improved from 0.52553 to 0.52550, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 8ms/step - loss: 0.4757 - accuracy: 0.7707 - val_loss: 0.5255 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 364/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.4701 - accuracy: 0.7730\n",
            "Epoch 364: val_loss improved from 0.52550 to 0.52547, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4709 - accuracy: 0.7724 - val_loss: 0.5255 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 365/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.4724 - accuracy: 0.7715\n",
            "Epoch 365: val_loss improved from 0.52547 to 0.52542, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4721 - accuracy: 0.7721 - val_loss: 0.5254 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 366/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.4765 - accuracy: 0.7752\n",
            "Epoch 366: val_loss improved from 0.52542 to 0.52539, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4761 - accuracy: 0.7756 - val_loss: 0.5254 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 367/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.4703 - accuracy: 0.7699\n",
            "Epoch 367: val_loss improved from 0.52539 to 0.52538, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4711 - accuracy: 0.7695 - val_loss: 0.5254 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 368/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4702 - accuracy: 0.7712\n",
            "Epoch 368: val_loss improved from 0.52538 to 0.52537, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4704 - accuracy: 0.7707 - val_loss: 0.5254 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 369/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4757 - accuracy: 0.7688\n",
            "Epoch 369: val_loss improved from 0.52537 to 0.52535, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4752 - accuracy: 0.7692 - val_loss: 0.5253 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 370/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4734 - accuracy: 0.7713\n",
            "Epoch 370: val_loss improved from 0.52535 to 0.52529, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4739 - accuracy: 0.7707 - val_loss: 0.5253 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 371/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.4712 - accuracy: 0.7732\n",
            "Epoch 371: val_loss improved from 0.52529 to 0.52525, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4701 - accuracy: 0.7739 - val_loss: 0.5252 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 372/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.4720 - accuracy: 0.7734\n",
            "Epoch 372: val_loss improved from 0.52525 to 0.52522, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4719 - accuracy: 0.7732 - val_loss: 0.5252 - val_accuracy: 0.7512 - lr: 1.0000e-06\n",
            "Epoch 373/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4669 - accuracy: 0.7739\n",
            "Epoch 373: val_loss improved from 0.52522 to 0.52519, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4686 - accuracy: 0.7721 - val_loss: 0.5252 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 374/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4746 - accuracy: 0.7685\n",
            "Epoch 374: val_loss did not improve from 0.52519\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4743 - accuracy: 0.7690 - val_loss: 0.5252 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 375/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.4663 - accuracy: 0.7751\n",
            "Epoch 375: val_loss did not improve from 0.52519\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4677 - accuracy: 0.7738 - val_loss: 0.5252 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 376/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4722 - accuracy: 0.7713\n",
            "Epoch 376: val_loss did not improve from 0.52519\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4702 - accuracy: 0.7738 - val_loss: 0.5252 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 377/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4705 - accuracy: 0.7715\n",
            "Epoch 377: val_loss improved from 0.52519 to 0.52515, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4705 - accuracy: 0.7714 - val_loss: 0.5252 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 378/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.4722 - accuracy: 0.7714\n",
            "Epoch 378: val_loss improved from 0.52515 to 0.52513, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 8ms/step - loss: 0.4722 - accuracy: 0.7714 - val_loss: 0.5251 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 379/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.4700 - accuracy: 0.7732\n",
            "Epoch 379: val_loss improved from 0.52513 to 0.52504, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4691 - accuracy: 0.7753 - val_loss: 0.5250 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 380/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4691 - accuracy: 0.7739\n",
            "Epoch 380: val_loss improved from 0.52504 to 0.52498, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4694 - accuracy: 0.7736 - val_loss: 0.5250 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 381/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.4664 - accuracy: 0.7745\n",
            "Epoch 381: val_loss improved from 0.52498 to 0.52498, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4671 - accuracy: 0.7738 - val_loss: 0.5250 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 382/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4693 - accuracy: 0.7731\n",
            "Epoch 382: val_loss improved from 0.52498 to 0.52497, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4690 - accuracy: 0.7732 - val_loss: 0.5250 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 383/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4699 - accuracy: 0.7688\n",
            "Epoch 383: val_loss did not improve from 0.52497\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4686 - accuracy: 0.7692 - val_loss: 0.5250 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 384/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4722 - accuracy: 0.7722\n",
            "Epoch 384: val_loss improved from 0.52497 to 0.52495, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4710 - accuracy: 0.7729 - val_loss: 0.5250 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 385/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4701 - accuracy: 0.7729\n",
            "Epoch 385: val_loss did not improve from 0.52495\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4697 - accuracy: 0.7715 - val_loss: 0.5250 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 386/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4721 - accuracy: 0.7678\n",
            "Epoch 386: val_loss did not improve from 0.52495\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4717 - accuracy: 0.7681 - val_loss: 0.5250 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 387/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4713 - accuracy: 0.7769\n",
            "Epoch 387: val_loss improved from 0.52495 to 0.52493, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4695 - accuracy: 0.7773 - val_loss: 0.5249 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 388/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.4718 - accuracy: 0.7677\n",
            "Epoch 388: val_loss improved from 0.52493 to 0.52488, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4733 - accuracy: 0.7675 - val_loss: 0.5249 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 389/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4676 - accuracy: 0.7725\n",
            "Epoch 389: val_loss improved from 0.52488 to 0.52487, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4696 - accuracy: 0.7698 - val_loss: 0.5249 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 390/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4655 - accuracy: 0.7748\n",
            "Epoch 390: val_loss improved from 0.52487 to 0.52485, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4648 - accuracy: 0.7751 - val_loss: 0.5249 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 391/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.4636 - accuracy: 0.7771\n",
            "Epoch 391: val_loss improved from 0.52485 to 0.52484, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4642 - accuracy: 0.7768 - val_loss: 0.5248 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 392/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4662 - accuracy: 0.7732\n",
            "Epoch 392: val_loss improved from 0.52484 to 0.52479, saving model to model2.h5\n",
            "184/184 [==============================] - 2s 9ms/step - loss: 0.4670 - accuracy: 0.7732 - val_loss: 0.5248 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 393/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.4692 - accuracy: 0.7712\n",
            "Epoch 393: val_loss did not improve from 0.52479\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4671 - accuracy: 0.7729 - val_loss: 0.5248 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 394/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4679 - accuracy: 0.7736\n",
            "Epoch 394: val_loss did not improve from 0.52479\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4665 - accuracy: 0.7751 - val_loss: 0.5248 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 395/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4661 - accuracy: 0.7790\n",
            "Epoch 395: val_loss did not improve from 0.52479\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4656 - accuracy: 0.7790 - val_loss: 0.5248 - val_accuracy: 0.7519 - lr: 1.0000e-06\n",
            "Epoch 396/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4703 - accuracy: 0.7683\n",
            "Epoch 396: val_loss improved from 0.52479 to 0.52478, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4699 - accuracy: 0.7690 - val_loss: 0.5248 - val_accuracy: 0.7512 - lr: 1.0000e-06\n",
            "Epoch 397/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4698 - accuracy: 0.7708\n",
            "Epoch 397: val_loss improved from 0.52478 to 0.52474, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4696 - accuracy: 0.7710 - val_loss: 0.5247 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 398/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.4723 - accuracy: 0.7714\n",
            "Epoch 398: val_loss did not improve from 0.52474\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4714 - accuracy: 0.7717 - val_loss: 0.5247 - val_accuracy: 0.7519 - lr: 1.0000e-06\n",
            "Epoch 399/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4653 - accuracy: 0.7735\n",
            "Epoch 399: val_loss improved from 0.52474 to 0.52473, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4666 - accuracy: 0.7719 - val_loss: 0.5247 - val_accuracy: 0.7519 - lr: 1.0000e-06\n",
            "Epoch 400/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4681 - accuracy: 0.7734\n",
            "Epoch 400: val_loss did not improve from 0.52473\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4672 - accuracy: 0.7731 - val_loss: 0.5247 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 401/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4665 - accuracy: 0.7788\n",
            "Epoch 401: val_loss improved from 0.52473 to 0.52470, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4662 - accuracy: 0.7790 - val_loss: 0.5247 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 402/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.4631 - accuracy: 0.7747\n",
            "Epoch 402: val_loss improved from 0.52470 to 0.52468, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4648 - accuracy: 0.7732 - val_loss: 0.5247 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 403/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.4674 - accuracy: 0.7705\n",
            "Epoch 403: val_loss improved from 0.52468 to 0.52466, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4675 - accuracy: 0.7705 - val_loss: 0.5247 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 404/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4679 - accuracy: 0.7741\n",
            "Epoch 404: val_loss improved from 0.52466 to 0.52466, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4681 - accuracy: 0.7738 - val_loss: 0.5247 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 405/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4664 - accuracy: 0.7748\n",
            "Epoch 405: val_loss improved from 0.52466 to 0.52461, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4659 - accuracy: 0.7749 - val_loss: 0.5246 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 406/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.4678 - accuracy: 0.7715\n",
            "Epoch 406: val_loss improved from 0.52461 to 0.52461, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4689 - accuracy: 0.7704 - val_loss: 0.5246 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 407/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4616 - accuracy: 0.7737\n",
            "Epoch 407: val_loss did not improve from 0.52461\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4614 - accuracy: 0.7741 - val_loss: 0.5246 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 408/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.4662 - accuracy: 0.7738\n",
            "Epoch 408: val_loss did not improve from 0.52461\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4662 - accuracy: 0.7738 - val_loss: 0.5246 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 409/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4615 - accuracy: 0.7766\n",
            "Epoch 409: val_loss improved from 0.52461 to 0.52455, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4611 - accuracy: 0.7770 - val_loss: 0.5246 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 410/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4660 - accuracy: 0.7727\n",
            "Epoch 410: val_loss did not improve from 0.52455\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4661 - accuracy: 0.7736 - val_loss: 0.5246 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 411/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.4623 - accuracy: 0.7788\n",
            "Epoch 411: val_loss improved from 0.52455 to 0.52453, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4630 - accuracy: 0.7772 - val_loss: 0.5245 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 412/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4703 - accuracy: 0.7717\n",
            "Epoch 412: val_loss improved from 0.52453 to 0.52448, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4711 - accuracy: 0.7710 - val_loss: 0.5245 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 413/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.4605 - accuracy: 0.7790\n",
            "Epoch 413: val_loss improved from 0.52448 to 0.52444, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4622 - accuracy: 0.7783 - val_loss: 0.5244 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 414/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.4624 - accuracy: 0.7721\n",
            "Epoch 414: val_loss improved from 0.52444 to 0.52441, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4654 - accuracy: 0.7700 - val_loss: 0.5244 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 415/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.4629 - accuracy: 0.7773\n",
            "Epoch 415: val_loss improved from 0.52441 to 0.52440, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4630 - accuracy: 0.7770 - val_loss: 0.5244 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 416/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4657 - accuracy: 0.7710\n",
            "Epoch 416: val_loss improved from 0.52440 to 0.52435, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4678 - accuracy: 0.7695 - val_loss: 0.5243 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 417/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4660 - accuracy: 0.7703\n",
            "Epoch 417: val_loss improved from 0.52435 to 0.52433, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4658 - accuracy: 0.7705 - val_loss: 0.5243 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 418/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.4621 - accuracy: 0.7802\n",
            "Epoch 418: val_loss did not improve from 0.52433\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4621 - accuracy: 0.7802 - val_loss: 0.5244 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 419/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.4626 - accuracy: 0.7775\n",
            "Epoch 419: val_loss did not improve from 0.52433\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4603 - accuracy: 0.7794 - val_loss: 0.5244 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 420/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4639 - accuracy: 0.7724\n",
            "Epoch 420: val_loss did not improve from 0.52433\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4644 - accuracy: 0.7721 - val_loss: 0.5243 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 421/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4619 - accuracy: 0.7789\n",
            "Epoch 421: val_loss improved from 0.52433 to 0.52432, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4622 - accuracy: 0.7789 - val_loss: 0.5243 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 422/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4619 - accuracy: 0.7756\n",
            "Epoch 422: val_loss improved from 0.52432 to 0.52426, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4619 - accuracy: 0.7756 - val_loss: 0.5243 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 423/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.4658 - accuracy: 0.7769\n",
            "Epoch 423: val_loss improved from 0.52426 to 0.52424, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4661 - accuracy: 0.7766 - val_loss: 0.5242 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 424/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.4623 - accuracy: 0.7735\n",
            "Epoch 424: val_loss improved from 0.52424 to 0.52422, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4618 - accuracy: 0.7738 - val_loss: 0.5242 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 425/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.4647 - accuracy: 0.7749\n",
            "Epoch 425: val_loss did not improve from 0.52422\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4639 - accuracy: 0.7761 - val_loss: 0.5242 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 426/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4634 - accuracy: 0.7823\n",
            "Epoch 426: val_loss improved from 0.52422 to 0.52419, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4642 - accuracy: 0.7807 - val_loss: 0.5242 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 427/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.4659 - accuracy: 0.7714\n",
            "Epoch 427: val_loss improved from 0.52419 to 0.52419, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4656 - accuracy: 0.7727 - val_loss: 0.5242 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 428/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.4605 - accuracy: 0.7782\n",
            "Epoch 428: val_loss improved from 0.52419 to 0.52417, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4605 - accuracy: 0.7782 - val_loss: 0.5242 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 429/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4629 - accuracy: 0.7779\n",
            "Epoch 429: val_loss did not improve from 0.52417\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4625 - accuracy: 0.7775 - val_loss: 0.5242 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 430/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.4663 - accuracy: 0.7749\n",
            "Epoch 430: val_loss improved from 0.52417 to 0.52415, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 8ms/step - loss: 0.4654 - accuracy: 0.7756 - val_loss: 0.5241 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 431/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4586 - accuracy: 0.7787\n",
            "Epoch 431: val_loss did not improve from 0.52415\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4585 - accuracy: 0.7785 - val_loss: 0.5242 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 432/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4613 - accuracy: 0.7811\n",
            "Epoch 432: val_loss improved from 0.52415 to 0.52412, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4626 - accuracy: 0.7797 - val_loss: 0.5241 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 433/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4612 - accuracy: 0.7755\n",
            "Epoch 433: val_loss did not improve from 0.52412\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4613 - accuracy: 0.7760 - val_loss: 0.5241 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 434/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.4624 - accuracy: 0.7758\n",
            "Epoch 434: val_loss improved from 0.52412 to 0.52410, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 8ms/step - loss: 0.4624 - accuracy: 0.7758 - val_loss: 0.5241 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 435/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4652 - accuracy: 0.7734\n",
            "Epoch 435: val_loss did not improve from 0.52410\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4626 - accuracy: 0.7744 - val_loss: 0.5241 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 436/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.4647 - accuracy: 0.7740\n",
            "Epoch 436: val_loss did not improve from 0.52410\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4638 - accuracy: 0.7744 - val_loss: 0.5241 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 437/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4565 - accuracy: 0.7821\n",
            "Epoch 437: val_loss improved from 0.52410 to 0.52407, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4561 - accuracy: 0.7823 - val_loss: 0.5241 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 438/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.4577 - accuracy: 0.7848\n",
            "Epoch 438: val_loss improved from 0.52407 to 0.52400, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 8ms/step - loss: 0.4577 - accuracy: 0.7848 - val_loss: 0.5240 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 439/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4612 - accuracy: 0.7786\n",
            "Epoch 439: val_loss did not improve from 0.52400\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4608 - accuracy: 0.7792 - val_loss: 0.5240 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 440/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4559 - accuracy: 0.7777\n",
            "Epoch 440: val_loss did not improve from 0.52400\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4570 - accuracy: 0.7768 - val_loss: 0.5240 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 441/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4601 - accuracy: 0.7795\n",
            "Epoch 441: val_loss did not improve from 0.52400\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4593 - accuracy: 0.7807 - val_loss: 0.5240 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 442/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.4580 - accuracy: 0.7778\n",
            "Epoch 442: val_loss did not improve from 0.52400\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4590 - accuracy: 0.7772 - val_loss: 0.5240 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 443/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.4575 - accuracy: 0.7828\n",
            "Epoch 443: val_loss improved from 0.52400 to 0.52399, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4581 - accuracy: 0.7823 - val_loss: 0.5240 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 444/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.4579 - accuracy: 0.7786\n",
            "Epoch 444: val_loss did not improve from 0.52399\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4573 - accuracy: 0.7792 - val_loss: 0.5240 - val_accuracy: 0.7458 - lr: 1.0000e-06\n",
            "Epoch 445/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.4588 - accuracy: 0.7793\n",
            "Epoch 445: val_loss improved from 0.52399 to 0.52398, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4577 - accuracy: 0.7799 - val_loss: 0.5240 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 446/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4562 - accuracy: 0.7797\n",
            "Epoch 446: val_loss improved from 0.52398 to 0.52396, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4560 - accuracy: 0.7795 - val_loss: 0.5240 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 447/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.4586 - accuracy: 0.7761\n",
            "Epoch 447: val_loss improved from 0.52396 to 0.52395, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4585 - accuracy: 0.7770 - val_loss: 0.5240 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 448/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.4564 - accuracy: 0.7828\n",
            "Epoch 448: val_loss improved from 0.52395 to 0.52391, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4574 - accuracy: 0.7824 - val_loss: 0.5239 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 449/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.4557 - accuracy: 0.7771\n",
            "Epoch 449: val_loss did not improve from 0.52391\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4547 - accuracy: 0.7782 - val_loss: 0.5239 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 450/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.4579 - accuracy: 0.7765\n",
            "Epoch 450: val_loss improved from 0.52391 to 0.52390, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4579 - accuracy: 0.7765 - val_loss: 0.5239 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 451/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.4525 - accuracy: 0.7792\n",
            "Epoch 451: val_loss improved from 0.52390 to 0.52387, saving model to model2.h5\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.4514 - accuracy: 0.7799 - val_loss: 0.5239 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 452/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4585 - accuracy: 0.7797\n",
            "Epoch 452: val_loss did not improve from 0.52387\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4561 - accuracy: 0.7816 - val_loss: 0.5239 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 453/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4609 - accuracy: 0.7772\n",
            "Epoch 453: val_loss did not improve from 0.52387\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4611 - accuracy: 0.7780 - val_loss: 0.5239 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 454/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.4559 - accuracy: 0.7782\n",
            "Epoch 454: val_loss improved from 0.52387 to 0.52383, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4559 - accuracy: 0.7782 - val_loss: 0.5238 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 455/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4584 - accuracy: 0.7745\n",
            "Epoch 455: val_loss did not improve from 0.52383\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4554 - accuracy: 0.7763 - val_loss: 0.5238 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 456/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.4609 - accuracy: 0.7752\n",
            "Epoch 456: val_loss improved from 0.52383 to 0.52382, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4592 - accuracy: 0.7758 - val_loss: 0.5238 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 457/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4551 - accuracy: 0.7811\n",
            "Epoch 457: val_loss did not improve from 0.52382\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4564 - accuracy: 0.7797 - val_loss: 0.5239 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 458/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.4578 - accuracy: 0.7790\n",
            "Epoch 458: val_loss did not improve from 0.52382\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4571 - accuracy: 0.7797 - val_loss: 0.5238 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 459/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4542 - accuracy: 0.7788\n",
            "Epoch 459: val_loss improved from 0.52382 to 0.52381, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4555 - accuracy: 0.7783 - val_loss: 0.5238 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 460/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4559 - accuracy: 0.7840\n",
            "Epoch 460: val_loss improved from 0.52381 to 0.52379, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 8ms/step - loss: 0.4572 - accuracy: 0.7823 - val_loss: 0.5238 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 461/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4536 - accuracy: 0.7830\n",
            "Epoch 461: val_loss improved from 0.52379 to 0.52377, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4548 - accuracy: 0.7834 - val_loss: 0.5238 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 462/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4566 - accuracy: 0.7853\n",
            "Epoch 462: val_loss improved from 0.52377 to 0.52376, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 8ms/step - loss: 0.4565 - accuracy: 0.7855 - val_loss: 0.5238 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 463/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4544 - accuracy: 0.7840\n",
            "Epoch 463: val_loss did not improve from 0.52376\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4530 - accuracy: 0.7860 - val_loss: 0.5238 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 464/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.4569 - accuracy: 0.7830\n",
            "Epoch 464: val_loss improved from 0.52376 to 0.52375, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4567 - accuracy: 0.7823 - val_loss: 0.5238 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 465/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.4561 - accuracy: 0.7775\n",
            "Epoch 465: val_loss did not improve from 0.52375\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4567 - accuracy: 0.7772 - val_loss: 0.5238 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 466/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4524 - accuracy: 0.7841\n",
            "Epoch 466: val_loss improved from 0.52375 to 0.52369, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4537 - accuracy: 0.7828 - val_loss: 0.5237 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 467/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.4521 - accuracy: 0.7811\n",
            "Epoch 467: val_loss did not improve from 0.52369\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4521 - accuracy: 0.7809 - val_loss: 0.5238 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 468/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.4536 - accuracy: 0.7811\n",
            "Epoch 468: val_loss did not improve from 0.52369\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4532 - accuracy: 0.7816 - val_loss: 0.5238 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 469/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.4498 - accuracy: 0.7864\n",
            "Epoch 469: val_loss did not improve from 0.52369\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4501 - accuracy: 0.7875 - val_loss: 0.5238 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 470/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.4479 - accuracy: 0.7876\n",
            "Epoch 470: val_loss did not improve from 0.52369\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4485 - accuracy: 0.7870 - val_loss: 0.5237 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 471/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.4554 - accuracy: 0.7778\n",
            "Epoch 471: val_loss did not improve from 0.52369\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4554 - accuracy: 0.7778 - val_loss: 0.5238 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 472/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.4556 - accuracy: 0.7792\n",
            "Epoch 472: val_loss did not improve from 0.52369\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4543 - accuracy: 0.7800 - val_loss: 0.5237 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 473/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4532 - accuracy: 0.7782\n",
            "Epoch 473: val_loss improved from 0.52369 to 0.52369, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 8ms/step - loss: 0.4522 - accuracy: 0.7787 - val_loss: 0.5237 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 474/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4557 - accuracy: 0.7819\n",
            "Epoch 474: val_loss did not improve from 0.52369\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4560 - accuracy: 0.7812 - val_loss: 0.5237 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 475/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.4523 - accuracy: 0.7820\n",
            "Epoch 475: val_loss did not improve from 0.52369\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4519 - accuracy: 0.7826 - val_loss: 0.5237 - val_accuracy: 0.7485 - lr: 1.0000e-06\n",
            "Epoch 476/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4549 - accuracy: 0.7825\n",
            "Epoch 476: val_loss improved from 0.52369 to 0.52366, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 8ms/step - loss: 0.4535 - accuracy: 0.7840 - val_loss: 0.5237 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 477/500\n",
            "179/184 [============================>.] - ETA: 0s - loss: 0.4518 - accuracy: 0.7806\n",
            "Epoch 477: val_loss did not improve from 0.52366\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4514 - accuracy: 0.7812 - val_loss: 0.5237 - val_accuracy: 0.7464 - lr: 1.0000e-06\n",
            "Epoch 478/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4497 - accuracy: 0.7821\n",
            "Epoch 478: val_loss improved from 0.52366 to 0.52365, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4520 - accuracy: 0.7812 - val_loss: 0.5237 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 479/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.4552 - accuracy: 0.7820\n",
            "Epoch 479: val_loss improved from 0.52365 to 0.52363, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 8ms/step - loss: 0.4565 - accuracy: 0.7811 - val_loss: 0.5236 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 480/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.4514 - accuracy: 0.7819\n",
            "Epoch 480: val_loss improved from 0.52363 to 0.52363, saving model to model2.h5\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.4514 - accuracy: 0.7819 - val_loss: 0.5236 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 481/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4519 - accuracy: 0.7820\n",
            "Epoch 481: val_loss improved from 0.52363 to 0.52360, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4545 - accuracy: 0.7809 - val_loss: 0.5236 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 482/500\n",
            "177/184 [===========================>..] - ETA: 0s - loss: 0.4521 - accuracy: 0.7811\n",
            "Epoch 482: val_loss improved from 0.52360 to 0.52356, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4509 - accuracy: 0.7817 - val_loss: 0.5236 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 483/500\n",
            "175/184 [===========================>..] - ETA: 0s - loss: 0.4528 - accuracy: 0.7841\n",
            "Epoch 483: val_loss improved from 0.52356 to 0.52355, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4511 - accuracy: 0.7851 - val_loss: 0.5236 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 484/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4491 - accuracy: 0.7826\n",
            "Epoch 484: val_loss improved from 0.52355 to 0.52355, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4490 - accuracy: 0.7829 - val_loss: 0.5235 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 485/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.4481 - accuracy: 0.7857\n",
            "Epoch 485: val_loss did not improve from 0.52355\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4519 - accuracy: 0.7833 - val_loss: 0.5236 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 486/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.4526 - accuracy: 0.7783\n",
            "Epoch 486: val_loss did not improve from 0.52355\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4529 - accuracy: 0.7778 - val_loss: 0.5236 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 487/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4517 - accuracy: 0.7819\n",
            "Epoch 487: val_loss did not improve from 0.52355\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4518 - accuracy: 0.7821 - val_loss: 0.5236 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 488/500\n",
            "182/184 [============================>.] - ETA: 0s - loss: 0.4509 - accuracy: 0.7837\n",
            "Epoch 488: val_loss improved from 0.52355 to 0.52352, saving model to model2.h5\n",
            "184/184 [==============================] - 2s 8ms/step - loss: 0.4511 - accuracy: 0.7834 - val_loss: 0.5235 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 489/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4532 - accuracy: 0.7869\n",
            "Epoch 489: val_loss improved from 0.52352 to 0.52351, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4531 - accuracy: 0.7868 - val_loss: 0.5235 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 490/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.4469 - accuracy: 0.7861\n",
            "Epoch 490: val_loss did not improve from 0.52351\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4467 - accuracy: 0.7851 - val_loss: 0.5235 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 491/500\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.4503 - accuracy: 0.7838\n",
            "Epoch 491: val_loss did not improve from 0.52351\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4503 - accuracy: 0.7838 - val_loss: 0.5235 - val_accuracy: 0.7492 - lr: 1.0000e-06\n",
            "Epoch 492/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.4471 - accuracy: 0.7852\n",
            "Epoch 492: val_loss did not improve from 0.52351\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4478 - accuracy: 0.7845 - val_loss: 0.5235 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 493/500\n",
            "174/184 [===========================>..] - ETA: 0s - loss: 0.4491 - accuracy: 0.7875\n",
            "Epoch 493: val_loss did not improve from 0.52351\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4475 - accuracy: 0.7894 - val_loss: 0.5236 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 494/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4501 - accuracy: 0.7809\n",
            "Epoch 494: val_loss did not improve from 0.52351\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4506 - accuracy: 0.7814 - val_loss: 0.5236 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 495/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.4537 - accuracy: 0.7807\n",
            "Epoch 495: val_loss did not improve from 0.52351\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4531 - accuracy: 0.7816 - val_loss: 0.5235 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 496/500\n",
            "180/184 [============================>.] - ETA: 0s - loss: 0.4540 - accuracy: 0.7774\n",
            "Epoch 496: val_loss did not improve from 0.52351\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4551 - accuracy: 0.7765 - val_loss: 0.5235 - val_accuracy: 0.7505 - lr: 1.0000e-06\n",
            "Epoch 497/500\n",
            "183/184 [============================>.] - ETA: 0s - loss: 0.4481 - accuracy: 0.7847\n",
            "Epoch 497: val_loss did not improve from 0.52351\n",
            "184/184 [==============================] - 1s 6ms/step - loss: 0.4481 - accuracy: 0.7843 - val_loss: 0.5235 - val_accuracy: 0.7498 - lr: 1.0000e-06\n",
            "Epoch 498/500\n",
            "181/184 [============================>.] - ETA: 0s - loss: 0.4518 - accuracy: 0.7785\n",
            "Epoch 498: val_loss improved from 0.52351 to 0.52350, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4507 - accuracy: 0.7789 - val_loss: 0.5235 - val_accuracy: 0.7478 - lr: 1.0000e-06\n",
            "Epoch 499/500\n",
            "178/184 [============================>.] - ETA: 0s - loss: 0.4450 - accuracy: 0.7886\n",
            "Epoch 499: val_loss improved from 0.52350 to 0.52348, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4455 - accuracy: 0.7885 - val_loss: 0.5235 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 500/500\n",
            "176/184 [===========================>..] - ETA: 0s - loss: 0.4468 - accuracy: 0.7862\n",
            "Epoch 500: val_loss improved from 0.52348 to 0.52348, saving model to model2.h5\n",
            "184/184 [==============================] - 1s 7ms/step - loss: 0.4499 - accuracy: 0.7840 - val_loss: 0.5235 - val_accuracy: 0.7478 - lr: 1.0000e-06\n"
          ]
        }
      ],
      "source": [
        "hist2 = model2.fit([train_X, train_X, train_X], train_Y, epochs = EPOCHS, batch_size = BS, validation_data = ([val_X, val_X, val_X], val_Y), callbacks = [callbacks_list], shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OhgORxfp5UF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "cecc571d-72e6-4e76-9d8c-ec47283dad1e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"66ab509d-f1c1-403f-b263-0d619722e1af\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"66ab509d-f1c1-403f-b263-0d619722e1af\")) {                    Plotly.newPlot(                        \"66ab509d-f1c1-403f-b263-0d619722e1af\",                        [{\"line\":{\"color\":\"blue\",\"width\":2},\"mode\":\"lines\",\"name\":\"loss\",\"y\":[0.7280182242393494,0.7118734121322632,0.7093455195426941,0.706279456615448,0.7011122703552246,0.7009074687957764,0.6905994415283203,0.6897898316383362,0.6909196376800537,0.6873936653137207,0.6831287741661072,0.679463803768158,0.6789326667785645,0.6768189072608948,0.6747283935546875,0.673538863658905,0.6697832942008972,0.6677147150039673,0.6687344312667847,0.6701176762580872,0.6651416420936584,0.6630226373672485,0.6572300791740417,0.6613984704017639,0.6564667224884033,0.6579626202583313,0.6518834233283997,0.651366651058197,0.650614857673645,0.6499622464179993,0.6495146155357361,0.6477398872375488,0.6467743515968323,0.6410558223724365,0.6389986276626587,0.6388456225395203,0.6355320811271667,0.6373565793037415,0.6338775157928467,0.6311661601066589,0.6323053240776062,0.6292369365692139,0.6279246211051941,0.6264455318450928,0.627324104309082,0.6241987347602844,0.6235142946243286,0.6242524981498718,0.6203170418739319,0.6157781481742859,0.6200997829437256,0.6171486973762512,0.6144979000091553,0.613104522228241,0.6133809685707092,0.6105302572250366,0.6072655320167542,0.6100078821182251,0.6025831699371338,0.6029617190361023,0.599689245223999,0.6057156920433044,0.5975019335746765,0.5994483828544617,0.5986892580986023,0.5974999070167542,0.5904401540756226,0.5912525653839111,0.593812108039856,0.5883635878562927,0.5913541316986084,0.5865763425827026,0.5873209238052368,0.5874097943305969,0.5857008695602417,0.5784892439842224,0.5875816345214844,0.580723762512207,0.5804283022880554,0.574645459651947,0.5757336616516113,0.5777240991592407,0.5773691534996033,0.5747485756874084,0.5706160664558411,0.5729972720146179,0.5717359185218811,0.5706475973129272,0.5700185298919678,0.571713387966156,0.5670097470283508,0.5673736929893494,0.5665574073791504,0.5639408826828003,0.5656594038009644,0.5612136125564575,0.5638406276702881,0.5633292198181152,0.5631793141365051,0.5619125962257385,0.5570315718650818,0.5555605292320251,0.5564649701118469,0.5555678606033325,0.5544859170913696,0.5550407767295837,0.5585936903953552,0.5537773370742798,0.5556729435920715,0.5529405474662781,0.5553010702133179,0.5545564293861389,0.5533692240715027,0.5524647235870361,0.5492485165596008,0.5496048927307129,0.5519580245018005,0.5493668913841248,0.540562629699707,0.5466200113296509,0.5439329743385315,0.5439510941505432,0.5422495007514954,0.5391042232513428,0.542296290397644,0.5396343469619751,0.5434682369232178,0.5428260564804077,0.5427985787391663,0.5405367612838745,0.5372419953346252,0.5378466248512268,0.537429690361023,0.5383708477020264,0.5380144119262695,0.5353797078132629,0.5370023846626282,0.5381176471710205,0.5378028154373169,0.5407773852348328,0.5353373289108276,0.537936270236969,0.533463180065155,0.5362191200256348,0.531880795955658,0.5340374708175659,0.53360515832901,0.5286409854888916,0.5345947742462158,0.533896267414093,0.5296618342399597,0.5321204662322998,0.5326751470565796,0.5296971797943115,0.5289782285690308,0.5305594205856323,0.5294773578643799,0.5290824770927429,0.5300261974334717,0.5284433960914612,0.5228140354156494,0.522843062877655,0.5257951021194458,0.5272183418273926,0.5235625505447388,0.523152768611908,0.5268055200576782,0.5271631479263306,0.5268457531929016,0.5236635208129883,0.5263827443122864,0.5249823331832886,0.5234262347221375,0.5262623429298401,0.5189803242683411,0.5263746380805969,0.5230194926261902,0.5205427408218384,0.5189278721809387,0.523457944393158,0.5180924534797668,0.5203491449356079,0.5201369524002075,0.5246935486793518,0.5214768052101135,0.5173106789588928,0.5121468901634216,0.5169481635093689,0.5182508826255798,0.5201603174209595,0.5181154012680054,0.5142061710357666,0.5177993178367615,0.5167156457901001,0.5136253833770752,0.5173128247261047,0.5140285491943359,0.5140151381492615,0.5138891935348511,0.5134558081626892,0.5108151435852051,0.5101727247238159,0.5127654671669006,0.5116594433784485,0.5140573382377625,0.5122021436691284,0.5141736268997192,0.5099276900291443,0.5120996832847595,0.5123591423034668,0.5087164640426636,0.5108581781387329,0.5104978084564209,0.5149020552635193,0.5099084377288818,0.5077404379844666,0.5066447257995605,0.508279025554657,0.5092121958732605,0.5082012414932251,0.5074345469474792,0.509689211845398,0.5028096437454224,0.5089544653892517,0.5055819749832153,0.5063760876655579,0.5055873990058899,0.5002572536468506,0.5070828795433044,0.5026680827140808,0.5015172362327576,0.505970299243927,0.503357470035553,0.5031238198280334,0.5064520835876465,0.5050166249275208,0.5063719749450684,0.5008050799369812,0.5058960914611816,0.5032307505607605,0.5002421736717224,0.4973345398902893,0.4992992877960205,0.5009149312973022,0.4982757568359375,0.49983686208724976,0.4972878098487854,0.49997687339782715,0.5014837980270386,0.4993579089641571,0.500093936920166,0.49732595682144165,0.49652549624443054,0.501615583896637,0.49426811933517456,0.5002940893173218,0.5001663565635681,0.5024756789207458,0.49536049365997314,0.49445876479148865,0.5016487240791321,0.5006794929504395,0.497487336397171,0.4954645037651062,0.49564436078071594,0.4946332573890686,0.49092239141464233,0.4966402053833008,0.4955645799636841,0.49295511841773987,0.4949655532836914,0.4943869113922119,0.49630191922187805,0.4953314960002899,0.4940149188041687,0.4911617636680603,0.49788668751716614,0.4977024793624878,0.49283650517463684,0.49212127923965454,0.48871588706970215,0.4919149875640869,0.4879951477050781,0.4872719347476959,0.48765337467193604,0.4892738163471222,0.4960930347442627,0.4916500151157379,0.48661181330680847,0.49143433570861816,0.4918445348739624,0.49062809348106384,0.49243196845054626,0.48617151379585266,0.48878994584083557,0.48913100361824036,0.48530861735343933,0.4880324900150299,0.48508691787719727,0.487422376871109,0.48856353759765625,0.48889896273612976,0.4854106605052948,0.4872545897960663,0.487723708152771,0.4869893789291382,0.4858531355857849,0.4864254295825958,0.48263514041900635,0.4868328273296356,0.4858182966709137,0.47841620445251465,0.48400723934173584,0.4854613244533539,0.4844323396682739,0.48482006788253784,0.485154390335083,0.4837819039821625,0.4832417368888855,0.48260676860809326,0.4816918969154358,0.47960349917411804,0.4844439625740051,0.48492431640625,0.4841044545173645,0.4809354841709137,0.47979217767715454,0.4796106219291687,0.48150360584259033,0.4826100766658783,0.47830668091773987,0.48426082730293274,0.4808560311794281,0.4820367693901062,0.48024773597717285,0.47847023606300354,0.47870317101478577,0.47969159483909607,0.479085236787796,0.47734957933425903,0.48111075162887573,0.47792157530784607,0.47694724798202515,0.4761480987071991,0.4805774390697479,0.47480088472366333,0.4749457538127899,0.4792780578136444,0.4766496419906616,0.47672176361083984,0.4777785539627075,0.4716593623161316,0.4789189100265503,0.4728718400001526,0.4765647351741791,0.47489917278289795,0.47724661231040955,0.4742104709148407,0.47832611203193665,0.4755014181137085,0.4744729995727539,0.47457340359687805,0.4757039248943329,0.47089940309524536,0.4721088409423828,0.47611239552497864,0.47105851769447327,0.4703883230686188,0.47518986463546753,0.4738517105579376,0.47014322876930237,0.4719253182411194,0.4685955047607422,0.4742695093154907,0.46768492460250854,0.47017955780029297,0.4705439805984497,0.4721927046775818,0.469072163105011,0.46938613057136536,0.467116117477417,0.4689541757106781,0.4686344563961029,0.47100237011909485,0.4696992039680481,0.47174468636512756,0.4695044457912445,0.4732968211174011,0.4696471393108368,0.4648178517818451,0.46416354179382324,0.4669742286205292,0.4670954942703247,0.46654027700424194,0.46558359265327454,0.469932496547699,0.4695530831813812,0.4713548719882965,0.46662792563438416,0.4671609401702881,0.46620604395866394,0.4647507071495056,0.46754190325737,0.4681001901626587,0.4659172594547272,0.4689010977745056,0.4613859951496124,0.4661974608898163,0.4611418545246124,0.46606263518333435,0.46296370029449463,0.4711436927318573,0.46221235394477844,0.4654168486595154,0.4629916846752167,0.46777912974357605,0.46579259634017944,0.4621380865573883,0.4602607190608978,0.46441972255706787,0.4621962904930115,0.46185576915740967,0.46605184674263,0.4617827236652374,0.4638669788837433,0.46419113874435425,0.4656200408935547,0.4605366587638855,0.4625284671783447,0.465408593416214,0.4585071802139282,0.4626162648200989,0.4612753093242645,0.4623899757862091,0.4626368284225464,0.4637683033943176,0.45614829659461975,0.45772677659988403,0.4607677161693573,0.45699983835220337,0.4593279957771301,0.45902085304260254,0.45805060863494873,0.45729386806488037,0.45768898725509644,0.45600828528404236,0.45853903889656067,0.4573563039302826,0.45468658208847046,0.4578606188297272,0.45138248801231384,0.45609623193740845,0.4610534608364105,0.4558926224708557,0.45544615387916565,0.4592471420764923,0.45638221502304077,0.45713675022125244,0.45554494857788086,0.4571760296821594,0.454773485660553,0.45647865533828735,0.45302242040634155,0.4566739499568939,0.4567221999168396,0.4537157714366913,0.4520534873008728,0.45323169231414795,0.45010218024253845,0.44852060079574585,0.4553675353527069,0.45427393913269043,0.45220884680747986,0.45600593090057373,0.45193201303482056,0.45351216197013855,0.45138755440711975,0.4519703686237335,0.4564536511898041,0.4513874650001526,0.45447543263435364,0.45087483525276184,0.45108410716056824,0.4490360617637634,0.4519214630126953,0.45293912291526794,0.4517922103404999,0.4510881006717682,0.4530692994594574,0.4467090368270874,0.45031803846359253,0.44777536392211914,0.4475071132183075,0.4506433308124542,0.4530580937862396,0.45506373047828674,0.44809988141059875,0.4507359266281128,0.4454629421234131,0.4499293565750122],\"type\":\"scatter\"},{\"line\":{\"color\":\"red\",\"width\":2},\"mode\":\"lines\",\"name\":\"val_loss\",\"y\":[0.6921934485435486,0.6867257952690125,0.6826845407485962,0.6794492602348328,0.676699697971344,0.6743760704994202,0.6722981333732605,0.6702419519424438,0.668384313583374,0.6666563749313354,0.6649905443191528,0.6633932590484619,0.6617230772972107,0.6602419018745422,0.6587399244308472,0.6572809815406799,0.6558566093444824,0.6544560194015503,0.6530452966690063,0.6516802310943604,0.6503039002418518,0.6489759087562561,0.6477855443954468,0.6464943289756775,0.6451371312141418,0.6439016461372375,0.6425944566726685,0.6413094997406006,0.6399854421615601,0.6387973427772522,0.6375535726547241,0.6362990140914917,0.6350495219230652,0.6337928175926208,0.6325516104698181,0.6313360929489136,0.6299551129341125,0.6286911964416504,0.6274034380912781,0.6262229681015015,0.6250134706497192,0.6237459182739258,0.6224511861801147,0.621221125125885,0.6200237274169922,0.6187463998794556,0.617536723613739,0.616349995136261,0.6150648593902588,0.6138247847557068,0.6125574707984924,0.611301600933075,0.6101205945014954,0.6089110970497131,0.6077666878700256,0.6065679788589478,0.6054345369338989,0.604330837726593,0.6031302213668823,0.601926326751709,0.6008126735687256,0.5996294021606445,0.5986378192901611,0.5974999666213989,0.5964056849479675,0.59532630443573,0.5942246317863464,0.593104362487793,0.5921729803085327,0.590968132019043,0.5899338722229004,0.5888423919677734,0.5877724289894104,0.5868427753448486,0.5859633088111877,0.5847590565681458,0.5838221311569214,0.5828513503074646,0.5819079875946045,0.5811032056808472,0.5802806615829468,0.579425036907196,0.578704297542572,0.5777973532676697,0.576911449432373,0.5761924386024475,0.5753705501556396,0.5745546221733093,0.5739188194274902,0.5731394290924072,0.5724373459815979,0.5717805624008179,0.5711065530776978,0.5703378319740295,0.5696594715118408,0.5688462257385254,0.5682929754257202,0.567717969417572,0.5669665932655334,0.5664100646972656,0.5658231377601624,0.5652440786361694,0.5644438862800598,0.5639897584915161,0.5634365081787109,0.5629468560218811,0.5624282360076904,0.5620592832565308,0.5614877343177795,0.5610042810440063,0.5603866577148438,0.5599471926689148,0.5594255328178406,0.5589874982833862,0.55855393409729,0.5581785440444946,0.5577672719955444,0.5573011040687561,0.5568808913230896,0.5564700961112976,0.5560168027877808,0.5556262135505676,0.5552654266357422,0.5549015402793884,0.5546164512634277,0.55422043800354,0.5537548065185547,0.5534088611602783,0.5530468821525574,0.5526970028877258,0.5524243712425232,0.5520748496055603,0.5518213510513306,0.5514869689941406,0.5512242913246155,0.5509274005889893,0.5505351424217224,0.5502156019210815,0.5499152541160583,0.5496386885643005,0.5493529438972473,0.5491519570350647,0.5488788485527039,0.5486524701118469,0.5484001040458679,0.5481576323509216,0.5479629039764404,0.547623872756958,0.547332763671875,0.5471199750900269,0.5468505024909973,0.5465899705886841,0.5463031530380249,0.5461556315422058,0.5458627343177795,0.5456464290618896,0.5454717874526978,0.5452937483787537,0.5450661182403564,0.5448276400566101,0.5445021986961365,0.5443041920661926,0.5441667437553406,0.5439825654029846,0.5438153147697449,0.543559193611145,0.5433289408683777,0.5431256890296936,0.5429040193557739,0.5427373647689819,0.5425223708152771,0.5423210263252258,0.5422156453132629,0.5420211553573608,0.5418468117713928,0.541593611240387,0.5414630770683289,0.5412605404853821,0.5411036610603333,0.5408990383148193,0.5407267808914185,0.5405550599098206,0.5404135584831238,0.5402276515960693,0.5401009321212769,0.53989177942276,0.539655327796936,0.5395055413246155,0.5393732190132141,0.5392156839370728,0.5391141176223755,0.5389337539672852,0.5387936234474182,0.5386602878570557,0.5384648442268372,0.5383469462394714,0.5381903052330017,0.5380539298057556,0.5378714799880981,0.5377488732337952,0.537588357925415,0.5374327898025513,0.5372443199157715,0.5371016263961792,0.5369209051132202,0.5367825031280518,0.5366323590278625,0.5364943146705627,0.5363805890083313,0.5362657308578491,0.5361528992652893,0.536043643951416,0.5359078645706177,0.5358342528343201,0.5356723666191101,0.5355457663536072,0.5354984402656555,0.5353899598121643,0.5352502465248108,0.5351566076278687,0.5350234508514404,0.5349368453025818,0.5348096489906311,0.5347415208816528,0.5345912575721741,0.5344572067260742,0.5343564748764038,0.5341843366622925,0.534071147441864,0.5339354276657104,0.5337952375411987,0.5337191820144653,0.5336421728134155,0.5335161685943604,0.5334287881851196,0.533340573310852,0.5332488417625427,0.5331020951271057,0.5330464243888855,0.5329576730728149,0.5328516364097595,0.5327210426330566,0.5325899720191956,0.5325060486793518,0.5323542356491089,0.5322325825691223,0.5321233868598938,0.5320309400558472,0.5319124460220337,0.5318365693092346,0.5317547917366028,0.5316588282585144,0.5315778851509094,0.5315014719963074,0.5314010977745056,0.5313459038734436,0.531283438205719,0.531198263168335,0.5311391949653625,0.5310476422309875,0.5309776663780212,0.5308965444564819,0.5308282971382141,0.5307263731956482,0.5306956768035889,0.5305926203727722,0.5305285453796387,0.5304676294326782,0.5303687453269958,0.5303105711936951,0.5302647352218628,0.5301631093025208,0.5301077365875244,0.5300211906433105,0.5299111008644104,0.5298237204551697,0.5297510623931885,0.529727578163147,0.5296638011932373,0.5296241641044617,0.5295600891113281,0.5294979214668274,0.5294169783592224,0.5292991399765015,0.529240071773529,0.5291609764099121,0.5290969014167786,0.529023289680481,0.5289520025253296,0.5288923382759094,0.5288108587265015,0.5287601351737976,0.5287042260169983,0.5286513566970825,0.5285788774490356,0.5285462737083435,0.5284643769264221,0.5284167528152466,0.5283405184745789,0.5282910466194153,0.5282447338104248,0.5281985402107239,0.5281318426132202,0.5281035304069519,0.5280419588088989,0.5279814004898071,0.5279161930084229,0.5278542637825012,0.5278142094612122,0.5277734994888306,0.5277068614959717,0.5276552438735962,0.52761310338974,0.5275723934173584,0.5275117754936218,0.5274848341941833,0.527420163154602,0.5273367762565613,0.5272785425186157,0.5272051095962524,0.5271542072296143,0.5270997881889343,0.5270306468009949,0.5269772410392761,0.5269597768783569,0.5269030332565308,0.5268558263778687,0.5268213152885437,0.5267559885978699,0.5267084240913391,0.5266571640968323,0.5266641974449158,0.5265795588493347,0.5265334844589233,0.5264974236488342,0.5264514684677124,0.5264038443565369,0.5264050960540771,0.5263723731040955,0.5263130068778992,0.5262734293937683,0.5262356400489807,0.5262169241905212,0.52618807554245,0.5261310338973999,0.5261090397834778,0.5260607004165649,0.5260480642318726,0.5260192155838013,0.5259789228439331,0.5259192585945129,0.525885820388794,0.5258267521858215,0.5257788896560669,0.5257453322410583,0.5257272124290466,0.525695264339447,0.5256296992301941,0.5256460309028625,0.5256271362304688,0.525556206703186,0.5255275964736938,0.5255048274993896,0.5254703164100647,0.5254245400428772,0.525385856628418,0.5253756642341614,0.5253660082817078,0.5253496766090393,0.5252907872200012,0.5252456665039062,0.5252156853675842,0.5251944661140442,0.5252271294593811,0.525203287601471,0.5252062082290649,0.5251545310020447,0.5251251459121704,0.5250381231307983,0.5249839425086975,0.524976909160614,0.5249693393707275,0.525000810623169,0.524954617023468,0.5249807834625244,0.5249903202056885,0.5249331593513489,0.5248806476593018,0.5248695015907288,0.5248506665229797,0.524841845035553,0.524787962436676,0.5248076319694519,0.5247905850410461,0.5248045325279236,0.5247804522514343,0.5247418880462646,0.5247431397438049,0.5247281193733215,0.5247442722320557,0.5247030854225159,0.5246779918670654,0.524660587310791,0.5246551632881165,0.524613082408905,0.5246092677116394,0.5246431231498718,0.5246332287788391,0.5245531797409058,0.5245729684829712,0.5245349407196045,0.5244811773300171,0.5244366526603699,0.5244061350822449,0.5243978500366211,0.5243475437164307,0.5243340730667114,0.5243819355964661,0.5243862867355347,0.5243359804153442,0.524318277835846,0.5242646932601929,0.5242437720298767,0.5242193937301636,0.5242297053337097,0.5241936445236206,0.5241900086402893,0.5241706967353821,0.5241913199424744,0.5241469144821167,0.5241715908050537,0.524120569229126,0.5241494178771973,0.5241000056266785,0.5241267681121826,0.524105966091156,0.5240740776062012,0.5239996910095215,0.5240076184272766,0.524009644985199,0.5240124464035034,0.5240291357040405,0.5239896774291992,0.5240384936332703,0.5239843726158142,0.5239612460136414,0.5239507555961609,0.5239137411117554,0.5239484310150146,0.5238951444625854,0.5238711833953857,0.523882269859314,0.5238720178604126,0.5238339900970459,0.5238378047943115,0.5238154530525208,0.5238643884658813,0.5238330960273743,0.5238063335418701,0.5237948298454285,0.5237739086151123,0.5237596035003662,0.5237669348716736,0.5237502455711365,0.5237672924995422,0.5236919522285461,0.5237722396850586,0.5237746834754944,0.523762047290802,0.5237481594085693,0.5237502455711365,0.5237061381340027,0.5236855149269104,0.5236919522285461,0.523703396320343,0.5236591696739197,0.5236768126487732,0.5236549377441406,0.5236337780952454,0.5236321687698364,0.5235973596572876,0.5235626101493835,0.523552656173706,0.5235483646392822,0.5236018300056458,0.5235869288444519,0.5235936045646667,0.5235192179679871,0.5235133767127991,0.5235468745231628,0.5235412120819092,0.5235174298286438,0.5235515832901001,0.5235536694526672,0.5235313177108765,0.5235401391983032,0.5235282778739929,0.5235040783882141,0.5234849452972412,0.5234794616699219],\"type\":\"scatter\"}],                        {\"title\":{\"text\":\"Loss\"},\"xaxis\":{\"title\":{\"text\":\"epochs\"}},\"yaxis\":{\"title\":{\"text\":\"\"}},\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('66ab509d-f1c1-403f-b263-0d619722e1af');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "h1 = go.Scatter(y=hist2.history['loss'], \n",
        "                    mode=\"lines\", line=dict(\n",
        "                    width=2,\n",
        "                    color='blue'),\n",
        "                    name=\"loss\"\n",
        "                   )\n",
        "h2 = go.Scatter(y=hist2.history['val_loss'], \n",
        "                    mode=\"lines\", line=dict(\n",
        "                    width=2,\n",
        "                    color='red'),\n",
        "                    name=\"val_loss\"\n",
        "                   )\n",
        "\n",
        "data = [h1,h2]\n",
        "layout1 = go.Layout(title='Loss',\n",
        "                   xaxis=dict(title='epochs'),\n",
        "                   yaxis=dict(title=''))\n",
        "fig1 = go.Figure(data = data, layout=layout1)\n",
        "fig1.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgkoDS3ip-nO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "77349d9c-c49a-4af8-95c8-8a3c9b714694"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"2143c075-22e7-4c19-b56e-0fb74553dc86\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2143c075-22e7-4c19-b56e-0fb74553dc86\")) {                    Plotly.newPlot(                        \"2143c075-22e7-4c19-b56e-0fb74553dc86\",                        [{\"line\":{\"color\":\"blue\",\"width\":2},\"mode\":\"lines\",\"name\":\"acc\",\"y\":[0.5077341198921204,0.5252422094345093,0.5221825838088989,0.5340812802314758,0.543090283870697,0.5315315127372742,0.5544790029525757,0.5539690852165222,0.553459107875824,0.5527791976928711,0.562808096408844,0.5714771151542664,0.565017819404602,0.5662077069282532,0.572157084941864,0.5838857889175415,0.5791262984275818,0.5811660885810852,0.5854156017303467,0.5871154069900513,0.5872853994369507,0.584395706653595,0.5962944030761719,0.5942546129226685,0.5983341932296753,0.593744695186615,0.6051334142684937,0.6015638113021851,0.6114227175712585,0.6075131893157959,0.6075131893157959,0.6121026873588562,0.6189019083976746,0.6158422827720642,0.6192418932914734,0.62655109167099,0.6296107172966003,0.6330103874206543,0.6257011890411377,0.6353901028633118,0.6279109120368958,0.6409994959831238,0.638789713382721,0.6433792114257812,0.6355600953102112,0.6447390913963318,0.6415094137191772,0.6459289193153381,0.6493285894393921,0.6666666865348816,0.6432092189788818,0.6528981924057007,0.6561278104782104,0.6573176980018616,0.6576576828956604,0.6627570986747742,0.6624171137809753,0.6647968888282776,0.6683664917945862,0.6698963046073914,0.6721060872077942,0.6678565144538879,0.6775454878807068,0.6734659075737,0.6734659075737,0.6714261174201965,0.6836647987365723,0.6797552108764648,0.677205502986908,0.6770355105400085,0.6761856079101562,0.694033682346344,0.6843447089195251,0.6799252033233643,0.6870644092559814,0.6918238997459412,0.685024619102478,0.6933537125587463,0.6913139820098877,0.6938636898994446,0.6862145066261292,0.6909739971160889,0.6933537125587463,0.695903480052948,0.6989631056785583,0.6945436000823975,0.7006629109382629,0.7037225961685181,0.7015128135681152,0.6989631056785583,0.7071222066879272,0.7086520195007324,0.7038925886154175,0.7033826112747192,0.695903480052948,0.7095019817352295,0.7015128135681152,0.6976032853126526,0.7016828060150146,0.7038925886154175,0.7095019817352295,0.7079721093177795,0.715621292591095,0.7139214873313904,0.7112017869949341,0.7122216820716858,0.7086520195007324,0.7130715847015381,0.7112017869949341,0.7091619968414307,0.710011899471283,0.7123916149139404,0.7164711952209473,0.7117117047309875,0.715961217880249,0.717491090297699,0.717491090297699,0.7164711952209473,0.7258201837539673,0.7190209031105042,0.7203807830810547,0.7198708057403564,0.7278599143028259,0.7222505807876587,0.7217406034469604,0.7220805883407593,0.7229304909706116,0.7236104011535645,0.7242903113365173,0.7244603037834167,0.725310206413269,0.7265000939369202,0.7266700863838196,0.7232704162597656,0.7281998991966248,0.7288798093795776,0.7227604985237122,0.7297297120094299,0.7263301014900208,0.7220805883407593,0.7307496070861816,0.7246302962303162,0.7304096817970276,0.7219105958938599,0.7375488877296448,0.729049801826477,0.7292197942733765,0.737208902835846,0.7300696969032288,0.7315995097160339,0.7363590002059937,0.7319394946098328,0.7361890077590942,0.7310895919799805,0.7338092923164368,0.7356790900230408,0.7361890077590942,0.7326194047927856,0.7327893972396851,0.7355090975761414,0.7392486929893494,0.7400985956192017,0.7341492176055908,0.7343192100524902,0.7377188801765442,0.7389087080955505,0.7414584159851074,0.7329593896865845,0.7358490824699402,0.7344892024993896,0.7360190153121948,0.7375488877296448,0.7346591949462891,0.7366989850997925,0.7412884831428528,0.7307496070861816,0.7339792847633362,0.7414584159851074,0.740948498249054,0.7395886182785034,0.7406085133552551,0.7375488877296448,0.7411184906959534,0.7366989850997925,0.7360190153121948,0.7443481087684631,0.7375488877296448,0.7457079887390137,0.7443481087684631,0.7400985956192017,0.7443481087684631,0.7438381910324097,0.7395886182785034,0.7419683933258057,0.7462179064750671,0.740948498249054,0.7443481087684631,0.7416284084320068,0.7431582808494568,0.7412884831428528,0.7480877041816711,0.7482576966285706,0.7460479140281677,0.7440081834793091,0.7411184906959534,0.7436681985855103,0.7463878989219666,0.7438381910324097,0.7434982061386108,0.7438381910324097,0.7470678091049194,0.7474077939987183,0.7453680038452148,0.746557891368866,0.745028018951416,0.7451980113983154,0.7482576966285706,0.750637412071228,0.7458779811859131,0.7472378015518188,0.7460479140281677,0.7378888130187988,0.7519972920417786,0.7536970973014832,0.752167284488678,0.7526772022247314,0.7514873147010803,0.7516573071479797,0.7492775917053223,0.7453680038452148,0.7514873147010803,0.7538670897483826,0.7482576966285706,0.7526772022247314,0.7501274943351746,0.7565867900848389,0.7552269101142883,0.7582865953445435,0.745028018951416,0.752507209777832,0.7516573071479797,0.7550569176673889,0.7553969025611877,0.752507209777832,0.7584565877914429,0.7528471946716309,0.7552269101142883,0.7513173818588257,0.7504674196243286,0.7560768127441406,0.7552269101142883,0.7572667002677917,0.7508074045181274,0.7508074045181274,0.754037082195282,0.748767614364624,0.7526772022247314,0.7526772022247314,0.7538670897483826,0.7570967078208923,0.7545469999313354,0.7569267153739929,0.7518272995948792,0.7519972920417786,0.755906879901886,0.7542070150375366,0.7594764828681946,0.7545469999313354,0.7611762881278992,0.7564167976379395,0.7572667002677917,0.7553969025611877,0.754037082195282,0.7557368874549866,0.7591364979743958,0.7616862058639526,0.7504674196243286,0.7526772022247314,0.7567567825317383,0.7598164081573486,0.7642359137535095,0.7560768127441406,0.7662757039070129,0.7676355838775635,0.763725996017456,0.758116602897644,0.7570967078208923,0.7576066851615906,0.7608363032341003,0.759986400604248,0.7576066851615906,0.7603263854980469,0.7545469999313354,0.7604963183403015,0.7587965130805969,0.7630460858345032,0.7638959884643555,0.7615162134170532,0.7676355838775635,0.7603263854980469,0.759986400604248,0.7625361084938049,0.7628760933876038,0.7611762881278992,0.7589665055274963,0.761856198310852,0.7613462805747986,0.7661057114601135,0.7613462805747986,0.7596464157104492,0.7628760933876038,0.7727349996566772,0.7655957937240601,0.7627061009407043,0.7633860111236572,0.7645758986473083,0.7596464157104492,0.7587965130805969,0.7645758986473083,0.759986400604248,0.7703552842140198,0.7645758986473083,0.7657657861709595,0.7672955989837646,0.7623661160469055,0.7610062956809998,0.7713751196861267,0.7696753144264221,0.7606663107872009,0.7608363032341003,0.770015299320221,0.7628760933876038,0.7698453068733215,0.7654258012771606,0.7701852917671204,0.7679755091667175,0.7674655914306641,0.7664456963539124,0.7686554193496704,0.7695053815841675,0.7638959884643555,0.7627061009407043,0.7679755091667175,0.7693353891372681,0.7649158835411072,0.7701852917671204,0.7679755091667175,0.7647458910942078,0.7659357190132141,0.7689954042434692,0.7632160186767578,0.7695053815841675,0.7712051868438721,0.7657657861709595,0.7696753144264221,0.7662757039070129,0.7667856812477112,0.7683154940605164,0.7678055167198181,0.7688254117965698,0.7730749845504761,0.7703552842140198,0.7706952095031738,0.7723950147628784,0.7720550894737244,0.775624692440033,0.7695053815841675,0.7706952095031738,0.7691653966903687,0.7706952095031738,0.7739248871803284,0.7732449173927307,0.7720550894737244,0.7689954042434692,0.773754894733429,0.773754894733429,0.7713751196861267,0.7713751196861267,0.7752847075462341,0.7735849022865295,0.773754894733429,0.7732449173927307,0.7691653966903687,0.7729049921035767,0.7715451121330261,0.7681455016136169,0.7773244976997375,0.7674655914306641,0.7698453068733215,0.7751147150993347,0.7768145799636841,0.7732449173927307,0.7729049921035767,0.7751147150993347,0.7790243029594421,0.7689954042434692,0.7710351943969727,0.7717151045799255,0.771885097026825,0.7730749845504761,0.7790243029594421,0.7732449173927307,0.7705252170562744,0.773754894733429,0.7749447822570801,0.7703552842140198,0.774094820022583,0.773754894733429,0.7769845128059387,0.7735849022865295,0.7771545052528381,0.7710351943969727,0.7783443927764893,0.770015299320221,0.7769845128059387,0.7695053815841675,0.7705252170562744,0.7802141904830933,0.779364287853241,0.7720550894737244,0.7788543105125427,0.775624692440033,0.7766445875167847,0.773754894733429,0.7761346101760864,0.7807241082191467,0.7727349996566772,0.7781744003295898,0.777494490146637,0.775624692440033,0.7785143852233887,0.779704213142395,0.775964617729187,0.7757946848869324,0.7744348049163818,0.7744348049163818,0.7822539806365967,0.7848036885261536,0.7791942954063416,0.7768145799636841,0.7807241082191467,0.7771545052528381,0.7822539806365967,0.7791942954063416,0.7798742055892944,0.7795342803001404,0.7769845128059387,0.7824239134788513,0.7781744003295898,0.7764745950698853,0.7798742055892944,0.781574010848999,0.7780044078826904,0.7781744003295898,0.7763046026229858,0.7757946848869324,0.779704213142395,0.779704213142395,0.7783443927764893,0.7822539806365967,0.783443808555603,0.7854835987091064,0.7859935164451599,0.7822539806365967,0.7771545052528381,0.7827638983726501,0.7808941006660461,0.781574010848999,0.7875233888626099,0.7870134115219116,0.777834415435791,0.7800441980361938,0.7786843180656433,0.781234085559845,0.7825939059257507,0.7839537858963013,0.781234085559845,0.781234085559845,0.7810640931129456,0.7819139957427979,0.7808941006660461,0.7817440032958984,0.7851436138153076,0.7829338908195496,0.7832738161087036,0.777834415435791,0.7820839881896973,0.783443808555603,0.7868434190750122,0.7851436138153076,0.7837837934494019,0.7844637036323547,0.7893931865692139,0.7814040184020996,0.781574010848999,0.7764745950698853,0.7842937111854553,0.7788543105125427,0.7885432839393616,0.7839537858963013],\"type\":\"scatter\"},{\"line\":{\"color\":\"red\",\"width\":2},\"mode\":\"lines\",\"name\":\"val_acc\",\"y\":[0.5390890836715698,0.5479266047477722,0.5560842752456665,0.560163140296936,0.5710400938987732,0.5785180330276489,0.5839564800262451,0.5859959125518799,0.5914344191551208,0.5927940011024475,0.5921142101287842,0.5921142101287842,0.5927940011024475,0.5975526571273804,0.602311372756958,0.6050305962562561,0.6091094613075256,0.6118286848068237,0.6118286848068237,0.6125084757804871,0.6138681173324585,0.6145479083061218,0.6186267733573914,0.6213459968566895,0.6240652799606323,0.6227056384086609,0.625424861907959,0.6261047124862671,0.6281441450119019,0.6281441450119019,0.6295037269592285,0.6342623829841614,0.6356220245361328,0.6376614570617676,0.6397008895874023,0.6430999040603638,0.6471787691116333,0.6478586196899414,0.6526172757148743,0.6532970666885376,0.6553364992141724,0.6566961407661438,0.6553364992141724,0.6600951552391052,0.6607750058174133,0.66213458776474,0.6634942293167114,0.6662134528160095,0.667573094367981,0.6730115413665771,0.6743711829185486,0.6764106154441833,0.6804894804954529,0.6811692714691162,0.683208703994751,0.6818490624427795,0.6852481365203857,0.6872875690460205,0.6893270015716553,0.6900067925453186,0.6940856575965881,0.6947654485702515,0.6968048810958862,0.6974847316741943,0.7015635371208191,0.7015635371208191,0.7036029696464539,0.704282820224762,0.7063222527503967,0.7063222527503967,0.7070020437240601,0.7076818346977234,0.7070020437240601,0.7076818346977234,0.7076818346977234,0.7097212672233582,0.7097212672233582,0.7110809087753296,0.7131203413009644,0.7117606997489929,0.7110809087753296,0.7110809087753296,0.7090414762496948,0.7104010581970215,0.7104010581970215,0.7110809087753296,0.7110809087753296,0.7104010581970215,0.7104010581970215,0.7110809087753296,0.7117606997489929,0.7117606997489929,0.7131203413009644,0.7104010581970215,0.7124404907226562,0.7131203413009644,0.7138001322746277,0.7138001322746277,0.714479923248291,0.714479923248291,0.7151597738265991,0.7158395648002625,0.7151597738265991,0.7151597738265991,0.7171992063522339,0.7178789973258972,0.7178789973258972,0.7171992063522339,0.7178789973258972,0.7178789973258972,0.719918429851532,0.7219578623771667,0.7205982208251953,0.7219578623771667,0.7212780714035034,0.7212780714035034,0.7219578623771667,0.7226376533508301,0.7226376533508301,0.7226376533508301,0.7239972949028015,0.7239972949028015,0.7246770858764648,0.7260367274284363,0.7273963093757629,0.7267165184020996,0.7273963093757629,0.7273963093757629,0.728076159954071,0.7273963093757629,0.7287559509277344,0.728076159954071,0.7294357419013977,0.7294357419013977,0.7287559509277344,0.7294357419013977,0.7294357419013977,0.7301155924797058,0.7294357419013977,0.7294357419013977,0.7287559509277344,0.7287559509277344,0.728076159954071,0.7287559509277344,0.7287559509277344,0.7294357419013977,0.7301155924797058,0.7301155924797058,0.7307953834533691,0.7307953834533691,0.7314751744270325,0.7314751744270325,0.7307953834533691,0.7307953834533691,0.7314751744270325,0.7321550250053406,0.7314751744270325,0.7321550250053406,0.7328348159790039,0.7328348159790039,0.7335146069526672,0.7328348159790039,0.7335146069526672,0.7341943979263306,0.7341943979263306,0.7341943979263306,0.7362338304519653,0.735554039478302,0.735554039478302,0.7362338304519653,0.7362338304519653,0.735554039478302,0.7362338304519653,0.7375934720039368,0.7375934720039368,0.7375934720039368,0.7369136810302734,0.7389531135559082,0.7389531135559082,0.7389531135559082,0.7369136810302734,0.7389531135559082,0.7375934720039368,0.7382732629776001,0.7382732629776001,0.7396329045295715,0.7375934720039368,0.7396329045295715,0.7396329045295715,0.7389531135559082,0.7389531135559082,0.7389531135559082,0.7396329045295715,0.7396329045295715,0.7403126955032349,0.7403126955032349,0.7403126955032349,0.740992546081543,0.740992546081543,0.7416723370552063,0.7416723370552063,0.7416723370552063,0.740992546081543,0.740992546081543,0.740992546081543,0.7396329045295715,0.7403126955032349,0.7396329045295715,0.740992546081543,0.740992546081543,0.7416723370552063,0.7416723370552063,0.7423521280288696,0.7423521280288696,0.7416723370552063,0.7416723370552063,0.7423521280288696,0.7423521280288696,0.7423521280288696,0.7423521280288696,0.7430319786071777,0.7423521280288696,0.7416723370552063,0.7437117695808411,0.7437117695808411,0.7437117695808411,0.7437117695808411,0.7430319786071777,0.7430319786071777,0.7430319786071777,0.7430319786071777,0.7437117695808411,0.7437117695808411,0.7450713515281677,0.7450713515281677,0.7450713515281677,0.7450713515281677,0.7464309930801392,0.7457512021064758,0.7457512021064758,0.7457512021064758,0.7477906346321106,0.7464309930801392,0.7471107840538025,0.7464309930801392,0.7464309930801392,0.7471107840538025,0.7464309930801392,0.7471107840538025,0.7471107840538025,0.7471107840538025,0.7471107840538025,0.7471107840538025,0.7484704256057739,0.7477906346321106,0.7484704256057739,0.7491502165794373,0.7471107840538025,0.7498300671577454,0.7505098581314087,0.7498300671577454,0.7505098581314087,0.751189649105072,0.7498300671577454,0.7505098581314087,0.7498300671577454,0.7484704256057739,0.7505098581314087,0.7491502165794373,0.7498300671577454,0.7491502165794373,0.7491502165794373,0.7491502165794373,0.7491502165794373,0.7498300671577454,0.7498300671577454,0.7505098581314087,0.7498300671577454,0.7498300671577454,0.7498300671577454,0.7484704256057739,0.7484704256057739,0.7491502165794373,0.7498300671577454,0.7498300671577454,0.7491502165794373,0.7484704256057739,0.7491502165794373,0.7491502165794373,0.7491502165794373,0.7491502165794373,0.7491502165794373,0.7484704256057739,0.7484704256057739,0.7484704256057739,0.7484704256057739,0.7484704256057739,0.7484704256057739,0.7484704256057739,0.7477906346321106,0.7477906346321106,0.7484704256057739,0.7471107840538025,0.7471107840538025,0.7471107840538025,0.7471107840538025,0.7471107840538025,0.7471107840538025,0.7464309930801392,0.7471107840538025,0.7471107840538025,0.7477906346321106,0.7464309930801392,0.7471107840538025,0.7477906346321106,0.7477906346321106,0.7484704256057739,0.7471107840538025,0.7471107840538025,0.7471107840538025,0.7464309930801392,0.7464309930801392,0.7471107840538025,0.7477906346321106,0.7477906346321106,0.7477906346321106,0.7477906346321106,0.7491502165794373,0.7491502165794373,0.7491502165794373,0.7505098581314087,0.7498300671577454,0.7491502165794373,0.7491502165794373,0.7498300671577454,0.7491502165794373,0.7491502165794373,0.7491502165794373,0.7498300671577454,0.7498300671577454,0.7491502165794373,0.7498300671577454,0.7498300671577454,0.7498300671577454,0.7498300671577454,0.7498300671577454,0.7498300671577454,0.7498300671577454,0.7505098581314087,0.7498300671577454,0.7498300671577454,0.7491502165794373,0.7491502165794373,0.7498300671577454,0.7491502165794373,0.7491502165794373,0.7491502165794373,0.7498300671577454,0.7505098581314087,0.7505098581314087,0.7505098581314087,0.7498300671577454,0.7505098581314087,0.7505098581314087,0.7505098581314087,0.7498300671577454,0.7505098581314087,0.7505098581314087,0.7491502165794373,0.7505098581314087,0.7505098581314087,0.751189649105072,0.7505098581314087,0.7505098581314087,0.7491502165794373,0.7505098581314087,0.7491502165794373,0.7498300671577454,0.7505098581314087,0.7505098581314087,0.7498300671577454,0.7498300671577454,0.7498300671577454,0.7491502165794373,0.7484704256057739,0.7491502165794373,0.7491502165794373,0.7484704256057739,0.7498300671577454,0.7484704256057739,0.7477906346321106,0.7471107840538025,0.7471107840538025,0.7491502165794373,0.7518694996833801,0.751189649105072,0.7471107840538025,0.7518694996833801,0.7518694996833801,0.7491502165794373,0.7477906346321106,0.7491502165794373,0.7505098581314087,0.7498300671577454,0.7498300671577454,0.7505098581314087,0.7505098581314087,0.7505098581314087,0.7491502165794373,0.7491502165794373,0.7491502165794373,0.7491502165794373,0.7491502165794373,0.7498300671577454,0.7491502165794373,0.7491502165794373,0.7498300671577454,0.7498300671577454,0.7498300671577454,0.7491502165794373,0.7498300671577454,0.7491502165794373,0.7491502165794373,0.7491502165794373,0.7491502165794373,0.7484704256057739,0.7484704256057739,0.7491502165794373,0.7491502165794373,0.7484704256057739,0.7484704256057739,0.7491502165794373,0.7477906346321106,0.7471107840538025,0.7464309930801392,0.7471107840538025,0.7471107840538025,0.7491502165794373,0.7484704256057739,0.7484704256057739,0.7464309930801392,0.7457512021064758,0.7457512021064758,0.7457512021064758,0.7477906346321106,0.7477906346321106,0.7477906346321106,0.7484704256057739,0.7484704256057739,0.7484704256057739,0.7484704256057739,0.7477906346321106,0.7484704256057739,0.7484704256057739,0.7471107840538025,0.7471107840538025,0.7484704256057739,0.7477906346321106,0.7484704256057739,0.7471107840538025,0.7471107840538025,0.7464309930801392,0.7471107840538025,0.7471107840538025,0.7464309930801392,0.7484704256057739,0.7464309930801392,0.7464309930801392,0.7477906346321106,0.7471107840538025,0.7471107840538025,0.7464309930801392,0.7464309930801392,0.7471107840538025,0.7484704256057739,0.7471107840538025,0.7464309930801392,0.7477906346321106,0.7471107840538025,0.7477906346321106,0.7477906346321106,0.7477906346321106,0.7477906346321106,0.7477906346321106,0.7491502165794373,0.7498300671577454,0.7491502165794373,0.7471107840538025,0.7471107840538025,0.7491502165794373,0.7491502165794373,0.7498300671577454,0.7505098581314087,0.7505098581314087,0.7505098581314087,0.7505098581314087,0.7498300671577454,0.7477906346321106,0.7471107840538025,0.7477906346321106],\"type\":\"scatter\"}],                        {\"title\":{\"text\":\"Accuracy\"},\"xaxis\":{\"title\":{\"text\":\"epochs\"}},\"yaxis\":{\"title\":{\"text\":\"\"}},\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('2143c075-22e7-4c19-b56e-0fb74553dc86');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "h1 = go.Scatter(y=hist2.history['accuracy'], \n",
        "                    mode=\"lines\", line=dict(\n",
        "                    width=2,\n",
        "                    color='blue'),\n",
        "                    name=\"acc\"\n",
        "                   )\n",
        "h2 = go.Scatter(y=hist2.history['val_accuracy'], \n",
        "                    mode=\"lines\", line=dict(\n",
        "                    width=2,\n",
        "                    color='red'),\n",
        "                    name=\"val_acc\"\n",
        "                   )\n",
        "\n",
        "data = [h1,h2]\n",
        "layout1 = go.Layout(title='Accuracy',\n",
        "                   xaxis=dict(title='epochs'),\n",
        "                   yaxis=dict(title=''))\n",
        "fig1 = go.Figure(data = data, layout=layout1)\n",
        "fig1.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNMQhFMLp-r2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62001a04-3d93-45b3-8c76-701be1c1100f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " input_5 (InputLayer)           [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, 128, 100)     2269800     ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_4 (Embedding)        (None, 128, 100)     2269800     ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_5 (Embedding)        (None, 128, 100)     2269800     ['input_6[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 125, 32)      12832       ['embedding_3[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 123, 32)      19232       ['embedding_4[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 121, 32)      25632       ['embedding_5[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 125, 32)      0           ['conv1d_3[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 123, 32)      0           ['conv1d_4[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 121, 32)      0           ['conv1d_5[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_3 (MaxPooling1D)  (None, 62, 32)      0           ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_4 (MaxPooling1D)  (None, 61, 32)      0           ['dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_5 (MaxPooling1D)  (None, 60, 32)      0           ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_3 (Flatten)            (None, 1984)         0           ['max_pooling1d_3[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_4 (Flatten)            (None, 1952)         0           ['max_pooling1d_4[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_5 (Flatten)            (None, 1920)         0           ['max_pooling1d_5[0][0]']        \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 5856)         0           ['flatten_3[0][0]',              \n",
            "                                                                  'flatten_4[0][0]',              \n",
            "                                                                  'flatten_5[0][0]']              \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 10)           58570       ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 2)            22          ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,925,688\n",
            "Trainable params: 116,288\n",
            "Non-trainable params: 6,809,400\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "predict_model2 = load_model(filename) \n",
        "predict_model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zP7KcRSp-u2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "395bee08-0811-4077-f269-4922bb3ed78e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validate loss: 0.5234794616699219\n",
            "Validate accuracy: 0.7477906346321106\n"
          ]
        }
      ],
      "source": [
        "score = predict_model2.evaluate([val_X, val_X, val_X], val_Y, verbose=0)\n",
        "print('Validate loss:', score[0])\n",
        "print('Validate accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34LlXxKCqRtc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60e95405-870d-4efa-dcab-cc9c4049b907"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1471,)"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "predicted_classes = np.argmax(predict_model2.predict([val_X, val_X, val_X]), axis=-1)\n",
        "predicted_classes.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ui-y1i-tqRwt"
      },
      "outputs": [],
      "source": [
        "(1720,)\n",
        "\n",
        "cm = confusion_matrix(y_true, predicted_classes)\n",
        "np.savetxt(\"confusion_matrix.csv\", cm, delimiter=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjMRy_m0qRz6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d4774bb-5e75-4719-e41f-af4aee6cf7ff"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABCIAAAMdCAYAAABOWtelAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzde5zWdZ03/tcMJpLDoOhGQgZoCRok4KnURH52QsvU1lJ2k7zTfp64o9Iou7No00o2Y5WVLXEV11xaNfFY22GtLNcMs3IU29YwUlI80QwxgHBd9x9uszuBwPe+9HM58Hz2uB4438N8P5d/2Yv36/tpqdfr9QAAAAAU0NrsBQAAAADbDkEEAAAAUIwgAgAAAChGEAEAAAAUI4gAAAAAitmuWQ/uvnV2sx4NAMVNPPWGZi8BAIq6e9kPmr2EF82zT/6m2UvYqJftukezl7BFTEQAAAAAxQgiAAAAgGIEEQAAAEAxTXtHBAAAAPRJtfXNXkGfZiICAAAAKEYQAQAAABSjmgEAAABV1GvNXkGfZiICAAAAKEYQAQAAABSjmgEAAABV1FQzGmEiAgAAAChGEAEAAAAUo5oBAAAAFdTtmtEQExEAAABAMYIIAAAAoBjVDAAAAKjCrhkNMREBAAAAFCOIAAAAAIpRzQAAAIAq7JrREBMRAAAAQDGCCAAAAKAY1QwAAACoora+2Svo00xEAAAAAMUIIgAAAIBiVDMAAACgCrtmNMREBAAAAFCMIAIAAAAoRjUDAAAAqqipZjTCRAQAAABQjIkIAAAAqKDuZZUNMREBAAAAFCOIAAAAAIpRzQAAAIAqvKyyISYiAAAAgGIEEQAAAEAxqhkAAABQhV0zGmIiAgAAAChGEAEAAAAUo5oBAAAAVdTWN3sFfZqJCAAAAKAYQQQAAABQjGoGAAAAVGHXjIaYiAAAAACKEUQAAAAAxahmAAAAQBU11YxGmIgAAAAAihFEAAAAAMWoZgAAAEAVds1oiIkIAAAAoBhBBAAAAFCMagYAAABUsRXsmnHJJZfk0ksvzQ477NBzbNKkSbnooouSJKNGjUr//v3Tr1+/nvMLFizIqFGjkiS1Wi2zZ8/Oddddl+7u7kyYMCGf/exnM2zYsM0+WxABAAAA26D9998///RP//S85y+77LIcdNBBGz03b9683HLLLbn66qszZMiQfOELX8hpp52WG2+8Ma2tmy5fCCIAAABgK9DZ2ZnOzs4Njre3t6e9vf0FfdaCBQtyyimnZI899kiSnHPOOTn44INzzz335IADDtjkvYIIAAAAqKBeX9/sJWzU/PnzM2fOnA2On3XWWZk2bdoGxzs6OvKGN7whAwYMyIQJEzJ9+vTsvvvuPec/+tGP5tlnn83QoUNz4okn5j3veU+SpKurK48++mjGjBnTc217e3uGDx+exYsXCyIAAABgWzB16tQce+yxGxzf2DTE2972thx33HEZOnRoli9fni996Us5+eSTc+ONN2bHHXfMlVdemfHjx6e1tTV33XVXzj777Kxbty5TpkzJypUrN/p7Bw4c2HNuUwQRAAAAsBWoUsHYa6+9ev55yJAhOf/887P//vvn3nvvzaGHHpo3vvGNPecPO+ywvP/9789NN92UKVOmpK2tLclzkxH/U1dXV8+5TbF9JwAAAFRRr700Pw1oaWlJS0tL6vX6Rs+3trb2nBs4cGCGDRuWjo6OnvNdXV1ZunRp9t57780+SxABAAAA25jbbrstTz/9dJLkqaeeyqc+9akMHjw448ePz/3335/77rsva9euzbp16/LjH/84V1xxRY466qie+0844YRcfvnlWbJkSVatWpVZs2ZlxIgR2W+//Tb7bNUMAAAA2MbcdNNN+exnP5vu7u60t7fngAMOyBVXXJG2trY8/vjjmTVrVh577LH069cvQ4cOzfTp03PiiSf23H/KKaekq6srU6ZMSXd3d/bbb7/MnTt3s1t3JklL/fnmLl5k3bfObsZjAaApJp56Q7OXAABF3b3sB81ewotm9c9uavYSNmqHCUc3ewlbRDUDAAAAKEYQAQAAABTjHREAAABQRYM7VGzrTEQAAAAAxQgiAAAAgGJUMwAAAKCK2vpmr6BPMxEBAAAAFCOIAAAAAIpRzQAAAIAq7JrREBMRAAAAQDGCCAAAAKAY1QwAAACooqaa0QgTEQAAAEAxgggAAACgGNUMAAAAqMKuGQ0xEQEAAAAUI4gAAAAAilHNAAAAgCrsmtEQExEAAABAMYIIAAAAoBjVDAAAAKhCNaMhJiIAAACAYgQRAAAAQDGqGQAAAFBBvb6+2Uvo00xEAAAAAMUIIgAAAIBiVDMAAACgCrtmNMREBAAAAFCMIAIAAAAoRjUDAAAAqqirZjTCRAQAAABQjCACAAAAKEY1AwAAAKqwa0ZDTEQAAAAAxQgiAAAAgGJUMwAAAKAKu2Y0xEQEAAAAUIwgAgAAAChGNQMAAACqsGtGQ0xEAAAAAMUIIgAAAIBiVDMAAACgCrtmNMREBAAAAFCMIAIAAAAoRjUDAAAAqrBrRkNMRAAAAADFCCIAAACAYlQzAAAAoArVjIaYiAAAAACKEUQAAAAAxahmAAAAQBV11YxGmIgAAAAAihFEAAAAAMWoZgAAAEAVds1oiIkIAAAAoBhBBAAAAFCMagYAAABUYdeMhpiIAAAAAIoRRAAAAADFqGYAAABAFXbNaIiJCAAAAKAYQQQAAABQjGoGAAAAVGHXjIaYiAAAAACKEUQAAAAAxahmAAAAQBV2zWiIiQgAAACgGEEEAAAAUIxqBgAAAFShmtEQExEAAABAMYIIAAAAoBjVDAAAAKiiXm/2Cvo0ExEAAABAMYIIAAAAoBjVDAAAAKjCrhkNMREBAAAAFCOIAAAAAIpRzQAAAIAqVDMaYiICAAAAKEYQAQAAAFXUay/NTwWXXHJJ9t5774wfP77n85GPfKTn/AMPPJATTjgh++67bw4//PBcddVVve5fvXp1zjvvvBx44IGZMGFCpk+fnhUrVmzRswURAAAAsA3af//9c++99/Z8LrrooiTJypUrc8opp+TQQw/N3XffndmzZ2fOnDn51re+1XPvBRdckI6Ojtx88825/fbbs2rVqsyYMWOLniuIAAAAAHp8+9vfTmtra84444z0798/48aNy/HHH59rrrkmyXPTEAsXLsyHPvShDBkyJIMGDcqMGTPy/e9/P8uWLdvs7/eySgAAAKjiJfqyys7OznR2dm5wvL29Pe3t7Rsc7+joyBve8IYMGDCgp16x++6758EHH8w+++yT1tb/nl0YM2ZMrr322iTJww8/nDVr1mTs2LE95/fcc88MGDAgixcvztChQze5TkEEAAAAbAXmz5+fOXPmbHD8rLPOyrRp03ode9vb3pbjjjsuQ4cOzfLly/OlL30pJ598cm688casXLkyAwcO7HV9e3t7Vq5cmSQ9f/75NQMHDuw5tymCCAAAANgKTJ06Nccee+wGxzc2DbHXXnv1/POQIUNy/vnn97wzoq2tLU899VSv6zs7O9PW1pYkPX92dXVl8ODBPdd0dXX1nNsUQQQAAABUUa83ewUb9XwVjC3R0tKSlpaW1Ov1jB49Ot/85jdTq9V66hn3339/Ro8enSQZMWJE+vfvn46Ojhx22GFJkoceeijd3d0912yKl1UCAADANua2227L008/nSR56qmn8qlPfSqDBw/O+PHj89a3vjXr16/P3Llzs3bt2vzyl7/MtddemxNPPDFJssMOO+SYY47JxRdfnOXLl+cPf/hDZs2alYkTJ2bYsGGbfbYgAgAAALYxN910U4488sjsu+++OeaYY7JmzZpcccUVaWtrS1tbW+bNm5cf/vCH2X///TNt2rSceeaZmTx5cs/95557bvbee+8cddRRmTRpUvr3758LL7xwi57dUq83Z6ak+9bZzXgsADTFxFNvaPYSAKCou5f9oNlLeNF0X/GxZi9howacvGVBQLOZiAAAAACKEUQAAAAAxdg1AwAAAKqo1Zq9gj7NRAQAAABQjCACAAAAKEY1AwAAAKqoq2Y0wkQEAAAAUIwgAgAAAChGNQMAAAAqqNfqzV5Cn2YiAgAAAChGEAEAAAAUo5oBAAAAVdTsmtEIExEAAABAMYIIAAAAoBjVDAAAAKiirprRCBMRAAAAQDGCCAAAAKAY1QwAAACoolZv9gr6NBMRAAAAQDGCCAAAAKAY1QwAAACoombXjEaYiAAAAACKEUQAAAAAxahmAAAAQBWqGQ0xEQEAAAAUI4gAAAAAilHNAAAAgCrq9WavoE8zEQEAAAAUI4gAAAAAilHNAAAAgCrsmtEQExEAAABAMYIIAAAAoBjVDAAAAKiiZteMRpiIAAAAAIoRRAAAAADFqGZAHzf3Wz/NV769qNexXQYOyPdmvj/Prl+fv7/t7vz4waX53VOdaeu/ffZ/zdB86B1vyG47D+y5fu269bnopjvzrXv/M6ufXZeDXjss5777sAzZqa301wGATZp61l9l0pGH5dV77p5n1z6bjp89kL+/4Kv5za+W9Fxz+OQ35bj3HZ1RY/fKzrvslNPe/aH87N9/vsHv2mfc6Jz+8VMzdr99Uq8nDz34m3z0/efmD0//oeRXAvqiul0zGiGIgK3AiFfslHlnvKvn59bWliTJ6rXrsvjRJ3PKm/fLqGG7pKt7bS666c6c+dVb8y9nvyfb9XtuKGrWwh/l+x0P5/N//ebstOMO+dsb78y0ebflnz/yl+nXanAKgJeO/Q4el+vmL8wDP38wLS0t+f/P+V/5+69/Ke89fGo6V3QlSQa8fEB+uagj37z+O5l5ySc3+nteN37vXHzNrFw9d0G+/Ok5Wffss9lj1Mise3Zdya8DsE0SRMBWoF9rS3Ztf/kGxwcO6J+vnPbOXsf+z/ET8+4Lv54ljz+T1w7dJV3da3LDTx7MzBMm5Y2jdk+SnD/liEz+3D/lJ//xSA4e/eoi3wEAtsT/nnJOr58/Pe38/Nuvbs3rDxibH33nziTJN6//dpJk0OBBz/t7PjzzrFx35cJccfHVPceW/uaRF2HFAPw5QQRsBR59qitv+cz8vGy7fhn76iGZdtRBedUu7Ru99o+r1yZJ2l/eP0my+JEnsm59rSeESJJX7tyWka/YOT9/+DFBBAAvaS9vG5B+/fql6w9dW3zPzrvslNfvPyb/esN389WFl+TVe+yepQ/9Lpd96Yr89Ec/exFXC2w17JrRkC0OIhYtWpQHH3wwK1euTFtbW0aPHp3999//xVwbsAXGDn9FZp44KSNfsXOeXtmdy75zT6Ze/I1c/7ETstOOO/S69tl16/Olm/49E183vOf9D092rkq/1pbs/GfX7jJwQJ7q7C72PQDg/8VHP/u/86uOX+e+Rfdv8T3Dhg9Nknzwoyfn4s/9Q37V8esc8Y7D83fXzMrUt38wv37goRdruQBkC4KIZcuW5bTTTsuSJUsyfPjwDBw4MF1dXVm6dGlGjhyZuXPnZujQoSXWCmzEoXsP7/Xz64cPyVHnX52bf/qrvO/wfXuOr1tfy7lf+166utfk7z4wufQyAeAFN/3TZ2bfA8fm1GPOSq225S+Oa/mvdyl94+qbc/OC25Ik/9Hx6+x/8Pgc97535YufuOhFWS8Az9lsEPHpT38648aNy9e+9rUMHPjfb9nv6urKrFmzct5552XevHkv6iKBLffy/i/LnkMGZ+mTK3qOrVtfy8ev/k7+8/dPZ94Z7+o1KbFr+8uzvlbPM39cncFtA3qOP9XVnfF77FZ07QCwpT78mTPzlncdkdOPn55lS39f6d6nHn8qSbLkPx7udXzJrx/OkGGveKGWCGzF6hXCTza02dfh33PPPfnEJz7RK4RIkoEDB+bjH/947rnnnhdtcUB1a55dl4eXr8iuA3dMkjy7fn1mXPXt/HrZU7nsjKM3eKnl3q/6i2zXrzV3/ep3PcceX7EyS5Y/k3EjXll07QCwJT7y2Wl56zFH5IzjP5zf/ufSyvcv+91jWf77JzJ8z917HX/1HrvnsUcef6GWCcDz2OxExIABA7J8+fIMHz58g3PLly/PgAEDNnIXUMpFN92Zw/YZkd12bsvTK7vz1e/ck+61z+adB4zKuvW1nDP/27l/6RO5+JTJaUlLnuxclSRp22H77LD9dhk4oH+OPWh0Zt9yVwYPfHl2enn//O2Nd+a1u+2Sg/Z6VZO/HQD0ds4F0zP53W/Nx/7XJ9P1h67s8heDkySr/tid7lXPvduofaeBGTJsSAa2P/c+pN1HDktX58o8vfzpPPXE00mSq+cuyAfPPjn/ufg3+VXHr/Pmd07KmAn7ZNYnZzfniwFsQzYbRLz73e/OBz7wgZx66ql53etel/b29nR1daWjoyOXX355jj/++BLrBJ7H4yv+mE9c/Z0888fV2XnHAXn98Ffkqg8dl6GDB+bRpzvz/Y6HkyQnXnRdr/tmnjAp7zpwdJLknGMOTb/W1sy46ttZ8+z6HPjaYfnclCPSr3WzQ1MAUNTx7z82SXLptb0Dg8u+dEUu+9KVSZI3vfWQfHr2J3rOffJvP7bBNQvmXZft+78sH/r0GRm0c3t+86uH86G//pgXVQJbxq4ZDWmp1+ub/DdYr9dz2WWXZcGCBVm2bFlaWlpSr9czdOjQnHDCCTn11FPT0tJS+cHdt0qbAdh2TDz1hmYvAQCKunvZD5q9hBfNH88/qdlL2KgdP3lVs5ewRTY7EdHS0pIPfvCD+eAHP5iVK1f2bN/Z1tZWYn0AAADAVmSzQcT/JIAAAABgm1e3a0YjFMABAACAYgQRAAAAQDGVqhkAAACwzbNrRkNMRAAAAADFCCIAAACAYlQzAAAAoIqaXTMaYSICAAAAKEYQAQAAABSjmgEAAABV2DWjISYiAAAAgGIEEQAAAEAxqhkAAABQRd2uGY0wEQEAAAAUI4gAAAAAilHNAAAAgCrsmtEQExEAAABAMYIIAAAAoBjVDAAAAKigXrNrRiNMRAAAAADFCCIAAACAYlQzAAAAoAq7ZjTERAQAAABQjCACAAAAKEY1AwAAAKpQzWiIiQgAAACgGEEEAAAAUIxqBgAAAFRRrzV7BX2aiQgAAACgGEEEAAAAUIxqBgAAAFRh14yGmIgAAAAAihFEAAAAAMUIIgAAAKCCeq3+kvz8vzrzzDMzatSo/OQnP0mSPPLIIxk1alTGjRuX8ePH93y6urp67lm9enXOO++8HHjggZkwYUKmT5+eFStWbNHzBBEAAACwjVq4cGFWr1690XO33HJL7r333p7PwIEDe85dcMEF6ejoyM0335zbb789q1atyowZM7bomV5WCQAAAFuBzs7OdHZ2bnC8vb097e3tGxx/7LHHMnv27FxzzTWZNGnSFj9n9erVWbhwYS655JIMGTIkSTJjxowceeSRWbZsWYYOHbrJ+wURAAAAUMVLdNeM+fPnZ86cORscP+usszJt2rRex+r1es4999ycfvrpzxscnHjiiVm7dm1GjhyZD3zgA3nLW96SJHn44YezZs2ajB07tufaPffcMwMGDMjixYsFEQAAALAtmDp1ao499tgNjm9sGuKaa65JvV7Pe9/73g3O7bzzzlmwYEFe97rXpVar5Tvf+U4+8pGPZM6cOZk4cWJWrlyZJL2qGn/6+U/nNkUQAQAAAFuB56tg/LmlS5dm7ty5+frXv77R8zvuuGPGjx/f8/M73/nO3HXXXbnpppsyceLEtLW1JUm6uroyePDgnuu6urp6zm2KIAIAAACqqNWavYKGLFq0KCtWrMhxxx3X6/gZZ5yRd7zjHZk5c+YG97S2tqZef66SMmLEiPTv3z8dHR057LDDkiQPPfRQuru7M3r06M0+XxABAAAA25DJkyfn4IMP7nVs4sSJ+dznPpeDDz44ixYtyk477ZQRI0akVqvlu9/9bm688cZ8+ctfTpLssMMOOeaYY3LxxRdn9OjR6d+/f2bNmpWJEydm2LBhm32+IAIAAAC2IQMGDMiAAQM2OD548OAMGjQoS5YsyVe+8pU8+eST2X777TNixIh88YtfzBFHHNFz7bnnnpvzzz8/Rx11VNavX583velNG52k2JiW+p9mKwrrvnV2Mx4LAE0x8dQbmr0EACjq7mU/aPYSXjRdZ0xu9hI2auCl32z2ErZIa7MXAAAAAGw7BBEAAABAMd4RAQAAAFXUmvKGg62GiQgAAACgGEEEAAAAUIxqBgAAAFTQpM0ntxomIgAAAIBiBBEAAABAMaoZAAAAUIVdMxpiIgIAAAAoxkQEAAAAVGEioiEmIgAAAIBiBBEAAABAMaoZAAAAUEFdNaMhJiIAAACAYgQRAAAAQDGqGQAAAFCFakZDTEQAAAAAxQgiAAAAgGJUMwAAAKCKWrMX0LeZiAAAAACKEUQAAAAAxahmAAAAQAV1u2Y0xEQEAAAAUIwgAgAAAChGNQMAAACqUM1oiIkIAAAAoBhBBAAAAFCMagYAAABUUWv2Avo2ExEAAABAMYIIAAAAoBjVDAAAAKigbteMhpiIAAAAAIoRRAAAAADFqGYAAABAFXbNaIiJCAAAAKAYQQQAAABQjGoGAAAAVGDXjMaYiAAAAACKEUQAAAAAxahmAAAAQBV2zWiIiQgAAACgGEEEAAAAUIxqBgAAAFRQV81oiIkIAAAAoBhBBAAAAFCMagYAAABUoZrREBMRAAAAQDGCCAAAAKAY1QwAAACowK4ZjTERAQAAABQjiAAAAACKUc0AAACAKlQzGmIiAgAAAChGEAEAAAAUo5oBAAAAFdg1ozEmIgAAAIBiBBEAAABAMaoZAAAAUIFqRmNMRAAAAADFCCIAAACAYlQzAAAAoALVjMaYiAAAAACKEUQAAAAAxahmAAAAQBX1lmavoE8zEQEAAAAUI4gAAAAAilHNAAAAgArsmtEYExEAAABAMYIIAAAAoBjVDAAAAKigXrNrRiNMRAAAAADFCCIAAACAYlQzAAAAoAK7ZjTGRAQAAABQjCACAAAAKEY1AwAAACqo1+2a0QgTEQAAAEAxgggAAACgGNUMAAAAqMCuGY0xEQEAAADbsDPPPDOjRo3KT37yk55jd955Z44++ujsu+++edvb3pbbbrut1z3PPPNMpk+fngkTJuTAAw/Meeedl7Vr127R8wQRAAAAsI1auHBhVq9e3evYI488ktNPPz3ve9/78tOf/jQf//jH84lPfCK/+MUveq45++yzs2rVqtx+++25+eab09HRkS984Qtb9ExBBAAAAFRQr7W8JD+dnZ155JFHNvh0dnZu9Hs89thjmT17dv7mb/6m1/Ebbrghe+21V44//vhsv/32mTRpUiZNmpQFCxYkeS6o+NGPfpQZM2Zk0KBBGTJkSD70oQ/lG9/4RtasWbPZf3/eEQEAAABbgfnz52fOnDkbHD/rrLMybdq0Xsfq9XrOPffcnH766Rk6dGivcw8++GDGjBnT69iYMWNy66239pwfMGBA9txzz57zY8eOTXd3d5YsWZLRo0dvcp2CCAAAANgKTJ06Nccee+wGx9vb2zc4ds0116Rer+e9733vBudWrlyZ17zmNRv8jpUrV/acHzhwYK/zf/r5T9dsiiACAAAAKqjXm72CjWtvb99o6PDnli5dmrlz5+brX//6Rs+3tbWlq6ur17HOzs60tbX1nP/zwOFP1//pmk0RRAAAAMA2ZNGiRVmxYkWOO+64XsfPOOOMvOMd78jo0aNzxx139Dp3//3391QuRo8enVWrVuWhhx7qqWd0dHRkhx12yMiRIzf7fEEEAAAAbEMmT56cgw8+uNexiRMn5nOf+1wOPvjgdHZ2Zt68ebn++utz9NFH584778ztt9+e+fPnJ0le9apX5dBDD82sWbPyxS9+MWvWrMnFF1+c4447Lv3799/s8wURAAAAUEG91tLsJTRkwIABGTBgwAbHBw8enEGDBmXQoEGZO3duPv/5z2fmzJl55StfmQsuuCD77rtvz7WzZs3KzJkzM2nSpPTr1y+TJ0/Oxz/+8S16fku93px2S/ets5vxWABoiomn3tDsJQBAUXcv+0Gzl/Ci+e2ENzd7CRs1/GffbfYStkhrsxcAAAAAbDtUMwAAAKCCvl7NaDYTEQAAAEAxgggAAACgGNUMAAAAqKA5Wz5sPUxEAAAAAMUIIgAAAIBiVDMAAACgArtmNMZEBAAAAFCMIAIAAAAoRjUDAAAAKqjXVTMaYSICAAAAKEYQAQAAABSjmgEAAAAV1GvNXkHfZiICAAAAKEYQAQAAABSjmgEAAAAV1Oya0RATEQAAAEAxgggAAACgGNUMAAAAqKCumtEQExEAAABAMYIIAAAAoBjVDAAAAKigXlPNaISJCAAAAKAYQQQAAABQjGoGAAAAVFCvN3sFfZuJCAAAAKAYExEAAABQgZdVNsZEBAAAAFCMIAIAAAAoRjUDAAAAKqjVVTMaYSICAAAAKEYQAQAAABSjmgEAAAAV1FUzGmIiAgAAAChGEAEAAAAUo5oBAAAAFdTrzV5B32YiAgAAAChGEAEAAAAUo5oBAAAAFdTsmtEQExEAAABAMYIIAAAAoBjVDAAAAKigrprREBMRAAAAQDGCCAAAAKAY1QwAAACooF5v9gr6NhMRAAAAQDGCCAAAAKAY1QwAAACooGbXjIaYiAAAAACKadpExMBjZzXr0QBQXPeyO5q9BACAlwTVDAAAAKigrprRENUMAAAAoBhBBAAAAFCMagYAAABUYNeMxpiIAAAAAIoRRAAAAADFqGYAAABABfVmL6CPMxEBAAAAFCOIAAAAAIpRzQAAAIAK7JrRGBMRAAAAQDGCCAAAAKAY1QwAAACooK6a0RATEQAAAEAxgggAAACgGNUMAAAAqKDW7AX0cSYiAAAAgGIEEQAAAEAxqhkAAABQQT12zWiEiQgAAACgGEEEAAAAUIxqBgAAAFRQqzd7BX2biQgAAACgGEEEAAAAUIxqBgAAAFRQs2tGQ0xEAAAAAMUIIgAAAIBiVDMAAACggrpqRkMEEQAAALCNufTSS/ONb3wjzzzzTLbbbruMGTMmZ599dvbee+8kyahRo9K/f//069ev554FCxZk1KhRSZJarZbZs2fnuuuuS3d3d8IXDnYAAB83SURBVCZMmJDPfvazGTZs2GafrZoBAAAA25jJkyfn+uuvzz333JM77rgjhxxySE499dTUarWeay677LLce++9PZ8/hRBJMm/evNxyyy25+uqr86Mf/ShDhw7Naaed1uv+52MiAgAAACrY/P/Vbo7Ozs50dnZucLy9vT3t7e29jo0cObLXz62trXniiSfS1dWVQYMGbfZZCxYsyCmnnJI99tgjSXLOOefk4IMPzj333JMDDjhgk/cKIgAAAGArMH/+/MyZM2eD42eddVamTZu2wfHvf//7Ofvss9PV1ZWWlpacfPLJvUKIj370o3n22WczdOjQnHjiiXnPe96TJOnq6sqjjz6aMWPG9Fzb3t6e4cOHZ/HixYIIAAAA2BZMnTo1xx577AbH/3wa4k8OP/zwLFq0KCtWrMjChQuz22679Zy78sorM378+LS2tuauu+7K2WefnXXr1mXKlClZuXLlRn/vwIEDe85tiiACAAAAKnip7pqxsQrGlthpp51y0kkn5YADDsgee+yR1772tXnjG9/Yc/6www7L+9///tx0002ZMmVK2trakjw3GfE/dXV19ZzbFC+rBAAAgG1crVbLunXr8tvf/naj51tbW1Ov15M8N/kwbNiwdHR09Jzv6urK0qVLe3bd2BRBBAAAAGxjrrrqqjzxxBNJkqeffjozZ87M9ttvn3HjxuX+++/Pfffdl7Vr12bdunX58Y9/nCuuuCJHHXVUz/0nnHBCLr/88ixZsiSrVq3KrFmzMmLEiOy3336bfbZqBgAAAFTwUt01o4q77rorX/nKV/LHP/4xbW1tGTt2bK688srsuuuu+eUvf5lZs2blscceS79+/TJ06NBMnz49J554Ys/9p5xySrq6ujJlypR0d3dnv/32y9y5c9Pauvl5h5b6n2YrCttu+2HNeCwANEX3sjuavQQAKOplu+7R7CW8aL415IRmL2Gj3v74gmYvYYuoZgAAAADFqGYAAABABVtDNaOZTEQAAAAAxQgiAAAAgGJUMwAAAKCCelqavYQ+zUQEAAAAUIwgAgAAAChGNQMAAAAqqGlmNMREBAAAAFCMIAIAAAAoRjUDAAAAKqjZNaMhJiIAAACAYgQRAAAAQDGqGQAAAFBBvdkL6ONMRAAAAADFCCIAAACAYlQzAAAAoIJasxfQx5mIAAAAAIoRRAAAAADFqGYAAABABbWWlmYvoU8zEQEAAAAUI4gAAAAAilHNAAAAgArqzV5AH2ciAgAAAChGEAEAAAAUo5oBAAAAFdSavYA+zkQEAAAAUIwgAgAAAChGNQMAAAAqqLU0ewV9m4kIAAAAoBhBBAAAAFCMagYAAABUUItuRiNMRAAAAADFCCIAAACAYlQzAAAAoIJ6sxfQx5mIAAAAAIoRRAAAAADFqGYAAABABTWbZjTERAQAAABQjCACAAAAKEY1AwAAACqoNXsBfZyJCAAAAKAYQQQAAABQjGoGAAAAVFBv9gL6OBMRAAAAQDGCCAAAAKAY1QwAAACooNbS7BX0bSYiAAAAgGJMRAAAAEAFtWYvoI8zEQEAAAAUI4gAAAAAilHNAAAAgApUMxpjIgIAAAAoRhABAAAAFKOaAQAAABXUW5q9gr7NRAQAAABQjCACAAAAKEY1AwAAACqwa0ZjTEQAAAAAxQgiAAAAgGJUMwAAAKAC1YzGmIgAAAAAihFEAAAAAMWoZgAAAEAF9WYvoI8zEQEAAAAUI4gAAAAAilHNAAAAgApqLc1eQd9mIgIAAAAoRhABAAAAFKOaAQAAABXUmr2APs5EBAAAAFCMIAIAAAAoRjUDAAAAKlDNaIyJCAAAAKAYQQQAAABQjGoGAAAAVFBv9gL6OBMRAAAAQDGCCAAAAKAY1QwAAACooNbS7BX0bSYiAAAAgGIEEQAAALCNufTSS/PmN785++23Xw466KB84AMfyOLFi3vOP/DAAznhhBOy77775vDDD89VV13V6/7Vq1fnvPPOy4EHHpgJEyZk+vTpWbFixRY9WxABAAAAFdReop8qJk+enOuvvz733HNP7rjjjhxyyCE59dRTU6vVsnLlypxyyik59NBDc/fdd2f27NmZM2dOvvWtb/Xcf8EFF6SjoyM333xzbr/99qxatSozZszYomd7RwQAAABsBTo7O9PZ2bnB8fb29rS3t/c6NnLkyF4/t7a25oknnkhXV1e+973vpbW1NWeccUZaW1szbty4HH/88bnmmmvy9re/PatXr87ChQtzySWXZMiQIUmSGTNm5Mgjj8yyZcsydOjQTa5TEAEAAABbgfnz52fOnDkbHD/rrLMybdq0DY5///vfz9lnn52urq60tLTk5JNPzqBBg/Lggw9mn332SWvrf5coxowZk2uvvTZJ8vDDD2fNmjUZO3Zsz/k999wzAwYMyOLFiwURAAAA8EKqN3sBz2Pq1Kk59thjNzj+59MQf3L44Ydn0aJFWbFiRRYuXJjddtstSbJy5coMHDhwg9+xcuXKnvNJNrhm4MCBPec2RRABAAAAW4GNVTC2xE477ZSTTjopBxxwQPbYY4+0tbXlqaee6nVNZ2dn2trakqTnz66urgwePLjnmq6urp5zm+JllQAAALCNq9VqWbduXX77299m9OjReeCBB1Kr/fcrMO+///6MHj06STJixIj0798/HR0dPecfeuihdHd391yzKYIIAAAAqKCW+kvyU8VVV12VJ554Ikny9NNPZ+bMmdl+++0zbty4vPWtb8369eszd+7crF27Nr/85S9z7bXX5sQTT0yS7LDDDjnmmGNy8cUXZ/ny5fnDH/6QWbNmZeLEiRk2bNhmny2IAAAAgG3MXXfdlWOOOSbjxo3L0UcfnSeffDJXXnlldt1117S1tWXevHn54Q9/mP333z/Tpk3LmWeemcmTJ/fcf+6552bvvffOUUcdlUmTJqV///658MILt+jZLfV6vSnv2dhu+82nJACwtehedkezlwAARb1s1z2avYQXzfnD/6rZS9ioT/72a81ewhbxskoAAACooLb5S9gE1QwAAACgGEEEAAAAUIxqBgAAAFTQlBctbkVMRAAAAADFCCIAAACAYlQzAAAAoAK7ZjTGRAQAAABQjCACAAAAKEY1AwAAACqotTR7BX2biQgAAACgGEEEAAAAUIxqBgAAAFRQS73ZS+jTTEQAAAAAxQgiAAAAgGJUMwAAAKACxYzGmIgAAAAAihFEAAAAAMWoZgAAAEAFtWYvoI8zEQEAAAAUI4gAAAAAilHNAAAAgApq9s1oiIkIAAAAoBhBBAAAAFCMagYAAABUoJjRGBMRAAAAQDGCCAAAAKAY1QwAAACooNbsBfRxJiIAAACAYgQRAAAAQDGqGQAAAFBBzb4ZDTERAQAAABQjiAAAAACKUc0AAACAChQzGmMiAgAAAChGEAEAAAAUo5oBAAAAFdSavYA+zkQEAAAAUIwgAgAAAChGNQMAAAAqqNs3oyEmIgAAAIBiBBEAAABAMaoZAAAAUIFdMxpjIgIAAAAoRhABAAAAFKOaAQAAABXU7JrREBMRAAAAQDGCCAAAAKAY1QwAAACoQDGjMSYiAAAAgGIEEQAAAEAxqhkAAABQgV0zGmMiAgAAAChGEAEAAAAUo5oBAAAAFdSavYA+zkQEAAAAUIwgAgAAAChGEAF93JsOPSg3fOOK/HbJoqxb+2hOet97ep2/fN6Xs27to70+P77j5l7XfO87125wzdeuvrTk1wCALfL3l1+dMYdM7vWZ+M4pPedXrerOBRddmiOO+evsN+ldeccJp+SqBTf0+h1r167NBRddmkOPfG8OOOKYnPWxz+Sx5U+U/ipAH1Z/if6vr/COCOjj2tp2zP33/yr/dPV1ufIf/26j13z3uz/M1JP/d8/Pa9c+u8E1V1y5IP/nU1/o+bm7e/ULv1gAeAGMfPWrcsWcL/b83Nr633+3duElX81di36ez3/qnAwb+sos+vl9+cwX/y477dSeo99+RJLkC3/3ldx+x1258DMzstOg9lx4yVdz5jmfyb/848Xp169f8e8DsK0RREAf981v/Vu++a1/S5L847wvb/SaNWvW5vHHN/03PatWdW/2GgB4KejXr1923WXwRs/9/L7Fecfb/r8cuN++SZJhuw3JDbf8a+67/1c5+u1HpGvlH/ONW76dz5374Rx84IQkyec/dU7e+u6puWvRz3PIQfsV+x4A2yrVDNgGHHLIAVn2yC/ywP135B/mXpi/+ItdNrjmve95Vx5bdl9+8fN/y4Vf+FTa2nZswkoBYPMeWfZYJh39V3nbX74/Z5/3+fzu0d/3nBv/+tflBz/+SX7/X+H6vfc9kAd//Zsc8obnAoYHfvXrrFu3rieESJLdhvxF9hi+e+6974GyXwTos2ov0U9f0fBERL1ez6JFi3LAAQe8EOsBXmD/+u3bc8PC2/Lww7/LiOG7Z+bMj+U73/6XHHjQ5KxduzZJ8s8LFmbp0key7PePZ5999sr5f/OJjB27dyYfNWUzvx0Aynr9PqPyuU9+JCOH756nn1mRr8z/5/z1aR/NjVf/Q3Ya1J5zP3xaZl54Sd5y3EnZ7r9qFp/48Ok5/JCDkiRPPvVM+vVrzc47Der1e3cZvHOeevqZ4t8HYFvUcBDx7LPP5qSTTsrixYtfiPUAL7B/+Zebev65o+PB3POzX+Y3//mTHHnkEVm48JtJknmXf63XNUt+szT/fuetGT9uTO79eUfxNQPA83nTG3v/5de+rxudtx9/cm785ncz9YTj8rXrbsrP73sgc7746ez2yiG55+f35W//fl6G7TYkh75h/yatGoD/6QWpZtTrfeftnLCt+/3vH88jj/w+r33NyOe9ZtE9v8i6devymtfuUXBlAFDdy18+IHuOHJ7f/u7RrF6zJrP/4cp85MwP5PBD35BRrxmZKX95dCYfMTFX/vP1SZJdd9k569fX8syKP/T6PU89/Ux2GbxzM74C0Ac1e3eMbWLXjL333nuT51taWl6QxQAvvl122TnDhr0yv39s+fNeM3bs3tluu+3y2O8fL7gyAKhuzZq1WbL0dzlwwuuzbt26rFu3rtcuGknSr19rarXn/gN9n1GvzXbbbZd//+n/be/+Y7yu7zuAv7yjKnB3glrt7kJhuMkhKMJZpYrWLotpwHRlqwYuRcw8M+IgMyCzkFS5rrjML7HEkh6ZXpQLcWTntmI9NGZOF7X+WJlJRWi2OKwNiLT+uu+N4w75fveH7XfiAfLJh76/wD0ehuTu/f7cfd/8Z568np/3qzHnuq9GRMSevb+K//nFL2P6xRclPz/AcHRMQcSoUaNi5cqVMW7cuCF7g4ODceuttx73gwHHZvToUfEHv5luqKmpiS9+sTGmTZsS7733frz33gdx93eWxT//y5Z4e887MWH8uFj9vRWxd++7lVrGxInjo3X+3HjiiX+LX7/7Xlw0+cK499674j9ffS1e+Ml/VPOvBgBDFNY9ENdedUX83vnnxXvvfxDrH/6H6O/fH38y+4+jbvTouGz6xbG246EYNXJkNH7hvPjpq6/FY088HUtv+/OIiKivGx1/ev11cd8PO+PssWNizFn1ce/9D8SFF/x+zLzs0ir/7QCGh2MKIpqbm2PkyJFx+eWXD9kbHBxUzYAquqxlWjz9r49Wvl919/JYdffy2ND1j/GXi1fE1KnN8a1vfTPGjGmIt9/eG8/++09iXuui6Ov734iIGBw8EH/01VmxZHFb1NWNil/+cndseeLp+JvvfT9KpZPp3bsADAfv7P11/PXdfxfvf9gbZ485Ky6Z0hyP/P33o/EL50dExJr2b8fa9Q/Ht9vvjQ97i9H4hfNi8a0LovWbX6/8jm//1V/EiNrauOOuv42BgcG44rJpcc937oja37zcEuCz+L/kfE4rH0OKsGXLljjrrLPiqquuGrJXKpVi8+bNMXfu3EwfPOL0pkzPA8DJrH/3c9U+AgAk9blzT933jS2c8GfVPsJhbXjzn6p9hGNyTBMRs2fPPuJeTU1N5hACAAAATlYlrYBcjsutGQAAAADHQhABAAAAJHNM1QwAAADgY4oZ+ZiIAAAAAJIRRAAAAADJqGYAAABABiXljFxMRAAAAADJCCIAAACAZFQzAAAAIIOyakYuJiIAAACAZExEAAAAwDBTKBTi2WefjbfffjtGjRoV11xzTSxfvjzGjh1beWbSpElxxhlnRG1tbWVt06ZNMWnSpIiIKJVKsXbt2nj00Uejv78/ZsyYEd/97nejqanpqJ9tIgIAAAAyKJ2gf7Kora2NQqEQL7/8cmzevDn27NkTK1asGPLcAw88EK+++mrlz29DiIiIBx98MB5//PHYuHFjPP/889HY2BiLFi2KUunopzERAQAAAKeA3t7e6O3tHbLe0NAQDQ0Nh6wtXbq08vU555wTCxYsiGXLlmX6vE2bNkVbW1tMnDgxIiKWL18eV155ZWzdujW+9KUvHfHnBBEAAABwCtiwYUOsW7duyPrixYtjyZIlR/3ZF198MZqbm4esL1u2LA4cOBCNjY0xf/78uPHGGyMiolgsxq5du2Lq1KmVZxsaGmL8+PGxY8cOQQQAAAAcL6UT9NaMhQsXxty5c4esf3oa4tO2bNkS3d3dsXHjxkPWH3744Zg+fXrU1NTESy+9FHfccUd89NFH0draGn19fYf93fX19ZW9IxFEAAAAwCngcBWMz9LT0xOrVq2Kjo6OmDJlyiF7X/7ylytfX3PNNXHzzTfHY489Fq2trVFXVxcRH09GfFKxWKzsHYmXVQIAAMAw1N3dHe3t7bF+/fqYOXPmZz5fU1MT5fLH0yD19fXR1NQU27Ztq+wXi8V46623YvLkyUf/PfmODQAAAMNL+QT9L4uurq5Ys2ZNdHZ2RktLy5D9119/PV577bUYHByMjz76KF544YV46KGHYs6cOZVn5s2bF52dnbFz587Yt29fFAqFmDBhwmF/3yepZgAAAMAws3r16hgxYkTcdNNNh6z39PREY2NjvPPOO1EoFGLPnj1RW1sbjY2Ncfvtt8f8+fMrz7a1tUWxWIzW1tbo7++PlpaW6OjoiJqao888nFb+7VxFYiNOb6rGxwJAVfTvfq7aRwCApD537sRqH+F35pvjv17tIxzWo794rNpHOCYmIgAAACCDUrUPcJLzjggAAAAgGUEEAAAAkIxqBgAAAGRQpVctnjJMRAAAAADJCCIAAACAZFQzAAAAIINSqGbkYSICAAAASEYQAQAAACSjmgEAAAAZlKp9gJOciQgAAAAgGUEEAAAAkIxqBgAAAGRQdmtGLiYiAAAAgGQEEQAAAEAyqhkAAACQQUk1IxcTEQAAAEAygggAAAAgGdUMAAAAyKBcVs3Iw0QEAAAAkIwgAgAAAEhGNQMAAAAyKFX7ACc5ExEAAABAMoIIAAAAIBnVDAAAAMigHG7NyMNEBAAAAJCMIAIAAABIRjUDAAAAMiipZuRiIgIAAABIRhABAAAAJKOaAQAAABmUy6oZeZiIAAAAAJIRRAAAAADJqGYAAABABm7NyMdEBAAAAJCMIAIAAABIRjUDAAAAMiirZuRiIgIAAABIRhABAAAAJKOaAQAAABmUyqoZeZiIAAAAAJIRRAAAAADJqGYAAABABooZ+ZiIAAAAAJIRRAAAAADJqGYAAABABiXljFxMRAAAAADJCCIAAACAZFQzAAAAIAPVjHxMRAAAAADJCCIAAACAZFQzAAAAIINyWTUjDxMRAAAAQDKCCAAAACAZ1QwAAADIwK0Z+ZiIAAAAAJIRRAAAAADJqGYAAABABmXVjFxMRAAAAADJCCIAAACAZFQzAAAAIINyWTUjDxMRAAAAQDKCCAAAACAZ1QwAAADIoOTWjFxMRAAAAADJCCIAAACAZFQzAAAAIAO3ZuRjIgIAAABIRhABAAAAJKOaAQAAABm4NSMfExEAAABAMoIIAAAAIBnVDAAAAMigrJqRi4kIAAAAIBlBBAAAAJCMagYAAABkUCqrZuRhIgIAAABIRhABAAAAJKOaAQAAABm4NSMfExEAAABAMoIIAAAAGGYKhULMmTMnZsyYEbNmzYqVK1fG+++/f8gz27dvj3nz5sW0adPi2muvja6urkP29+/fH3fddVdcfvnlMWPGjLj99tvjgw8++MzPFkQAAABABqVy+YT8k0VtbW0UCoV4+eWXY/PmzbFnz55YsWJFZb+vry/a2tpi1qxZ8corr8TatWtj3bp18eSTT1aeueeee2Lbtm3x4x//OJ555pnYt29f3HnnnZ/52d4RAQAAAKeA3t7e6O3tHbLe0NAQDQ0Nh6wtXbq08vU555wTCxYsiGXLllXWnnrqqaipqYnbbrstampq4tJLL40bbrghHnnkkfja174W+/fvjx/96Efxgx/8IM4///yIiLjzzjtj9uzZsXv37mhsbDziOQURAAAAcArYsGFDrFu3bsj64sWLY8mSJUf92RdffDGam5sr3//85z+Piy66KGpq/r9IMXXq1Oju7o6IiDfffDMGBgbi4osvruxfcMEFMXLkyNixY4cgAgAAAI6XE/XWjIULF8bcuXOHrH96GuLTtmzZEt3d3bFx48bKWl9fX9TX1w/5PX19fZX9iBjyTH19fWXvSAQRAAAAcAo4XAXjs/T09MSqVauio6MjpkyZUlmvq6uLd99995Bne3t7o66urrIfEVEsFuPss8+uPFMsFit7R+JllQAAADAMdXd3R3t7e6xfvz5mzpx5yF5zc3Ns3749SqVSZe3111+v1DcmTJgQZ5xxRmzbtq2y/8Ybb0R/f/8hFY/DEUQAAABABtW+HeN43JrR1dUVa9asic7OzmhpaRmyf91118XBgwejo6MjBgcH42c/+1l0d3fH/PnzIyLizDPPjG984xtx//33x969e+PDDz+MQqEQX/nKV6Kpqemon31auZzxtMfJiNOPfjAAOJX0736u2kcAgKQ+d+7Eah/hd+bCz19W7SMc1n/96qfH/OykSZNixIgRcfrppx+y3tPTU3nR5Pbt26O9vT127NgRY8eOjVtuuSVuuummyrP79++P1atXx5NPPhkHDx6Mq6++Otrb22PMmDFH/WxBBAAkIIgAYLg5lYOIP/z80AmCE8F//2prtY9wTFQzAAAAgGQEEQAAAEAyru8EAACADLK+GJJDmYgAAAAAkhFEAAAAAMmoZgAAAEAG5VDNyMNEBAAAAJCMIAIAAABIRjUDAAAAMiiXS9U+wknNRAQAAACQjCACAAAASEY1AwAAADIouTUjFxMRAAAAQDKCCAAAACAZ1QwAAADIoFxWzcjDRAQAAACQjCACAAAASEY1AwAAADJwa0Y+JiIAAACAZAQRAAAAQDKqGQAAAJCBWzPyMREBAAAAJCOIAAAAAJJRzQAAAIAMSqoZuZiIAAAAAJIRRAAAAADJqGYAAABABuVQzcjDRAQAAACQjCACAAAASEY1AwAAADIouzUjFxMRAAAAQDKCCAAAACAZ1QwAAADIoOTWjFxMRAAAAADJCCIAAACAZFQzAAAAIAO3ZuRjIgIAAABIRhABAAAAJKOaAQAAABmUVDNyMREBAAAAJCOIAAAAAJJRzQAAAIAM3JqRj4kIAAAAIBlBBAAAAJCMagYAAABkUArVjDxMRAAAAADJCCIAAACAZFQzAAAAIAO3ZuRjIgIAAABIRhABAAAAJKOaAQAAABmUVDNyMREBAAAAJCOIAAAAAJJRzQAAAIAMyqGakYeJCAAAACAZQQQAAACQjGoGAAAAZODWjHxMRAAAAADJCCIAAACAZFQzAAAAIIOyakYuJiIAAACAZAQRAAAAQDKqGQAAAJBBOVQz8jARAQAAACQjiAAAAACSUc0AAACADNyakY+JCAAAACAZQQQAAACQjGoGAAAAZKCakY+JCAAAACAZQQQAAACQjGoGAAAAZKCYkY+JCAAAACCZ08resgEAAAAkYiICAAAASEYQAQAAACQjiAAAAACSEUQAAAAAyQgiAAAAgGQEEQAAAEAygggAAAAgGUEEAAAAkIwgAgAAAEhGEAEAAAAkI4gAAAAAkhFEwDBRKpXivvvuiyuvvDKmT58et9xyS+zatavaxwKA466npydaW1tjxowZMWnSpGofB4BPEUTAMPHggw/G448/Hhs3boznn38+GhsbY9GiRVEqlap9NAA4rhoaGqK1tTVWrlxZ7aMAcBiCCBgmNm3aFG1tbTFx4sQYPXp0LF++PHbu3Blbt26t9tEA4Li6+uqr4/rrr49x48ZV+ygAHIYgAoaBYrEYu3btiqlTp1bWGhoaYvz48bFjx44qngwAABhuBBEwDPT19UXEx+HDJ9XX11f2AAAAUhBEwDBQV1cXER9PRnxSsVis7AEAAKQgiIBhoL6+PpqammLbtm2VtWKxGG+99VZMnjy5iicDAACGG0EEDBPz5s2Lzs7O2LlzZ+zbty8KhUJMmDAhWlpaqn00ADiuDh48GAMDA3HgwIGIiBgYGIiBgQE3RQGcIEZU+wBAGm1tbVEsFqO1tTX6+/ujpaUlOjo6oqZGHgnAqWXz5s2xYsWKyveXXHJJRER0dXXFFVdcUa1jAfAbp5XL5XK1DwEAAAAMD/4pFAAAAEhGEAEAAAAkI4gAAAAAkhFEAAAAAMkIIgAAAIBkBBEAAABAMoIIAAAAIBlBBAAAAJDM/wE3f8f/IQDFDwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1440x1008 with 2 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "df_cm = pd.DataFrame(cm, range(2), range(2))\n",
        "plt.figure(figsize=(20,14))\n",
        "sn.set(font_scale=1.2) # for label size\n",
        "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 14}, fmt='g') # for num predict size\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZVaLI0sqWxP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1993232e-c382-4e73-8e82-f6cba9e030ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "ไม่เดือดร้อน     0.7704    0.7065    0.7371       736\n",
            "   เดือดร้อน     0.7286    0.7891    0.7577       735\n",
            "\n",
            "    accuracy                         0.7478      1471\n",
            "   macro avg     0.7495    0.7478    0.7474      1471\n",
            "weighted avg     0.7495    0.7478    0.7474      1471\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_true, predicted_classes, target_names=label, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKYWrOdPqy3F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zRfzyA_8eiSB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ITlAaZ04ejwZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}